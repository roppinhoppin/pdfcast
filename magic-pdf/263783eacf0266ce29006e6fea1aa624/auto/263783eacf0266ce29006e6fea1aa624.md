# Continuous LWE  

Joan Bruna $^{*}\mathrm{a},\mathrm{b},\mathrm{c}$ c, Oded Regev $\cdot\dag\mathbf{a}$ , Min Jae Song $\ddagger\mathbf{a}$ , and Yi Tang §d  

$\mathrm{a}$ Courant Institute of Mathematical Sciences, New York University, New York $\mathrm{b}$ Center for Data Science, New York University, New York $\mathrm{c}$ Institute for Advanced Study, Princeton  

$\mathrm{_d}$ Computer Science and Engineering, University of Michigan, Ann Arbor  

October 27, 2020  

# Abstract  

We introduce a continuous analogue of the Learning with Errors (LWE) problem, which we name CLWE. We give a polynomial-time quantum reduction from worst-case lattice problems to CLWE, showing that CLWE enjoys similar hardness guarantees to those of LWE. Alternatively, our result can also be seen as opening new avenues of (quantum) attacks on lattice problems. Our work resolves an open problem regarding the computational complexity of learning mixtures of Gaussians without separability assumptions (Diakonikolas 2016, Moitra 2018). As an additional motivation, (a slight variant of) CLWE was considered in the context of robust machine learning (Diakonikolas et al. FOCS 2017), where hardness in the statistical query (SQ) model was shown; our work addresses the open question regarding its computational hardness (Bubeck et al. ICML 2019).  

# 1 Introduction  

The Learning with Errors (LWE) problem has served as a foundation for many lattice-based cryptographic schemes [ Pei16 ]. Informally, LWE asks one to solve noisy random linear equations. To be more precise, the goal is to find a secret vector $s\in\mathbb{Z}_{q}^{n}$ given polynomially many samples of the form $(a_{i},b_{i})$ , where $a_{i}\in\mathbb{Z}_{q}^{n}$ is uniformly chosen and $b_{i}\approx\langle\pmb{a}_{i},\pmb{s}\rangle$ (mod $q$ ). In the absence of noise, LWE can be efficiently solved using Gaussian elimination. However, LWE is known to be hard assuming hardness of worst-case lattice problems such as Gap Shortest Vector Problem (GapSVP) or Shortest Independent Vectors Problem (SIVP) in the sense that there is a polynomial-time quantum reduction from these worst-case lattice problems to LWE [ Reg05 ].  

In this work, we introduce a new problem, called Continuous LWE (CLWE). As the name suggests, this problem can be seen as a continuous analogue of LWE, where equations in $\mathbb{Z}_{q}^{n}$ are replaced with vectors in $\mathbb{R}^{n}$ (see Figure 1 ). More precisely, CLWE considers noisy inner products $z_{i}\,\approx\,\gamma\langle{\pmb y}_{i},{\pmb w}\rangle$ (mod 1), where the noise is drawn from a Gaussian distribution of width $\beta\,>\,0$ ,$\gamma>0$ is a problem parameter, $\pmb{w}\in\mathbb{R}^{n}$ is a secret unit vector, and the public vectors $\pmb{y}_{i}\in\mathbb{R}^{n}$ are drawn from the standard Gaussian. Given polynomially many samples of the form $(y_{i},z_{i})$ , CLWE asks one to find the secret direction $\mathbf{\delta}^{\prime}\mathbf{\delta}w$ .  

![](images/5c18b7fdc29d6b70fe3d738264c32bed957e51f9ff5eba76fa329e72c4917630.jpg)  

Figure 1: Scatter plot of two-dimensional CLWE samples. Color indicates the last ( $\mathcal{L}$ ) coordinate.  

One can also consider a closely related homogeneous variant of CLWE (see Figure 2 ). This distribution, which we call homogeneous CLWE, can be obtained by essentially conditioning on $z_{i}\approx0$ . It is a mi of “Gaussian pancakes” of width $\approx\beta/\gamma$ in the secret direction and width 1 in the remaining n$n-1$ −1 directions. The Gaussian components are equally spaced, with a separation of $\approx1/\gamma$ . (See Definition 2.19 for the precise statement.)  

![](images/5908596ecf5a111ade155046aa3c2c823bab254254359168220681705ed37b1c.jpg)  
Figure 2: Left: Scatter plot of two-dimensional homogeneous CLWE samples. Right: Unnormalized probability densities of homogeneous CLWE (blue) and Gaussian (orange) along the hidden direction.  

Our main result is that CLWE (and homogeneous CLWE) enjoy hardness guarantees similar to those of LWE.  

Theorem 1.1 (Informal) .Let nbe an integer, $\beta=\beta(n)\in(0,1)$ and $\gamma=\gamma(n)\geq2{\sqrt{n}}$ such that the ratio $\gamma/\beta$ is polynomially bounded. If there exists an efficient algorithm that solves $\mathrm{CLWE}_{\beta,\gamma}$ ,then there exists an efficient quantum algorithm that approximates worst-case lattice problems to within polynomial factors.  

Although we defined CLWE above as a search problem of finding the hidden direction, Theorem 1.1 is actually stronger, and applies to the decision variant of CLWE in which the goal is to distinguish CLWE samples $(y_{i},z_{i})$ from samples where the noisy inner product $z_{i}$ is replaced by a random number distributed uniformly on $[0,1)$ (and similarly for the homogeneous variant).  

Motivation: Lattice algorithms. Our original motivation to consider CLWE is as a possible approach to finding quantum algorithms for lattice problems. Indeed, the reduction above (just like the reduction to LWE [ Reg05 ]), can be interpreted in an algorithmic way: in order to quantumly solve worst-case lattice problems, “all” we have to do is solve CLWE (classically or quantumly). The elegant geometric nature of CLWE opens up a new toolbox of techniques that can potentially be used for solving lattice problems, such as sum-of-squares-based techniques and algorithms for learning mixtures of Gaussians [ MV10 ]. Indeed, some recent algorithms (e.g., [ KKK19 ,RY20 ]) solve problems that include CLWE or homogeneous CLWE as a special case (or nearly so), yet as far as we can tell, so far none of the known results leads to an improvement over the state of the art in lattice algorithms.  

To demonstrate the usefulness of CLWE as an algorithmic target, we show in Section 7 a simple moment-based algorithm that solves CLWE in time $\exp(\gamma^{2})$ . Even though this does not imply subexponential time algorithms for lattice problems (since Theorem 1.1 requires $\gamma>\sqrt{n}$ ), it is interesting to contrast this algorithm with an analogous algorithm for LWE by Arora and Ge [ AG11 ]. The two algorithms have the same running time (where $\gamma$ is replaced by the absolute noise αq in the LWE samples), and both rely on related techniques (moments in our case, powering in Arora-Ge’s), yet the Arora-Ge algorithm is technically more involved than our rather trivial algorithm (which just amounts to computing the empirical covariance matrix). We interpret this as an encouraging sign that CLWE might be a better algorithmic target than LWE.  

Motivation: Hardness of learning Gaussian mixtures. Learning mixtures of Gaussians is a classical problem in machine learning [ Pea94 ]. Efficient algorithms are known for the task if the Gaussian components are guaranteed to be sufficiently well separated (e.g., [ Das99 ,VW02 ,AK05 ,DS07 ,BV08 ,RV17 ,HL18 ,KSS18 ,DKS18 ]). Without such strong separation requirements, it is known that efficiently recovering the individual components of a mixture (technically known as “parameter estimation”) is in general impossible [ MV10 ]; intuitively, this exponential information theoretic lower bound holds because the Gaussian components “blur into each other”, despite being mildly separated pairwise.  

This leads to the question of whether there exists an efficient algorithm that can learn mixtures of Gaussians without strong separation requirement, not in the above strong parameter estimation sense (which is impossible), but rather in the much weaker density estimation sense, where the goal is merely to output an approximation of the given distribution’s density function. See [ Dia16 ,Moi18 ]for the precise statement and [ DKS17 ] where a super-polynomial lower bound for density estimation is shown in the restricted statistical query (SQ) model [ Kea98 ,Fel+17 ]. Our work provides a negative answer to this open question, showing that learning Gaussian mixtures is computationally difficult even if the goal is only to output an estimate of the density (see Proposition 5.2 ). It is worth noting that our hard instance has almost non-overlapping components, i.e., the pairwise statistical distance between distinct Gaussian components is essentially 1, a property shared by the SQ-hard instance of [ DKS17 ].  

Motivation: Robust machine learning. Variants of CLWE have already been analyzed in the context of robust machine learning [ Bub+19 ], in which the goal is to learn a classifier that is robust against adversarial examples at test time [ Sze+14 ]. In particular, Bubeck et al. [ Bub+19 ] use the SQ-hard Gaussian mixture instance of Diakonikolas et al. [ DKS17 ] to establish SQ lower bounds for learning a certain binary classification task, which can be seen as a variant of homogeneous CLWE. The key difference between our distribution and that of [ DKS17 ,Bub+19 ] is that our distribution has equal spacing between the “layers” along the hidden direction, whereas their “layers” are centered around roots of Hermite polynomials (the goal being to exactly match the lower moments of the standard Gaussian). The connection to lattices, which we make for the first time here, answers an open question by Bubeck et al. [ Bub+19 ].  

As additional evidence of the similarity between homogeneous CLWE and the distribution considered in [ DKS17 ,Bub+19 ], we prove a super-polynomial SQ lower bound for homogeneous CLWE (even with super-polynomial precision). For $\gamma=\Omega({\sqrt{n}})$ , this result translates to an exponential SQ lower bound for exponential precision, which corroborates our computational hardness result based on worst-case lattice problems. The uniform spacing in the hidden structure of homogeneous CLWE leads to a simplified proof of the SQ lower bound compared to previous works, which considered non-uniform spacing between the Gaussian components. Note that computational hardness does not automatically imply SQ hardness as query functions in the SQ framework need not be efficiently computable.  

Bubeck et al. [ Bub+19 ] were also interested in a variant of the learning problem where instead of one hidden direction, there are $m~\geq~1$ orthogonal hidden directions. So, for instance, the “Gaussian pancakes” in the $m=1$ case above are replaced with “Gaussian baguettes” in the case $m=2$ , forming an orthogonal grid in the secret two-dimensional space. As we show in Section 9 ,our computational hardness easily extends to the $m\,>\,1$ case using a relatively standard hybrid argument. The same is true for the SQ lower bound we show in Section 8 (as well as for the SQ lower bound in [ DKS17 ,Bub $^+$ 19 ]; the proof is nearly identical). The advantage of the $m>1$ variant is that the distance between the Gaussian mixture components increases from $\approx1/\gamma$ (which can be as high as $\approx1/\sqrt{n}$ if we want our hardness to hold) to $\approx\sqrt{m}/\gamma$ (which can be as high as $\approx1$ by taking $m\approx n$ ). This is a desirable feature for showing hardness of robust machine learning.  

Motivation: Cryptographic applications. Given the wide range of cryptographic applications of LWE [ Pei16 ], it is only natural to expect that CLWE would also be useful for some cryptographic tasks, a question we leave for future work. CLWE’s clean and highly symmetric definition should make it a better fit for some applications; its continuous nature, however, might require a discretization step due to efficiency considerations.  

Analogy with LWE. As argued above, there are apparently nontrivial differences between CLWE and LWE, especially in terms of possible algorithmic approaches. However, there is undoubtedly also strong similarity between the two. In terms of parameters, the $\gamma$ parameter in CLWE (density of layers) plays the role of the absolute noise level αq in LWE. And the $\beta$ parameter in CLWE plays the role of the relative noise parameter $\alpha$ in LWE. Using this correspondence between the parameters, the hardness proved for CLWE in Theorem 1.1 is essentially identical to the one proved for LWE in [ Reg05 ]. The similarity extends even to the noiseless case, where $\alpha=0$ in LWE and $\beta=0$ in CLWE. In particular, in Section 6 we present an efficient LLL-based algorithm for solving noiseless CLWE, which is analogous to Gaussian elimination for noiseless LWE.  

Comparison with previous work. The CLWE problem is related to the hard problem introduced in the seminal work of Ajtai and Dwork [ AD97 ]. Specifically, both problems involve finding a hidden direction in samples from a continuous distribution. One crucial difference, though, is in the density of the layers. Whereas in our hardness result the separation between the layers can be as large as $\approx1/\sqrt{n}$ , in Ajtai and Dwork the separation is exponentially small. This larger separation in CLWE is more than just a technicality. First, it is the reason we need to employ the quantum machinery from the LWE hardness proof [ Reg05 ]. Second, it is nearly tight, as demonstrated by the algorithm in Section 7 . Third, it is necessary for applications such as hardness of learning Gaussian mixtures. Finally, this larger separation is analogous to the main difference between LWE and earlier work [ Reg04 ], and is what leads to the relative efficiency of LWE-based cryptography.  

Acknowledgements. We thank Aravindan Vijayaraghavan and Ilias Diakonikolas for useful comments.  

# 1.1 Technical Overview  

Broadly speaking, our proof follows the iterative structure of the original LWE hardness proof [ Reg05 ](in fact, one might say most of the ingredients for CLWE were already present in that 2005 paper!). We also make use of some recent techniques, such as a way to reduce to decision problems directly [ PRS17 ].  

In more detail, as in previous work, our main theorem boils down to solving the following problem: we are given a CLWE $\beta,\gamma$ oracle and polynomially many samples from $D_{L,r}$ , the discrete Gaussian distribution on $L$ of width $r$ ,and our goal is to solve $\mathrm{BDD}_{L^{*},\gamma/r}$ , which is the problem of finding the closest vector in the dual lattice $L^{*}$ given a vector $^t$ that is within distance $\gamma/r$ of $L^{*}$ . (It is known that $\mathrm{BDD}_{L^{*},1/\tau}$ rcan be efficiently solved even if all we are given is polynomially many samples from $D_{L,r}$ , without any need for an oracle [ AR05 ]; the point here is that the CLWE oracle allows us to extend the decoding radius from $1/r$ to $\gamma/r$ .) Once this is established, the main theorem follows from previous work [ PRS17 ,Reg05 ]. Very briefly, the resulting BDD solution is used in a quantum procedure to produce discrete Gaussian samples that are shorter than the ones we started with. This process is then repeated, until eventually we end up with the desired short discrete Gaussian samples. We remark that this process incurs a $\sqrt{n}$ loss in the Gaussian width (Lemma 3.4 ), and the reason we require $\gamma\geq2\sqrt{n}$ is to overcome this loss.  

We now explain how we solve the above problem. For simplicity, assume for now that we have a search CLWE oracle that recovers the secret exactly. (Our actual reduction is stronger and only requires a decision CLWE oracle.) Let the given BDD instance be $\pmb{u}+\pmb{w}$ , where $\pmb{u}\ \in\ \cal L^{*}$ and $\|\pmb{w}\|\,=\,\gamma/r$ . We will consider the general case of $\|\pmb{w}\|\leq\gamma/r$ in Section 3 . The main idea is to generate CLWE samples whose secret is essentially the desired BDD solution $\mathbf{\nabla}^{\prime}\mathbf{w}$ , which would then complete the proof. To begin, take a sample from the discrete Gaussian distribution $y\sim D_{L,r}$ (as provided to us) and consider the inner product  

$$
\langle{\pmb y},{\pmb u}+{\pmb w}\rangle=\langle{\pmb y},{\pmb w}\rangle{\pmod{1}}~,
$$  

where the equality holds since $\langle\pmb{y},\pmb{u}\rangle\in\mathbb{Z}$ by definition. The $(n{+}1)$ -dimensional vector ( ${\bf\nabla}_{y},\langle{\bf\boldsymbol{y}},{\boldsymbol w}\rangle$ mod 1) is almost a CLWE sample (with parameter $\gamma$ since $\gamma=r\|w\|$ is the width of $\langle y,w\rangle$ ) — the only problem is that in CLWE the $\mathbf{\deltay}$ ’s need to be distributed according to a standard Gaussian, but here the $\mathbf{\deltay}$ ’s are distributed according to a discrete Gaussian over $L$ . To complete the transformation into bonafide CLWE samples, we add Gaussian noise of appropriate variance to both $\mathbf{\deltay}$ and $\langle y,w\rangle$ (and rescale $\mathbf{\deltay}$ so that it is distributed according to the standard Gaussian distribution). We then apply the search $\mathrm{CLWE}_{\beta,\gamma}$ oracle on these CLWE samples to recover $\mathbf{\nabla}^{\prime}\mathbf{w}$ and thereby solve $\mathrm{BDD}_{L^{*},\gamma/r}$ .  

As mentioned previously, our main result actually uses a decision CLWE oracle, which does not recover the secret $\mathbf{\nabla}^{\prime}\mathbf{w}$ immediately. Working with this decision oracle requires some care. To that end, our proof will incorporate the “oracle hidden center” finding procedure from [ PRS17 ], the details of which can be found in Section 3.3 .  

# 2 Preliminaries  

Definition 2.1 (Statistical distance) .For two distributions $\mathcal{D}_{1}$ and $D_{2}$ over $\mathbb{R}^{n}$ with density functions $\phi_{1}$ and $\phi_{2}$ , respectively, we define the statistical distance between them as  

$$
\Delta(\mathcal{D}_{1},\mathcal{D}_{2})=\frac{1}{2}\int_{\mathbb{R}^{n}}|\phi_{1}(\pmb{x})-\phi_{2}(\pmb{x})|d\pmb{x}\;.
$$  

We denote the statistical distance by $\Delta(\phi_{1},\phi_{2})$ if only the density functions are specified. Moreover, for random variables $X_{1}\sim\mathcal{D}_{1}$ and $X_{2}\sim\mathcal{D}_{2}$ , we also denote $\Delta(X_{1},X_{2})=\Delta(\mathcal{D}_{1},\mathcal{D}_{2})$ .One important fact is that applying (possibly a randomized) function cannot increase statistical distance, i.e., for random variables $X,Y$ and function $f$ ,  

$$
\Delta(f(X),f(Y))\leq\Delta(X,Y)~.
$$  

We define the advantage of an algorithm $\mathcal{A}$ solving the decision problem of distinguishing two distributions $D_{n}$ and $\mathcal{D}_{n}^{\prime}$ parameterized by $n$ as  

$$
\left|\operatorname*{Pr}_{x\sim\mathcal{D}_{n}}[A(x)=\mathrm{YES}]-\operatorname*{Pr}_{x\sim\mathcal{D}_{n}^{\prime}}[A(x)=\mathrm{YES}]\right|\,.
$$  

Moreover, we define the advantage of an algorithm $\boldsymbol{\mathcal{A}}$ solving the average-case decision problem of distinguishing two distributions $\mathcal{D}_{n,s}$ and $\mathcal{D}_{n,s}^{\prime}$ parameterized by $n$ and $s$ , where $s$ is equipped with some distribution $S_{n}$ , as  

$$
\left|\operatorname*{Pr}_{s\sim S_{n}}[A^{\mathcal{B}_{n,s}}(1^{n})=\mathrm{YES}]-\operatorname*{Pr}_{s\sim S_{n}}[A^{\mathcal{B}_{n,s}^{\prime}}(1^{n})=\mathrm{YES}]\right|\,,
$$  

where $B_{n,s}$ and $B_{n,s}$ are respectively the sampling oracles of $\mathcal{D}_{n,s}$ and $\mathcal{D}_{n,s}^{\prime}$ . We say that an algorithm $\mathcal{A}$ has non-negligible advantage if its advantage is a non-negligible function in $n$ , i.e., a function in $\Omega(n^{-c})$ for some constant $c>0$ .  

# 2.1 Lattices and Gaussians  

Lattices. A lattice is a discrete additive subgroup of $\mathbb{R}^{n}$ . Unless specified otherwise, we assume all lattices are full rank, i.e., their linear span is $\mathbb{R}^{n}$ . For an $n$ -dimensional lattice $L$ , a set of linearly independent vectors $\{b_{1},\ldots,b_{n}\}$ is called a basis of $L$ if $L$ is generated by the set, i.e., $L=B\mathbb{Z}^{n}$ where $B=[\pmb{b}_{1},\dots,\pmb{b}_{n}]$ . The determinant of a lattice $L$ with basis $B$ is defined as $\operatorname*{det}(L)=|\operatorname*{det}(B)|$ ;it is easy to verify that the determinant does not depend on the choice of basis.  

The dual lattice of a lattice $L$ , denoted by $L^{*}$ , is defined as  

$$
L^{*}=\left\{\pmb{y}\in\mathbb{R}^{n}\mid\langle\pmb{x},\pmb{y}\rangle\in\mathbb{Z}\mathrm{~for~all~}\pmb{x}\in L\right\}.
$$  

If $B$ is a basis of $L$ then $(B^{T})^{-1}$ is a basis of $L^{*}$ ; in particular, $\operatorname*{det}(L^{*})=\operatorname*{det}(L)^{-1}$ .  

Definition 2.2. For an $n$ -dimensional lattice $L$ and $1\leq i\leq n$ , the $i$ -th successive minimum of $L$ is defined as  

$$
\lambda_{i}(L)=\operatorname*{inf}\{r\mid\dim(\operatorname{span}(L\cap{\overline{{B}}}(\mathbf{0},r)))\geq i\}~,
$$  

where ${\overline{{B}}}(\mathbf{0},r)$ is the closed ball of radius $r$ centered at the origin.  

We define the function $\rho_{s}(\pmb{x})=\exp(-\pi\|\pmb{x}/s\|^{2})$ . Note that $\rho_{s}(\pmb{x})/s^{n}$ , where $n$ is the dimension of $\mathbf{\nabla}_{\mathbf{x}}$ , is the probability density of the Gaussian distribution with covariance $s^{2}/(2\pi)\cdot I_{n}$ .  

Definition 2.3 (Discrete Gaussian) .For lattice $L\subset\mathbb{R}^{n}$ , vector $\pmb{y}\in\mathbb{R}^{n}$ , and parameter $r>0$ , the discrete Gaussian distribution $\boldsymbol{D}_{y+L,r}$ on coset $y+L$ with width $r$ is defined to have support $y+L$ and probability mass function proportional to $\rho_{r}$ .  

For ${\pmb y}={\bf0}$ , we simply denote the discrete Gaussian distribution on lattice $L$ with width $r$ by $D_{L,r}$ . Abusing notation, we denote the $n$ -dimensional continuous Gaussian distribution with zero mean and isotropic variance $r^{2}/(2\pi)$ as $D_{\mathbb{R}^{n},r}$ . Finally, we omit the subscript $r$ when $r\,=\,1$ and refer to $D_{\mathbb{R}^{n}}$ as the standard Gaussian (despite it having covariance $I_{n}/(2\pi))$ ).  

([ Pei1 $r_{1},r_{2}>0$ and vectors $\mathbf{x},c_{1},c_{2}\in\mathbb{R}^{n}$ , let r$r_{0}\,=\,\sqrt{r_{1}^{2}+r_{2}^{2}}$ p,$r_{3}=r_{1}r_{2}/r_{0}$ , and c$c_{3}=(r_{3}/r_{1})^{2}c_{1}+(r_{3}/r_{2})^{2}c_{2}$ . Then  

$$
\rho_{r_{1}}({\pmb x}-{\pmb c}_{1})\cdot\rho_{r_{2}}({\pmb x}-{\pmb c}_{2})=\rho_{r_{0}}({\pmb c}_{1}-{\pmb c}_{2})\cdot\rho_{r_{3}}({\pmb x}-{\pmb c}_{3})~.
$$  

Fourier analysis. We briefly review basic tools of Fourier analysis required later on. The Fourier transform of a function $f:\mathbb{R}^{n}\rightarrow\mathbb{C}$ is defined to be  

$$
\hat{f}(\pmb{w})=\int_{\mathbb{R}^{n}}f(\pmb{x})e^{-2\pi i\langle\pmb{x},\pmb{w}\rangle}d\pmb{x}\;.
$$  

An elementary property of the Fourier transform is that if $f(\boldsymbol{w})=g(\boldsymbol{w}+\boldsymbol{v})$ for some $\pmb{v}\in\mathbb{R}^{n}$ ,then $\hat{f}(\pmb{w})=e^{2\pi i\langle\pmb{v},\pmb{w}\rangle}\hat{g}(\pmb{w})$ . Another important fact is that the Fourier transform of a Gaussian is also a Gaussian, i.e., ${\hat{\rho}}=\rho$ ; more generally, $\hat{\rho}_{s}\,=\,s^{\prime\prime}\rho_{1/s}$ . We also exploit the Poisson summation formula sta discrete set A d below. Note that we denote by .$\textstyle f(A)\,=\,\sum_{\mathbf{x}\in A}f(\mathbf{x})$ ∈) for any function $f$ and any Lemma 2.5 (Poisson summation formula) .For any lattice $L$ and any function $f$ ,  

$$
f(L)=\operatorname*{det}(L^{*})\cdot{\widehat{f}}(L^{*})~.
$$  

Smoothing parameter. An important lattice parameter induced by discrete Gaussian which will repeatedly appear in our work is the smoothing parameter , defined as follows.  

Definition 2.6 (Smoothing parameter) .For lattice $L$ and real $\varepsilon\,>\,0$ , we define the smoothing parameter $\eta_{\varepsilon}(L)$ as  

$$
\eta_{\varepsilon}(L)=\operatorname*{inf}\{s\mid\rho_{1/s}(L^{*}\setminus\{\mathbf{0}\})\leq\varepsilon\}~.
$$  

Intuitively, this parameter is the width beyond which the discrete Gaussian distribution behaves like a continuous Gaussian. This is formalized in the lemmas below.  

satisfying Lemma 2.7 $r s/t\;\geq\;\eta_{\varepsilon}(L)$ ([ Reg05 , Claim 3.9]) for some .For any $\varepsilon\ <\ \textstyle{\frac{1}{2}}$ , where $n$ -dim t$t\;=\;{\sqrt{r^{2}+s^{2}}}$ √, the statistical distance between e$L$ , vector $\pmb{u}\in\mathbb{R}^{n}$ , and $r,s>0$ $D_{u+L,r}+D_{\mathbb{R}^{n},s}$ and $D_{\mathbb{R}^{n},t}$ is at most $4\varepsilon$ .  

Lemma 2.8 ([ PRS17 , Lemma 2.5]) .For any $n$ -dimensional lattice $L$ , real $\varepsilon>0$ , and $r\ge\eta_{\varepsilon}(L)$ ,the statistical distance between $D_{\mathbb{R}^{n},r}$ mod $L$ and the uniform distribution over $\mathbb{R}^{n}/L$ is at most $\varepsilon/2$ .  

Lemma 2.7 states that if we take a sample from $D_{L,r}$ and add continuous Gaussian noise $D_{\mathbb{R}^{n},s}$ to the sample, the resulting distribution is statistically close to $D_{\mathbb{R}^{n},\sqrt{r^{2}+s^{2}}}$ , which is precisely what one gets by adding two continuous Gaussian distributions of width $r$ and $s$ . Unless specified otherwise, we always assume $\varepsilon$ is negligibly small in $n$ , say $\varepsilon=\exp(-n)$ . The following are some useful upper and lower bounds on the smoothing parameter $\eta_{\varepsilon}(L)$ .  

Lemma 2.9 ([ PRS17 , Lemma 2.6]) .For any $n$ -dimensional lattice $L$ and $\varepsilon=\exp(-c^{2}n)$ ,  

$$
\eta_{\varepsilon}(L)\leq c\sqrt{n}/\lambda_{1}(L^{*})\;.
$$  

Lemma 2.10 ([ MR07 , Lemma 3.3]) .For any $n$ -dimensional lattice $L$ and $\varepsilon>0$ ,  

$$
\eta_{\varepsilon}(L)\leq\sqrt{\frac{\ln(2n(1+1/\varepsilon))}{\pi}}\cdot\lambda_{n}(L)\;.
$$  

Lemma 2.11 ([ Reg05 , Claim 2.13]) .For any $n$ -dimensional lattice $L$ and $\varepsilon>0$ ,  

$$
\eta_{\varepsilon}(L)\geq\sqrt{\frac{\ln{1/\varepsilon}}{\pi}}\cdot\frac{1}{\lambda_{1}(L^{*})}\;.
$$  

Computational problems. GapSVP and SIVP are among the main computational problems on lattices and are believed to be computationally hard (even with quantum computation) for polynomial approximation factor $\alpha(n)$ . We also define two additional problems, DGS and BDD.  

Definition 2.12 (GapSVP) .For an approximation factor $\alpha=\alpha(n)$ , an instance of GapSVP $_\alpha$ is given by an $n$ -dimensional lattice $L$ and a number $d>0$ . In YES instances, $\lambda_{1}(L)\leq d$ , whereas in NO instances, $\lambda_{1}(L)>\alpha\cdot d$ .  

Definition 2.13 (SIVP) .For an approximation factor $\alpha=\alpha(n)$ , an instance of SIVP $\alpha$ is given by an $n$ -dimensional lattice $L$ . The goal is to output a set of $n$ linearly independent lattice vectors of length at most $\alpha\cdot\lambda_{n}(L)$ .  

Definition 2.14 (DGS) .For a function $\varphi$ that maps lattices to non-negative reals, an instance of $\mathrm{DGS}_{\varphi}$ is given by a lattice $L$ and a parameter $r\,\geq\,\varphi(L)$ . The goal is to output an independent sample whose distribution is within negligible statistical distance of $D_{L,r}$ .  

Definition 2.15 (BDD) .For an $n$ -dimensional lattice $L$ and distance bound $d>0$ , an instance of $\mathrm{BDD}_{L,d}$ is given by a vector $\pmb{t}=\pmb{w}+\pmb{u}$ , where $\pmb{u}\in L$ and $\|w\|\leq d$ . The goal is to output $\mathbf{\nabla}w$ .  

# 2.2 Learning with errors  

We now define the learning with errors (LWE) problem. This definition will not be used in the sequel, and is included for completeness. Let $n$ and $q$ be positive integers, and $\alpha>0$ an error rate. We denote the quotient ring of integers modulo $q$ as $\mathbb{Z}_{q}=\mathbb{Z}/q\mathbb{Z}$ and quotient group of reals modulo the integers as $\mathbb{T}=\mathbb{R}/\mathbb{Z}=[0,1)$ .  

Definition 2.16 (LWE distribution) .For integer $q\geq2$ and vector $s\in\mathbb{Z}_{q}^{n}$ , the LWE distribution $A_{s,\alpha}$ over $\mathbb{Z}_{q}^{n}\times\mathbb{T}$ independently choosing uniformly random $\pmb{a}\in\mathbb{Z}_{q}^{n}$ and $e\sim D_{\mathbb{R},\alpha}$ ,and outputting ($(\pmb{a},(\langle\pmb{a},\pmb{s}\rangle/q+e)$ ⟨⟩) mod 1) .  

Definition 2.17. For an integer $q=q(n)\ge2$ and error parameter $\alpha=\alpha(n)>0$ , the average-case decision problem $\mathrm{LWE}_{q,\alpha}$ is to distinguish the following two distributions over $\mathbb{Z}_{q}^{n}\times\mathbb{T}$ : (1) the LWE distribution $A_{s,\alpha}$ for some uniformly random $s\,\in\,\mathbb{Z}_{q}^{n}$ (which is fixed for all samples), or (2) the uniform distribution.  

# 2.3 Continuous learning with errors  

We now define the CLWE distribution, which is the central subject of our analysis.  

Definition 2.18 (CLWE distributi For unit vector $\pmb{w}\in\mathbb{R}^{n}$ and parameters $\beta,\gamma>0$ , define the CLWE distribution $A_{w,\beta,\gamma}$ over R$\mathbb{R}^{n+1}$ to have density at $(y,z)$ proportional to  

$$
\rho(\pmb{y})\cdot\sum_{k\in\mathbb{Z}}\rho_{\beta}(z+k-\gamma\langle\pmb{y},\pmb{w}\rangle)\ .
$$  

Equivalently, a sample $(y,z)$ from the CLWE distribution $A_{w,\beta,\gamma}$ is given by the $(n+1)$ -dimensional vector $(y,z)$ where $y\sim D_{\mathbb{R}^{n}}$ and $z=(\gamma\langle{\pmb y},{\pmb w}\rangle+e)$ mod 1 where $e\sim D_{\mathbb{R},\beta}$ . The vector $\mathbf{\delta}^{\prime}\mathbf{\delta}w$ is the hidden direction, $\gamma$ is the density of layers, and $\beta$ is the noise added to each equation. From the CLWE distribution, we can arrive at the homogeneous CLWE distribution by conditioning on $z=0$ . A formal definition is given as follows.  

Definition 2.19 (Homogeneous CLWE distribution) .For unit vector $\pmb{w}\ \in\ \mathbb{R}^{n}$ and parameters $\beta,\gamma>0$ , define the homogeneous CLWE distribution $H_{w,\beta,\gamma}$ over $\mathbb{R}^{n}$ to have density at $\mathbf{\deltay}$ proportional to  

$$
\rho(\pmb{y})\cdot\sum_{\pmb{k}\in\mathbb{Z}}\rho_{\beta}(\pmb{k}-\gamma\langle\pmb{y},\pmb{w}\rangle)~.
$$  

The homogeneous CLWE distribution can be equivalently defined as a mixture of Gaussians. To see this, notice that Eq. ( 1 ) is equal to  

$$
\sum_{k\in\mathbb{Z}}\rho_{\sqrt{\beta^{2}+\gamma^{2}}}(k)\cdot\rho(\pi_{{\pmb w}^{\perp}}({\pmb y}))\cdot\rho_{\beta/\sqrt{\beta^{2}+\gamma^{2}}}\Big(\langle{\pmb y},{\pmb w}\rangle-\frac{\gamma}{\beta^{2}+\gamma^{2}}k\Big)\;,
$$  

where a mixture of Gaussian components of width $\pi_{w^{\perp}}$ denotes the projection on the orthogonal space to $\beta/\sqrt{\beta^{2}+\gamma^{2}}$ p(which is roughly $\mathbf{\nabla}^{\prime}\mathbf{w}$ . Hence, $H_{w,\beta,\gamma}$ $\beta/\gamma$ for can be viewed as $\beta\ll\gamma$ ) in the secret direction, and width 1 in the orthogonal space. The components are equally spaced, with a separation of $\gamma/(\beta^{2}+\gamma^{2})$ between them (which is roughly $1/\gamma$ for $\beta\ll\gamma$ ).  

We remark that the integral of ( 1 ) (or equivalently, of ( 2 )) over all $\mathbf{\deltay}$ is  

$$
Z=\frac{\beta}{\sqrt{\beta^{2}+\gamma^{2}}}\cdot\rho\left(\frac{1}{\sqrt{\beta^{2}+\gamma^{2}}}\mathbb{Z}\right)\,.
$$  

$\beta/\sqrt{\beta^{2}+\gamma^{2}}$ This is easy to see since the integral over pindependently of $k$ .$\textit{\textbf{y}}$ of the product of the last two $\rho$ terms in ( 2 ) is Definition 2.20. For parameters $\beta,\gamma~>~0$ , the average-case decision problem CLWE $\beta,\gamma$ is to distinguish the following two distributions over $\mathbb{R}^{n}\times\mathbb{T}$ :$(\boldsymbol{1})$ the CLWE distribution $A_{w,\beta,\gamma}$ for some uniformly random unit vector $\pmb{w}\in\mathbb{R}^{n}$ (which is fixed for all samples), or (2) $D_{\mathbb{R}^{n}}\times U$ .  

Definition 2.21. For parameters $\beta,\gamma\,>\,0$ , the average-case decision problem $\mathrm{hCLWE}_{\beta,\gamma}$ is to distinguish the following two distributions over $\mathbb{R}^{n}$ : (1) the homogeneous CLWE distribution $H_{w,\beta,\gamma}$ for some uniformly random unit vector $\pmb{w}\in\mathbb{R}^{n}$ (which is fixed for all samples), or (2) $D_{\mathbb{R}^{n}}$ .  

Note that $\mathrm{CLWE}_{\beta,\gamma}$ and $\mathrm{hCLWE}_{\beta,\gamma}$ are defined as average-case problems. We could have equally well defined them to be worst-case problems, requiring the algorithm to distinguish the distributions for all hidden directions $\pmb{w}\in\mathbb{R}^{n}$ . The following claim shows that the two formulations are equivalent.  

Claim 2.22. For any $\beta,\gamma>0$ , there is a polynomial-time reduction from worst-case CLWE $\beta,\gamma$ to (average-case) $\mathrm{CLWE}_{\beta,\gamma}$ .  

Proof. Given CLWE samples $\{(\boldsymbol{y}_{i},\boldsymbol{z}_{i})\}_{i=1}^{\mathrm{poly}(n)}$ from $A_{w,\beta,\gamma}$ , we apply a random rotation $\boldsymbol{R}$ , giving us samples of the form $\{(\pmb{R}\pmb{y}_{i},z_{i}\}_{i=1}^{\mathrm{poly}(n)}$ . Since the standard Gaussian is rotationally invariant and $\langle\pmb{y},\pmb{w}\rangle=\langle\pmb{R}\pmb{y},\pmb{R}^{l}\pmb{w}\rangle$ , the rotated CLWE sa s are distributed according to $A_{R^{T}{\pmb w},\beta,\gamma}$ . Since $\boldsymbol{R}$ is a random rotation, the random direction R$R^{T}w$ is uniformly distributed on the sphere.  

# 3 Hardness of CLWE  

# 3.1 Background and overview  

In this section, we give an overview of the quantum reduction from worst-case lattice problems to CLWE. Our goal is to show that we can efficiently solve worst-case lattice problems, in particular GapSVP and SIVP, using an oracle for CLWE (and with quantum computation). We first state our main theorem, which was stated informally as Theorem 1.1 in the introduction.  

Theorem 3.1. Let $\beta\,=\,\beta(n)\,\in\,(0,1)$ and $\!\!\!\!\!\!\!\gamma\,=\,\gamma(n)\,\geq\,2{\sqrt{n}}$ be such that $\gamma/\beta$ is polynomially bounded. Then there is a polynomial-time quantum reduction from $\mathrm{DGS}_{2\sqrt{n}\eta_{\varepsilon}(L)/\beta}$ to $\mathrm{CLWE}_{\beta,\gamma}$ .  

Using standard reductions from GapSVP and SIVP to DGS (see, e.g., [ Reg05 , Section 3.3]), our main theorem immediately implies the following corollary.  

Corollary 3.2. Let $\beta=\beta(n)\in(0,1)$ and $\gamma=\gamma(n)\geq2{\sqrt{n}}$ such that $\gamma/\beta$ is polynomially bounded. Then, there is a polynomial-time quantum reduction from $\mathrm{SIVP}_{\alpha}$ and $\mathrm{GapSVP}_{\alpha}$ to $\mathrm{CLWE}_{\beta,\gamma}$ for some $\alpha=\tilde{O}(n/\beta)$ .  

Based on previous work, to prove Theorem 3.1 , it suffices to prove the following lemma, which is the goal of this section.  

Lemma 3.3. Let $\beta\,=\,\beta(n)\,\in\,(0,1)$ and $\!\!\!\!\!\!\!\gamma\,=\,\gamma(n)\,\geq\,2{\sqrt{n}}$ such that $q\,=\,\gamma/\beta$ is polynomially bounded. There exists a probabilistic polynomial-time (classical) algorithm with access to an oracle that solves $\mathrm{CLWE}_{\beta,\gamma}$ , that takes as input a lattice $L\subset\mathbb{R}^{n}$ , parameters $\beta,\gamma$ , and $r\geq2q\cdot\eta_{\varepsilon}(L)$ , and $\mathrm{poly}(n)$ mles f ete Gaussian distribution $D_{L,r_{i}}$ for poly $(n)$ parameters $r_{i}\geq r$ and solves $\mathrm{BDD}_{L^{*},d}$ for $d=\gamma/({\sqrt{2}}r)$ .  

In other words, we can implement an oracle for BD $^{\prime\!\scriptscriptstyle D}L^{*},\!\gamma/(\sqrt{2}r)$ √using polynomially many discrete Gaussian samples and the CLWE oracle as a sub-routine. The proof of Lemma 3.3 will be given in Section 3.2 (which is the novel contribution) and Section 3.3 (which mainly follows [ PRS17 ]).  

In the rest of this subsection, we briefly explain how Theorem 3.1 follows from Lemma 3.3 .This derivation is already implicit in past work [ PRS17 ,Reg05 ], and is included here mainly for completeness. Readers familiar with the reduction may skip directly to Section 3.2 .  

The basic idea is to start with samples from a very wide discrete Gaussian (which can be efficiently sampled) and then iteratively sample from narrower discrete Gaussians, until eventually we end up with short discrete Gaussian samples, as required (see Figure 3 ). Each iteration consists of two steps: the first classical step is given by Lemma 3.3 , allowing us to solve BDD on the dual lattice; the second step is quantum and is given in Lemma 3.4 below, which shows that solving BDD leads to sampling from narrower discrete Gaussian.  

![](images/3e8802c261b916e371f7cdde1b5fdc085f257bdb8f6c9b610c47bc3205ea0313.jpg)  
Figure 3: Two iterations of the reduction.  

Lemma 3.4 ([ Reg05 , Lemma 3.14]) .There exists an efficient quantum algorithm that, given any $n$ -dimensional lattice $L$ , a number $d<\,\lambda_{1}(L^{*})/2$ , and an oracle that solves BDD $L^{*},d$ , outputs a sample from $D_{L,\sqrt{n}/(\sqrt{2}d)}$ .  

Similar to [ PRS17 ], there is a subtle requirement in Lemma 3.3 that we need discrete Gaussian samples from several different parameters $r^{\prime}\geq r$ . However, this is a non-issue since an oracle for $\mathrm{BDD}_{L^{*},\gamma/(\sqrt{2}r)}$ also solves BD $^{\cal J}L^{*},\gamma/(\sqrt{2}r^{\prime})$ √for any $r^{\prime}\geq r$ , so Lemma 3.4 in fact allows us to efficiently sample from $D_{L,r^{\prime}\sqrt{n}/\gamma}$ for any $r^{\prime}\geq r$ .  

# 3.2 CLWE samples from BDD  

In this subsection we prove Lemma 3.5 , showing how to generate CLWE samples from the given BDD instance using discrete Gaussian samples. In the next subsection we will show how to solve the BDD instance by applying the decision CLWE oracle to these samples, thereby completing the proof of Lemma 3.3 .  

$\varepsilon<\,\textstyle{\frac{1}{2}}$ vector Lemma 3.5. ${\pmb w}+{\pmb u}$ and t$t=\sqrt{r^{2}+s_{1}^{2}}$ where There is an efficient algorithm that takes as input an p$\pmb{u}\in L^{*}$ , and samples from , reals $r,s_{1},s_{2}>0$ such that $D_{L,r}$ , and outputs samples that are within statistical $r s_{1}/\sqrt{\|\pmb{w}\|^{2}(r s_{1}/s_{2})^{2}+t^{2}}\geq\eta_{\varepsilon}(L)$ p∥∥$n$ -dimensional lattice ≥for some $L$ , a distance 8 εof the CLWE distribution $A_{w^{\prime},\beta,\gamma}$ for ${\pmb w}^{\prime}={\pmb w}/\|{\pmb w}\|$ ,$\beta\,=\,\|\pmb{w}\|\sqrt{(r s_{1}/t)^{2}+(s_{2}/\|\pmb{w}\|)^{2}}$ ∥∥pand $\gamma=\lVert\pmb{w}\rVert r^{2}/t$ .  

Proof. We start by describing the algorithm. For each $\textbf{\em x}$ from the given samples from $D_{L,r}$ , do the following. First, take the inner product $\langle\pmb{x},\pmb{w}+\pmb{u}\rangle$ , which gives us  

$$
\langle{\pmb x},{\pmb w}+{\pmb u}\rangle=\langle{\pmb x},{\pmb w}\rangle{\bmod{1}}\ .
$$  

Appending this inner product modulo 1 to the sample $\textbf{\em x}$ , we get $(\boldsymbol{x},\langle\boldsymbol{x},\boldsymbol{w}\rangle$ mod 1). Next, we “smooth out” the lattice structure of $\mathbf{\nabla_{\mathcal{N}}}$ by adding Gaussian noise $\pmb{v}\sim D_{\mathbb{R}^{n},s_{1}}$ to $\mathbf{\nabla}_{\mathbf{x}}$ and $e\sim D_{\mathbb{R},s_{2}}$ to $\langle x,w\rangle$ (modulo 1). Then, we have  

$$
(\pmb{x}+\pmb{v},(\langle\pmb{x},\pmb{w}\rangle+e)~\mathrm{mod}~1)~.
$$  

Finally, we normalize the first component by $t$ so that its marginal distribution has unit width, giving us  

$$
((\pmb{x}+\pmb{v})/t,(\langle\pmb{x},\pmb{w}\rangle+e)\bmod1)\ ,
$$  

which the algorithm outputs.  

Our goal is to show that the distribution of ( 5 ) is within statistical distance $8\varepsilon$ of the CLWE distribution $A_{w^{\prime},\beta,\gamma}$ , given by  

$$
\left({\pmb y}^{\prime},(\gamma\langle{\pmb y}^{\prime},{\pmb w}^{\prime}\rangle+e^{\prime})~\mathrm{mod}~1\right)\,,
$$  

where $\pmb{y}^{\prime}\sim D_{\mathbb{R}^{n}}$ and $e^{\prime}\sim D_{\mathbb{R},\beta}$ . Because applying a function cannot increase statistical distance (specifically, dividing the first component by $t$ and taking mod 1 of the second), it suffices to show that the distribution of  

$$
(x+v,\langle x,w\rangle+e)\;,
$$  

is within statistical distance $8\varepsilon$ of that of  

$$
({\pmb y},(r/t)^{2}\langle{\pmb y},{\pmb w}\rangle+e^{\prime})\;,
$$  

where $\pmb{y}\sim D_{\mathbb{R}^{n},t}$ and $e^{\prime}\sim D_{\mathbb{R},\beta}$ . First, observe that by Lemma 2.7 , the statistical distance between the marginals on the first component (i.e., between $\mathbf{\boldsymbol{x}}+\mathbf{\boldsymbol{v}}$ and $\mathbf{\deltay}$ ) is at most $4\varepsilon$ . It is therefore sufficient to bound the statistical distance between the second components conditioned on any fixed value $\overline{y}$ of the first component. Conditioned on the first component being $\overline{y}$ , the second component in ( 6 ) has the same distribution as  

$$
\langle{\pmb x}+{\pmb h},{\pmb w}\rangle
$$  

where $h\sim D_{\mathbb{R}^{n},s_{2}/\|\pmb{w}\|}$ , and the second component in ( 7 ) has the same distribution as  

$$
\langle(r/t)^{2}\overline{{y}}+h^{\prime},w\rangle
$$  

where $h^{\prime}\sim D_{\mathbb{R}^{n},\beta/\|\pmb{w}\|}$ .  

By Claim 3.6 below, conditioned on $\pmb{x}+\pmb{v}=\overline{{\pmb{y}}}$ , the distribution of $\mathbf{\nabla}_{\mathbf{x}}$ is $(r/t)^{2}{\overline{{\pmb{y}}}}\!+\!D_{L-(r/t)^{2}{\overline{{\pmb{y}}}},r s_{1}/t}$ .Therefore, by Lemma 2.7 , the conditional distribution of $x+h$ given $\pmb{x}+\pmb{v}=\pmb{y}$ is within statistical distance $4\varepsilon$ of that of $(r/t)^{2}{\overline{{y}}}+h^{\prime}$ . Since statistical distance cannot increase by applying a function (inner product with $\mathbf{\nabla}^{\prime}\mathbf{w}$ in this case), ( 8 ) is within statistical distance $4\varepsilon$ of ( 9 ). Hence, the distribution of ( 6 ) is within statistical distance $8\varepsilon$ of that of ( 7 ).  

Claim 3.6. Let ${\pmb y}={\pmb x}+{\pmb v}$ , where $\pmb{x}\sim D_{L,r}$ and $\pmb{v}\sim D_{\mathbb{R}^{n},s}$ Then, the conditional distribution of $\textbf{\em x}$ given $\pmb{y}=\overline{{\pmb{y}}}$ is $(r/t)^{2}{\overline{{\pmb{y}}}}+D_{L-(r/t)^{2}{\overline{{\pmb{y}}}},r s/t}$ where t$t=\sqrt{r^{2}+s^{2}}$ .  

Proof. Observe that $\textbf{\em x}$ conditioned on $\pmb{y}=\mpb{\overline{{y}}}$ is a discrete random variable supported on $L$ . The probability of $\textbf{\em x}$ given $\pmb{y}=\overline{{\pmb{y}}}$ is proportional to  

$$
\rho_{r}({\pmb x})\cdot\rho_{s}(\overline{{{\pmb y}}}-{\pmb x})=\rho_{t}(\overline{{{\pmb y}}})\cdot\rho_{r s/t}({\pmb x}-(r/t)^{2}\overline{{{\pmb y}}})\propto\rho_{r s/t}({\pmb x}-(r/t)^{2}\overline{{{\pmb y}}})\;,
$$  

where the equality follows from Claim 2.4 . Hence, the conditional distribution of $x-(r/t)^{2}y$ given $\pmb{y}=\overline{{\pmb{y}}}$ is $D_{L-(r/t)^{2}\overline{{{y}}},r s/t}$ .  

# 3.3 Solving BDD with the CLWE oracle  

In this subsection, we complete the proof of Lemma 3.3 . We first give some necessary background on the Oracle Hidden Center Problem (OHCP) [ PRS17 ]. The problem asks one to search for a “hidden center” $\boldsymbol{w}^{*}$ using a decision oracle whose acceptance probability depends only on the distance to $\boldsymbol{w}^{*}$ . The problem’s precise statement is as follows.  

Definition 3.7 (OHCP) .For parameters $\varepsilon,\delta\in[0,1)$ and $\zeta\geq1$ , the $(\varepsilon,\delta,\zeta)$ -OHCP is an approximate search problem that tries to find the “hidden” center $\boldsymbol{w}^{*}$ . Given a scale parameter $d>0$ and access to a randomized oracle $\mathcal{O}:\mathbb{R}^{n}\times\mathbb{R}^{\geq0}\rightarrow\{0,1\}$ such that its acceptance probability $p(\pmb{w},t)$ only depends on $\exp(t)\|\pmb{w}-\pmb{w}^{*}\|$ for some (unknown) “hidden center” $\pmb{w}^{\ast}\in\mathbb{R}^{n}$ with $\delta d\leq\|\pmb{w}^{*}\|\leq d$ and for any $\pmb{w}\in\mathbb{R}^{n}$ with $\|\pmb{w}-\pmb{w}^{*}\|\leq\zeta d$ , the goal is to output ws.t. $\|\pmb{w}-\pmb{w}^{*}\|\leq\varepsilon d$ .  

Notice that OHCP corresponds to our problem since we want to solve BDD, which is equivalent to finding the “hidden” offset vector $\boldsymbol{w}^{*}$ , using a decision oracle for $\mathrm{CLWE}_{\beta,\gamma}$ . The acceptance probability of the $\mathrm{CLWE}_{\beta,\gamma}$ oracle will depend on the distance between our guess $\mathbf{\nabla}w$ and the true offset $\boldsymbol{w}^{*}$ . For OHCP, we have the following result from [ PRS17 ].  

Lemma 3.8 ([ PRS17 ], Proposition 4.4) .There is a poly $(\kappa,n)$ -time algorithm that takes as input a confidence parameter $\kappa\geq20\log(n{+}1)$ (and the scale parameter $d>0$ ) and solves $(\exp(-\kappa),\exp(-\kappa),1+$ $1/\kappa)$ )-OHCP in dimension nexcept with probability $\exp(-\kappa)$ , provided that the oracle $\mathcal{O}$ co nding to the OHCP instance satisfies the following conditions. For some $p(\infty)\in[0,1]$ and t$t^{*}\geq0$ ≥,  

1. $p(\mathbf{0},t^{*})-p(\infty)\geq1/\kappa$ ;  
2. $|p(\mathbf{0},t)-p(\infty)|\leq2\exp(-t/\kappa)$ for any $t\geq0$ ; and   
3. $p(\pmb{w},t)$ is $\kappa$ -Lipschitz in $t$ for any $\pmb{w}\in\mathbb{R}^{n}$ such that $\|\pmb{w}\|\leq(1+1/\kappa)d$ .  

Furthermore, each of the algorithm’s oracle calls takes the form $O(\cdot,i\Delta)$ for some $\Delta\ <\ 1$ that depends only on $\kappa$ and $n$ and $0\leq i\leq\mathrm{poly}(\kappa,n)$ .  

The main idea in the proof of Lemma 3.8 is performing a guided random walk with advice from the decision oracle $\mathcal{O}$ . The decision oracle $\mathcal{O}$ rejects a random step with high probability if it increases the distance $\lVert\boldsymbol{w}-\boldsymbol{w}^{*}\rVert$ . Moreover, there is non-negligible probability of decreasing the distance by a factor $\exp(1/n)$ unless $\log\|\pmb{w}-\pmb{w}^{*}\|\leq-\kappa$ . Hence, with sufficiently many steps, the random walk will reach w, a guess of the hidden center, which is within $\exp(-\kappa)$ distance to $\boldsymbol{w^{*}}$ with high probability.  

Our goal is to show that we can construct an oracle $\mathcal{O}$ satisfying the above conditions using an oracle for CLWE $\beta,\gamma$ . Then, it follows from Lemma 3.8 that BDD with discrete Gaussian samples can be solved using an oracle for CLWE. We first state some lemmas useful for our proof. Lemma 3.9 is Babai’s closest plane algorithm and Lemma 3.10 is an upper bound on the statistical distance between two one-dimensional Gaussian distributions.  

Lemma 3.9 ([ LLL82 ,Bab86 ]) .For any $n$ -dimensional lattice $L$ , there is an efficient algorithm that solves $\mathrm{BDD}_{L,d}$ for $d=2^{-n/2}\cdot\lambda_{1}(L)$ .  

Lemma 3.10 ([ DMR18 , Theorem 1.3]) .For all $\mu_{1},\mu_{2}\in\mathbb{R}$ , and $\sigma_{1},\sigma_{2}>0$ , we have  

$$
\Delta\big({\mathcal N}(\mu_{1},\sigma_{1}),\!{\mathcal N}(\mu_{2},\sigma_{2})\big)\leq\frac{3|\sigma_{1}^{2}-\sigma_{2}^{2}|}{2\operatorname*{max}(\sigma_{1}^{2},\sigma_{2}^{2})}+\frac{|\mu_{1}-\mu_{2}|}{2\operatorname*{max}(\sigma_{1},\sigma_{2})}\;,
$$  

where ${\mathcal{N}}(\mu,\sigma)$ denotes the Gaussian distribution with mean $\mu$ and standard deviation $\sigma$ .  

Now, we prove Lemma 3.3 , restated below.  

Lemma 3.3 .Let $\beta\,=\,\beta(n)\,\in\,(0,1)$ and $\!\!\!\!\!\!\!\!\gamma\,=\,\gamma(n)\,\geq\,2{\sqrt{n}}$ such that $q\,=\,\gamma/\beta$ is polynomially bounded. There exists a probabilistic polynomial-time (classical) algorithm with access to an oracle that solves $\mathrm{CLWE}_{\beta,\gamma}$ , that takes as input a lattice $L\subset\mathbb{R}^{n}$ , parameters $\beta,\gamma$ , and $r\geq2q\cdot\eta_{\varepsilon}(L)$ , and $\mathrm{poly}(n)$ mles f ete Gaussian distribution $D_{L,r_{i}}$ for poly( n)parameters $r_{i}\geq r$ and solves $\mathrm{BDD}_{L^{*},d}$ for $d=\gamma/({\sqrt{2}}r)$ .  

Proof. Let $d^{\prime}\,=\,\left(1\,-\,1/(2n)\right)\,\cdot\,d$ .By [ LM09 , Corollary 2], it suffices to solve $\mathrm{BDD}_{L^{*},d^{\prime}}$ .Let $\kappa\,=\,\mathrm{poly}(n)$ with $\kappa\,\geq\,8q n\ell$ be such that the advantage of our $\mathrm{CLWE}_{\beta,\gamma}$ oracle is at least $1/\kappa$ ,where $\ell\geq1$ is the number of samples required by the oracle.  

Given as input a lattice $L\,\subset\,\mathbb{R}^{n}$ , a parameter $r\,\geq\,2q\cdot\eta_{\varepsilon}(L)$ , samples from $D_{L,r_{i}}$ for $1\,\leq$ $i\leq\mathrm{poly}(n)$ , and a BDD instance $\boldsymbol{w}^{*}+\boldsymbol{u}$ where $\pmb{u}\in\mathbb{L}^{*}$ and $\|\pmb{w}^{*}\|\leq d^{\prime}$ , we want to recover w∗.Without loss of generality, we can assume th $\|w^{*}\|\geq\exp(-n/2)\cdot\lambda_{1}(L^{*})\geq(2q/r)\cdot\exp(-n/2)$ (Lemma 2.11 ), since we can otherwise find w∗efficiently using Babai’s closest plane algorithm (Lemma 3.9 ).  

We will use the CLWE oracle to simulate an oracle $\mathcal{O}:\mathbb{R}^{n}\times\mathbb{R}^{\geq0}\,\rightarrow\,\{0,1\}$ such that the probability that ${\mathcal{O}}({\boldsymbol{w}},t)$ outputs 1 (“accepts”) only depends on $\exp(t)\|\pmb{w}-\pmb{w}^{*}\|$ . Our oracle $\mathcal{O}$ corresponds to the oracle in Definition 3.7 with $\boldsymbol{w}^{*}$ as the “hidden center”. We will use Lemma 3.8 to find $\boldsymbol{w}^{*}$ .  

On input $\mathbf{\Pi}(\pmb{w},t)$ , our oracle $\mathcal{O}$ receives $\ell$ independent samples from $D_{L,\exp(t)r}$ . Then, w generate CLWE samples using the procedure from Lemma 3.5 . The procedure takes as input these ℓsamples, the vector $\pmb{u}+\pmb{w}^{*}-\pmb{w}$ where $\pmb{u}\in L^{*}$ , and parameters $\exp(t)r,\exp(t)s_{1},s_{2}$ . Our choice of $s_{1}$ and $s_{2}$ will be specified below. Note that the CLWE oracle requires the “hidden direction” $(\pmb{w}-\pmb{w}^{*})/\|\pmb{w}-$ $\pmb{w}^{*}\|$ to be uniformly distributed on the unit sphere. To this end, we apply the worst-toerage case reduction from Claim 2.22 . Let $S_{w,t}$ be the resulting CLWE distribution. Our oracle Othen calls the $\mathrm{CLWE}_{\beta,\gamma}$ oracle on $S_{\pmb{w},t}^{\ell}$ and outputs 1 if and only if it accepts.  

Using the oracle $\mathcal{O}$ 3.8 with confide ce parameter $\kappa$ and scale para $d^{\prime}$ he output of me approximation bto the oracle’s “hidden center” with the guarantee that ∥$\|\widehat{\pmb{w}}-\pmb{w}^{*}\|\leq\exp(-\kappa)d^{\prime}$ b−∥≤ −. Finally, running Babai’s algorithm on the vector u$\boldsymbol{u}+\boldsymbol{w}^{*}-\boldsymbol{\widehat{w}}$ −will give us w∗exactly since  

$$
\begin{array}{r}{\|\widehat{\pmb{w}}-\pmb{w}^{*}\|\leq\exp(-\kappa)d\leq\beta\exp(-\kappa)/\eta_{\varepsilon}(L)\leq2^{-n}\lambda_{1}(L^{*})\;,}\end{array}
$$  

where the last inequality is from Lemma 2.9 .  

The running time of the above procedure is clearly polynomial in $n$ . It remains to check that our oracle $\mathcal{O}$ (1) is a valid instance of $(\exp(-\kappa),\exp(-\kappa),1+1/\kappa)$ -OHCP with hidden center $\boldsymbol{w^{*}}$ and (2) satisfies all the conditions of Lemma 3.8 . First, note that $S_{w,t}$ will be negligibly close in statistical distance to the CLWE distribution with parameters  

$$
\begin{array}{r l}&{\beta^{\prime}=\sqrt{(\exp(t)\|\pmb{w}-\pmb{w}^{*}\|)^{2}s_{1}^{\prime2}+s_{2}^{2}}\,,}\\ &{\gamma^{\prime}=\exp(t)\|\pmb{w}-\pmb{w}^{*}\|r^{\prime}\,,}\end{array}
$$  

where Lemma $r^{\prime}\;=\;r^{2}/\sqrt{r^{2}+s_{1}^{2}}$ 3.5 . Then, we set p$s_{1}=r/(\sqrt{2}q)$ and $s_{1}^{\prime}\ =\ r s_{1}/\sqrt{r^{2}+s_{1}^{2}}$ √) and choose p$s_{2}$ as long as such that $r,s_{1},s_{2}$ satisfy the conditions of  

$$
s_{2}^{2}=\beta^{2}-(s_{1}^{\prime}/r^{\prime})^{2}\gamma^{2}=\beta^{2}-(s_{1}/r)^{2}\gamma^{2}=\beta^{2}/2\;.
$$  

and Lemma $s_{1}\;\geq\;\sqrt{2}\,\cdot\,\eta_{\varepsilon}(L)$ 3.5 √·requires ), so it remains to determine a sufficient condition for the aforementioned $r s_{1}/\sqrt{r^{2}\|{\pmb w}-{\pmb w}^{*}\|^{2}(s_{1}/s_{2})^{2}+r^{2}+s_{1}^{2}}\geq\eta_{\varepsilon}(L)$ p≥). We know that $r\geq2q\cdot\eta_{\varepsilon}(L)$ inequality. Obs $\mathbf{\nabla}w$ such that $\|\pmb{w}-\pmb{w}^{*}\|\,\leq\,d$ the condition $s_{2}\,\geq\,2d\cdot\eta_{\varepsilon}(L)$ is sufficient. Since $r\geq2(\gamma/\beta)\cdot\eta_{\varepsilon}(L)$ ≥·), this translates to $s_{2}\geq\beta/(\sqrt{2})$ ≥2). Hence, the transformation from Lemma 3.5 will output samples negligibly close to CLWE samples for our choice of $s_{1}$ and $s_{2}$ as long as $\|\pmb{w}-\pmb{w}^{*}\|\leq d$ (beyond the BDD distance bound $d^{\prime}$ ).  

Since $S_{w,t}$ is negligibly close to the CLWE distribution, the acceptance probability $p(\pmb{w},t)$ of $\mathcal{O}$ only depends on $\exp(t)\|\pmb{w}\!-\!\pmb{w}^{*}\|$ . Moreover, by assumption $\|w^{*}\|\geq\exp(-n/2){\cdot}(2q/r)\geq\exp(-\kappa)d^{\prime}$ .Hence, $O,\kappa,d^{\prime}$ correspond to a valid instance of $(\exp(-\kappa),\exp(-\kappa),1+1/\kappa)$ -OHCP with “hidden center” $\boldsymbol{w}^{*}$ .  

Next, we show that $p(\pmb{w},t)$ of $\mathcal{O}$ satisfies all three conditions of Lemma 3.8 with $p(\infty)$ taken to be the acceptance probability of the CLWE oracle on samples from $D_{\mathbb{R}^{n}}\times U$ . Item 1 of Lemma 3.8 $s_{1}$ follows from our assumption that our , and $s_{2}$ , when $t^{*}=\log(\gamma/(\|\pmb{w}^{*}\|r^{\prime}))>\log(\sqrt{2})$ $\mathrm{CLWE}_{\beta,\gamma}$ √2), the generated CLWE samples satisfy oracle has advantage $1/\kappa$ , and by our choice of $\gamma^{\prime}(t^{*})=\gamma$ $r$ ,and $\beta^{\prime}(t^{*})=\beta$ . Hence, $p(\mathbf{0},t^{*})-p(\infty)\geq1/\kappa$ .  

We now show that Item 2 holds, which states that $|p(\mathbf{0},t)-p(\infty)|\leq2\exp(-t/\kappa)$ for any $t>0$ .We will show that $S_{\mathbf{0},t}$ converges exponentially fast to $D_{\mathbb{R}^{n}}\times U$ in statistical distance. Let $f(\pmb{y},z)$ be the probability density of $S_{\mathbf{0},t}$ . Then,  

$$
\begin{array}{l}{\displaystyle\Delta(S_{\bf0},t,D_{\mathbb{R}^{n}}\times U)=\frac12\int|f(z|{\pmb y})-U(z)|\rho({\pmb y})d{\pmb y}d z}\\ {\displaystyle\qquad\qquad\qquad\qquad=\frac12\int\Big(\int|f(z|{\pmb y})-U(z)|d z\Big)\rho({\pmb y})d{\pmb y}\;.}\end{array}
$$  

Hence, it suffices to show that the conditional density of $\mathcal{Z}$ given $\mathbf{\deltay}$ for $S_{\mathbf{0},t}$ converges exponentially fast to the uniform distribution on $\mathbb{T}$ . Notice that the conditional distribution of $z$ given $\mathbf{\deltay}$ is the Gaussian distribution with width parameter $\beta^{\prime}\geq\exp(t)\|\pmb{w}^{*}\|r/(2q)\geq\exp(t-n/2)$ , where we have used our assumption that $\|\pmb{w}^{*}\|\geq(2q/r)\!\cdot\!\exp(-n/2)$ . By Lemma 2.9 applied to $\mathbb{Z}$ , we know that $\beta^{\prime}$ is larger than $\eta_{\varepsilon}(\mathbb{Z})$ for $\varepsilon=\exp(-\exp(2t\!-\!n))$ . Hence, one sample from this conditional distribution is within statistical distance $\varepsilon$ of the uniform distribution by Lemma 2.8 . By the triangle inequality applied to $\ell$ samples,  

$$
\Delta\Big(S_{\mathbf{0},t}^{\ell},(D_{\mathbb{R}^{n}}\times U)^{\ell}\Big)\leq\operatorname*{min}(1,\ell\exp(-\exp(2t-n)))\leq2\exp(-t/\kappa)\;,
$$  

where in the last inequality, we use the the fact that we can choose $\kappa$ to be such that $2\exp(-t/\kappa)\geq1$ unless $t\ge\kappa/2$ . And when $t\geq\kappa/2\geq4q n\ell$ , we have $\ell\exp(-\exp(2t-n))\ll\exp(-t/\kappa)$ .  

It remains to verify Item 3, which states that $p(\pmb{w},t)$ is $\kappa$ -Lipschitz in $t$ for any $\lVert\pmb{w}\rVert\,\leq\,(1\,+$ $1/\kappa)d^{\prime}\leq d$ . We show this by boundi tatistical distance between $S_{w,t_{1}}$ and $S_{w,t_{2}}$ f$t_{1}\geq t_{2}$ .With a slight abuse in notation, let $f_{t_{i}}(y,z)$ ) be the probability density of $S_{w,t_{i}}$ and let ( $(\beta_{i},\gamma_{i})$ ) be the corresponding CLWE distribution parameters. For simplicity, also denote the hidden direction by $\pmb{w}^{\prime}=(\pmb{w}-\pmb{w}^{*})/\|\pmb{w}-\pmb{w}^{*}\|$ . Then,  

$$
\begin{array}{r l}{\Delta(f_{t_{1}},f_{t_{2}})=\cfrac{1}{2}\int\Big(\int|f_{t_{1}}(z|y)-f_{t_{2}}(z|y)|d z\Big)\rho(y)d y}\\ &{=\int\Delta\Big(N(\gamma_{1}\langle y,w^{\prime}\rangle,\beta_{1}/\sqrt{2\pi}),N(\gamma_{2}\langle y,w^{\prime}\rangle,\beta_{2}/\sqrt{2\pi})\Big)\rho(y)d y}\\ &{\leq\cfrac{1}{2}\int\Big(3(1-(\beta_{2}/\beta_{1})^{2})+\sqrt{2\pi}(\gamma_{1}-\gamma_{2})/\beta_{1}\cdot|\langle y,w^{\prime}\rangle|\Big)\cdot\rho(y)d y}\\ &{\leq\cfrac{\mathbb{E}}{y\sim\rho}[M(y)]\cdot\Big(1-\exp(-2(t_{1}-t_{2}))\Big)\mathrm{~where~}M(y)=\cfrac{1}{2}\Big(3+2\sqrt{\pi}q\cdot|\langle y,w^{\prime}\rangle|\Big)\times}\\ &{\leq\cfrac{\mathbb{E}}{y\sim\rho}[M(y)]\cdot2(t_{1}-t_{2})}\\ &{\leq(\kappa/\ell)\cdot(t_{1}-t_{2})\,,}\end{array}
$$  

where ( 10 ) follows from fact that $1-\exp(-2(t_{1}-t_{2}))\leq2(t_{1}-t_{2})$ , and (12 ) uses the fact that E$\mathbb{E}_{\pmb{y}\sim\rho}[M(\pmb{y})]\le4q\le\kappa/(2\ell)$ ∼≤≤). Using the triangle inequality over ℓsamples, the statistical distance between $S_{\pmb{w},t_{1}}^{\ell}$ and $S_{\pmb{w},t_{2}}^{\ell}$ is at most  

$$
\begin{array}{r}{\operatorname*{min}(1,\ell\cdot(\kappa/\ell)(t_{1}-t_{2}))\leq\kappa(t_{1}-t_{2})\;.}\end{array}
$$  

Therefore, $p(\pmb{w},t)$ is $\kappa$ -Lipschitz in $t$ .  

# 4 Hardness of Homogeneous CLWE  

In this section, we show the hardness of homogeneous CLWE by reducing from CLWE, whose hardness was established in the previous section. The main step of the reduction is to transform CLWE samples to homogeneous CLWE samples using rejection sampling (Lemma 4.1 ).  

Consider the samples $({\pmb y},z)\sim A_{{\pmb w},\beta,\gamma}$ in $\mathrm{CLWE}_{\beta,\gamma}$ . If we condition $\mathbf{\deltay}$ on $z\,=\,0$ (mod 1) then we get exactly samples $\pmb{y}\sim H_{w,\beta,\gamma}$ for $\mathrm{hCLWE}_{\beta,\gamma}$ . However, this approach is impractical as $z=0$ (mod 1) happens with probability 0. Instead we condition $\mathbf{\deltay}$ on $z\approx0$ (mod 1) somehow. One can imagine that the resulting samples $\mathbf{\deltay}$ will still have a “wavy” probability density in the direction of $\mathbf{\nabla}^{\prime}\mathbf{w}$ with spacing $1/\gamma$ , which accords with the picture of homogeneous CLWE. To avoid throwing away too many samples, we will do rejection sampling with some small “window” $\delta=1/\operatorname{poly}(n)$ .Formally, we have the following lemma.  

Lemma 4.1. There is $a$ poly $(n,1/\delta)$ -time probabilistic algorithm that takes as input a parameter $\delta\in(0,1)$ and samples from $A_{w,\beta,\gamma}$ , and outputs samples from $H_{w,\sqrt{\beta^{2}+\delta^{2}},\gamma}$ .  

Proof. Without loss of generality assume that $\pmb{w}=e_{1}$ . By definition, the probability density of sample $({\pmb y},z)\sim A_{{\pmb w},\beta,\gamma}$ is  

$$
p(\pmb{y},z)=\frac{1}{\beta}\cdot\rho(\pmb{y})\cdot\sum_{k\in\mathbb{Z}}\rho_{\beta}(z+k-\gamma y_{1})\ .
$$  

$\operatorname{sup}_{z\in\mathbb{T}}g_{0}(z)$ $g\,:\,\mathbb{T}\,\rightarrow\,[0,1]$ ∈). We perform rejection sampling on the samples ( e the function $g(z)\;=\;g_{0}(z)/M$ , where $\begin{array}{r}{g_{0}(z)\:=\:\sum_{k\in\mathbb{Z}}\rho_{\delta}(z+k)}\end{array}$ $(y,z)$ ) with acceptance probability ∈) and $M\,=$ $\mathrm{Pr}[\mathrm{accept}|{\pmb y},z]={g}(z)$ |). We remark that $g(z)$ ) is efficiently computable (see [ Bra+13 , Section 5.2]).  

The probability density of outputting $\mathbf{\deltay}$ and accept is  

$$
\begin{array}{l}{{\displaystyle\int_{\mathbb T}p({\pmb y},z)g(z)d z=\frac{\rho({\pmb y})}{\beta M}\cdot\int_{\mathbb T}\sum_{k_{1},k_{2}\in\mathbb Z}\rho_{\beta}(z+k_{1}-\gamma y_{1})\rho_{\delta}(z+k_{2})d z}\ ~}\\ {{\displaystyle~~~~~~~~~~~~~~~~~~=\frac{\rho({\pmb y})}{\beta M}\cdot\int_{\mathbb T}\sum_{k,k_{2}\in\mathbb Z}\rho\sqrt{\beta^{2}+\delta^{2}}(k-\gamma y_{1})\rho_{\beta\delta/\sqrt{\beta^{2}+\delta^{2}}}\Big(z+k_{2}+\frac{\delta^{2}(k-\gamma y_{1})}{\beta^{2}+\delta^{2}}\Big)}}\\ {{\displaystyle~~~~~~~~~~~~~=\frac{\delta}{M\sqrt{\beta^{2}+\delta^{2}}}\cdot\rho({\pmb y})\cdot\sum_{k\in\mathbb Z}\rho\sqrt{\beta^{2}+\delta^{2}}(k-\gamma y_{1})~,}}\end{array}
$$  

where the second equality follows from Claim 2.4 . This shows that the conditional distribution of $\mathbf{\deltay}$ upon acceptance is indeed $H_{e_{1},\sqrt{\beta^{2}+\delta^{2}},\gamma}$ . Moreover, a byproduct of this calculation is that the expected acceptance probability is $\mathrm{Pr}[\mathrm{accept}]=Z\delta/(M\sqrt{\beta^{2}+\delta^{2}})$ p), where, according to Eq. ( 3 ),  

$$
\begin{array}{r l}&{Z=\sqrt{\frac{\beta^{2}+\delta^{2}}{\beta^{2}+\delta^{2}+\gamma^{2}}}\cdot\rho_{\sqrt{\beta^{2}+\delta^{2}+\gamma^{2}}}(\mathbb{Z})}\\ &{\quad=\sqrt{\beta^{2}+\delta^{2}}\cdot\rho_{1/\sqrt{\beta^{2}+\delta^{2}+\gamma^{2}}}(\mathbb{Z})}\\ &{\quad\geq\sqrt{\beta^{2}+\delta^{2}}\ ,}\end{array}
$$  

and the second equality uses Lemma 2.5 . Observe that  

$$
\begin{array}{l}{\displaystyle g_{0}(z)=\sum_{k\in\mathbb{Z}}\rho_{\delta}(z+k)}\\ {\displaystyle\qquad\leq2\cdot\sum_{k=0}^{\infty}\rho_{\delta}(k)}\\ {\displaystyle\qquad<2\cdot\sum_{k=0}^{\infty}\exp(-\pi k)<4}\end{array}
$$  

since $\delta\,<\,1$ , implying that $M\leq4$ . Therefore, $\mathrm{Pr}[\mathrm{accept}]\,\geq\,\delta/4$ , and so the rejection sampling procedure has poly $(n,1/\delta)$ expected running time.  

The above lemma reduces CLWE to homogeneous CLWE with slightly worse parameters. Hence, $\beta/\sqrt{2})$ homogeneous CLWE is as hard as CLWE. Specifically, combining Theorem √2) and Lemma 4.1 (with $\delta$ also taken to be $\beta/{\sqrt{2}}$ √2), we obtain the following corollary. 3.1 (with $\beta$ taken to be  

Corollary 4.2. For any $\beta\,=\,\beta(n)\,\in\,(0,1)$ and $\gamma=\gamma(n)\,\geq\,2{\sqrt{n}}$ such that $\gamma/\beta$ is polynomially bounded, there is a polynomial-time quantum reduction from $\mathrm{DGS}_{2\sqrt{2n}\eta_{\varepsilon}(L)/\beta}$ to hCLWE $\beta,\gamma$ .  

# 5 Hardness of Density Estimation for Gaussian Mixtures  

In this section, we prove the hardness of density estimation for $k$ -mixtures of $n$ -dimensional Gaussians by showing a reduction from homogeneous CLWE. This answers an open question regarding its computational complexity [ Dia16 ,Moi18 ]. We first formally define density estimation for Gaussian mixtures.  

Definition 5.1 (Density estimation of Gaussian mixtures) .Let $\mathcal{G}_{n,k}$ be the family of $k$ -mixtu $n$ -dimensional Gaussians. The problem of density estimation for $\mathcal{G}_{n,k}$ is the following. Given $\delta>0$ and sample access to an unknown $P\in\mathcal G_{n,k}$ , with probability $9/10$ , output a hypothesis distribution $Q$ (in the form of an evaluation oracle) such that $\Delta(Q,P)\leq\delta$ .  

For our purposes, we fix the precision parameter $\delta$ to a very small constant, say, $\delta=10^{-3}$ . Now we show a reduction from $\mathrm{hCLWE}_{\beta,\gamma}$ to the problem of density estimation for Gaussian mixtures. Corollary 4.2 shows that $\mathrm{hCLWE}_{\beta,\gamma}$ is hard for $\gamma\geq2{\sqrt{n}}$ (assuming worst-case lattice problems are hard). Hence, by taking $\gamma=2\sqrt{n}$ and $g(n)=O(\log n)$ in Proposition 5.2 , we rule out the possibility of a poly $(n,k)$ -time density estimation algorithm for $\mathcal{G}_{n,k}$ under the same hardness assumption.  

Proposition 5.2. Let $\beta=\beta(n)\in(0,1/32)$ ,$\gamma=\gamma(n)\geq1$ , and $g(n)\ge4\pi$ . For $k=2\gamma\sqrt{g(n)/\pi}$ p,if there is an $\exp(g(n))$ -time algorithm that solves density estimation for $\mathcal{G}_{n,2k+1}$ , then there is $\footnote{I n t h i s p a p e r,w e h a v e a s s u m e d p e r f e c t C S I t o i n v e s t i g a t e t h e b e s t a c h i e v a b l e p e r f o r m a n c e.T h e p e r f o r m a n c e u n d e r i m p e r f e c t C S I i s w o r t h f u r t h e r i n v e s t i g a t i o n b y r e f e r i n g t o e x i s t i n g a n a l y t i c a l w o r k s~.}$ $O(\exp(g(n)))$ -time algorithm that solves hCLWE $\beta,\gamma$ .  

Proof. We apply the density estimation algorithm $\mathcal{A}$ to the unknown given distribution $P$ . As we will show below, with constant probability, it outputs a density estimate $f$ that satisfies $\Delta(f,P)<$ $2\delta=2{\cdot}10^{-3}$ $P=D_{\mathbb{R}^{n}}$ or not using the following procedure. We repeat the following procedure (and this is even though $H_{w,\beta,\gamma}$ has infinitely many components). We then test whether $m=1/(6\sqrt{\delta})$ √times. We draw $x\sim D_{\mathbb{R}^{n}}$ and check whether the following holds  

$$
\frac{f(\pmb{x})}{D(\pmb{x})}\in[1-\sqrt{\delta},1+\sqrt{\delta}]\;,
$$  

where $D$ denotes the density of $D_{\mathbb{R}^{n}}$ . We output $P=D_{\mathbb{R}^{n}}$ if Eq. ( 13 ) holds for all $m$ independent trials and $P=H_{w,\beta,\gamma}$ otherwise. Since $\Delta(H_{w,\beta,\gamma},D_{\mathbb{R}^{n}})>1/2$ (Claim 5.3 ), it is not hard to see that this test solves $\mathrm{hCLWE}_{\beta,\gamma}$ with probability at least $2/3$ (see [ RS09 , Observation 24] for a closely related statement). Moreover, the total running time is $O(\exp(g(n))$ since this test uses a constant number of samples.  

If $P\,=\,D_{\mathbb{R}^{n}}$ , it is obvious that $\mathcal{A}$ outputs a close density estimate with constant probability since $D_{\mathbb{R}^{n}}\in\mathcal{G}_{n,2k+1}$ . It remains to consider the case $P\,=\,H_{w,\beta,\gamma}$ . To this end, we observe that $H_{w,\beta,\gamma}$ is close to a $(2k+1)$ -mixture of Gaussians. Indeed, by Claim 5.4 below,  

$$
\Delta(H_{w,\beta,\gamma},H^{(k)})\leq2\exp(-\pi\cdot k^{2}/(\beta^{2}+\gamma^{2}))<2\exp(-\pi\cdot k^{2}/(2\gamma^{2}))\;,
$$  

where $H^{(k)}$ is the distribution given by truncating $H_{w,\beta,\gamma}$ to the $(2k+1)$ central mixture components. Hence, the statistical distance between the joint distribution of $\exp(g(n))$ samples from $H_{w,\beta,\gamma}$ and that of $\exp(g(n))$ samples from $H^{(k)}$ is bounded by  

$$
2\exp(-\pi\cdot k^{2}/(2\gamma^{2}))\cdot\exp(g(n))=2\exp(-g(n))\leq2\exp(-4\pi)\;.
$$  

Since the two distributions are statistically close, a standard argument shows that $\mathcal{A}$ will output $f$ satisfying $\Delta(f,H_{w,\beta,\gamma})\leq\Delta(f,H^{(k)})+\Delta(H^{(k)},H_{w,\beta,\gamma})<2\delta$ with constant probability. $\boxed{\begin{array}{r l}\end{array}}$  

Claim 5.3. Let $\beta=\beta(n)\in(0,1/32)$ and $\gamma=\gamma(n)\geq1$ . Then,  

$$
\Delta(H_{w,\beta,\gamma},D_{\mathbb{R}^{n}})>1/2\ .
$$  

Proof. Let $\gamma^{\prime}=\sqrt{\beta^{2}+\gamma^{2}}>\gamma$ p. Let $\pmb{y}\in\mathbb{R}^{n}$ be a random vector distributed according to $H_{w,\beta,\gamma}$ .Using the Gaussian mixture form of ( 2 ), we observe that $\langle y,w\rangle$ mod $\gamma/\gamma^{\prime2}$ is distributed according to $D_{\beta/\gamma^{\prime}}$ mod $\gamma/\gamma^{\prime2}$ . Since statistical distance cannot increase by applying a function (inner product with $\mathbf{\nabla}^{\prime}\mathbf{w}$ and then applying the modulo operation in this case), it suffices to lower bound the statistical distance between $D_{\beta/\gamma^{\prime}}$ mod $\gamma/\gamma^{\prime2}$ and $D$ mod $\gamma/\gamma^{\prime2}$ , where $D$ denotes the 1-dimensional standard Gaussian.  

By Chernoff, for all $\zeta>0$ , at least $1\!-\!\zeta$ mass of $D_{\beta/\gamma^{\prime}}$ is contained in $[-a\!\cdot\!(\beta/\gamma^{\prime}),a\!\cdot\!(\beta/\gamma^{\prime})]$ , where $a=\sqrt{\log(1/\zeta)}$ p). Hence, $D_{\beta/\gamma^{\prime}}$ mod $\gamma/\gamma^{\prime2}$ is at least $1-2a\beta\gamma^{\prime}/\gamma-\zeta$ far in statistical distance from the uniform distribution over $\mathbb{R}/(\gamma/\gamma^{\prime2})\mathbb{Z}$ , which we denote by $U$ . Moreover, by Lemma 2.8 and Lemma 2.9 ,$D$ mod $\gamma/\gamma^{\prime2}$ is within statistical distance $\varepsilon/2=\exp(-\gamma^{\prime4}/\gamma^{2})/2$ from $U$ . Therefore,  

$$
\begin{array}{r l}{\Delta(D_{\beta/\gamma^{\prime}}\,\mathrm{mod}\,\,\gamma/\gamma^{\prime2},D\,\mathrm{mod}\,\,\gamma/\gamma^{\prime2})\ge\Delta(D_{\beta/\gamma^{\prime}}\,\mathrm{mod}\,\,\gamma/\gamma^{\prime2},U)-\Delta(U,D\,\mathrm{mod}\,\,\gamma/\gamma^{\prime2})}&{}\\ {\ge1-2a\beta\gamma/\gamma-\zeta-\varepsilon/2}&{}\\ {>1-2\sqrt{2}a\beta-\zeta-\exp(-\gamma^{2})/2}&{>1/2\;,}\end{array}
$$  

where we set $\zeta=\exp(-2)$ and use the fact that $\beta\leq1/32$ and $\gamma\geq1$ in ( 14 ).  

Claim 5.4. Let $\beta=\beta(n)\in(0,1),\gamma=\gamma(n)\geq1$ , and $k\in\mathbb{Z}^{+}$ . Then,  

$$
\Delta(H_{w,\beta,\gamma},H^{(k)})\le2\exp(-\pi\cdot k^{2}/(\beta^{2}+\gamma^{2}))\;,
$$  

where $H^{(k)}$ is the distribution given by truncating $H_{w,\beta,\gamma}$ to the central $(2k+1)$ mixture components.  

Proof. We express $H_{w,\beta,\gamma}$ in its Gaussian mixture form given in Eq. ( 2 ) and define a random variable $X$ taking on values in $\mathbb{Z}$ such that the probability of $X=i$ is equal to the probability that a sample comes from the $i$ -th component in $H_{w,\beta,\gamma}$ . Then, we observe that $H^{(k)}$ is the distribution given by conditioning on $|X|\leq k$ . Since $X$ is a discrete Gaussian random variable with distribution $D_{\mathbb{Z},\sqrt{\beta^{2}+\gamma^{2}}}$ , we observe that $\operatorname*{Pr}[|X|>k]\le\varepsilon:=2\exp(-\pi\cdot k^{2}/(\beta^{2}+\gamma^{2}))$ by [ MP12 , Lemma 2.8]. Since conditioning on an event of probability $1-\varepsilon$ cannot change the statistical distance by more than $\varepsilon$ , we have  

$$
\begin{array}{r}{\Delta(H_{\pmb{w},\beta,\gamma},H^{(k)})\leq\varepsilon\;.}\end{array}
$$  

# 6 LLL Solves Noiseless CLWE  

The noiseless CLWE problem ( $\beta=0$ ) can be solved in polynomial time using LLL. This applies both to the homogeneous and the inhomogeneous versions, as well as to the search version. The argument can be extended to the case of exponentially small $\beta>0$ .  

The key idea is to take samples $(y_{i},z_{i})$ , and find integer coefficients $c_{1},\ldots,c_{m}$ such that $\pmb{y}=$ $\textstyle\sum_{i=1}^{m}c_{i}\pmb{y}_{i}$ is short, say $\|y\|\ll1/\gamma$ . By Cauchy-Schwarz, we then have that $\begin{array}{r}{\gamma\langle\pmb{y},\pmb{w}\rangle=\sum_{i=1}^{m}c_{i}z_{i}}\end{array}$ over the reals (not modulo 1!). This is formalized in Theorem 6.2 . We first state Minkowski’s Convex Body Theorem, which we will use in the proof of our procedure.  

Lemma 6.1 ([ Min10 ]) .Let $L$ be a full-rank $n$ -dimensional lattice. Then, for any centrallysymmetric convex set $S$ , if $\operatorname{vol}(S)>2^{n}\cdot|\operatorname*{det}(L)|$ , then $S$ contains a non-zero lattice point.  

Theorem 6.2. Let $\gamma=\gamma(n)$ be a polynomial in $n$ . Then, there exists a polynomial-time algorithm for solving $\mathrm{CLWE}_{0,\gamma}$ .  

Proof. Take $n+1$ CLWE samples $\{(y_{i},z_{i})\}_{i=1}^{n+1}$ and consider the matrix  

$$
Y=\left[\!\!\begin{array}{c c c c}{{\pmb y_{1}}}&{{\cdots}}&{{\pmb y_{n}}}&{{\pmb y_{n+1}}}\\ {{0}}&{{\dots}}&{{0}}&{{\delta}}\end{array}\!\!\right]\ ,
$$  

where $\delta=2^{-3n^{2}}$ .  

Consider the lattice $L$ generated by the columns of $Y$ . Since $\mathbf{\nabla}^{\!3}d_{2}$ ’s are drawn from the Gaussian distribution, $L$ is full rank. By Hadamard’s inequality, and the fact that with probability exponentially close to $1$ ,$\lVert y_{i}\rVert\leq{\sqrt{n}}$ for all $i$ , we have  

$$
|\operatorname*{det}(L)|\leq\delta\cdot n^{n/2}<2^{-2n^{2}}\ .
$$  

Now consider the $n$ -dimensional cube $S$ centered at 0 with side length $2^{-n}$ . Then, $\mathrm{vol}(S)=2^{-n^{2}}$ ,and by Lemma 6.1 ,$L$ contains a vector $\pmb{v}$ satisfying $\|\pmb{v}\|_{\infty}\leq2^{-n}$ and so $\|\pmb{v}\|_{2}\leq\sqrt{n}\cdot2^{-n}$ . Applying the LLL algorithm [ LLL82 ] gives us an integer combination of the columns of Ywhose length is within $2^{(n+1)/2}$ factor of the shortest vector in $L$ , which will therefore have $\ell_{2}$ norm less than ${\sqrt{n}}\cdot2^{-(n-1)/2}$ . Let $\mathbf{\deltay}$ be the corresponding combination of the $\mathbf{\nabla}^{\!3}d_{2}$ vectors (which is equivalently given by the first $n$ coordinates of the output of LLL) and $z\in(-1/2,1/2]$ a representative of the corresponding integer combination of t $z_{i}$ Then, we have $\|y\|_{2}\,\leq\,{\sqrt{n}}\,\cdot\,2^{-(n-1)/2}$ and therefore we obtain the linear equation $\gamma\cdot\langle\pmb{y},\pmb{w}\rangle=z$ · ⟨ ⟩over the reals (without mod 1).  

We now repeat the above procedure $n$ times, and recover $\mathbf{\delta}^{\prime}\mathbf{\delta}w$ by solving the resulting $n$ linear equations. It remains to argue why the $n$ vectors $\textit{\textbf{y}}$ we collect are linearly independent. First, note that the output $\mathbf{\deltay}$ is guaranteed to be a non-zero vector since with probability 1, no integer combination of the Gaussian distributed $\mathbf{\nabla}^{\!3}d_{\i}$ is ${\mathbf0}$ . Next, note that LLL is equivariant to rotations, i.e., if we rotate the input basis then the output vector will also be rotated by the same rotation. Moreover, spherical Gaussians are rotationally invariant. Hence, the distribution of the output vector $\pmb{y}\in\mathbb{R}^{n}$ is also rotationally invariant. Therefore, repeating the above procedure $n$ times will give us $n$ linearly independent vectors.  

# 7 Subexponential Algorithm for Homogeneous CLWE  

For $\gamma\,=\,o({\sqrt{n}})$ , the covariance matrix will reveal the discrete structure of homogeneous CLWE, which will lead to a subexponential time algorithm for the problem. This clarifies why the hardness results of homogeneous CLWE do not extend beyond $\gamma\geq2{\sqrt{n}}$ .  

We define noiseless homogeneous CLWE distribution $H_{w,\gamma}$ as $H_{w,\beta,\gamma}$ with $\beta\,=\,0$ . We begin with a claim that will allow us to focus on the noiseless case.  

the resulting distribution is Claim 7.1. By adding Gaussian noise $H_{w,\tilde{\beta},\tilde{\gamma}}$ , where $D_{\mathbb{R}^{n},\beta/\gamma}$ $\tilde{\gamma}=\gamma/\sqrt{1+(\beta/\gamma)^{2}}$ to $H_{w,\gamma}$ pand then rescaling by a factor of and $\tilde{\beta}=\tilde{\gamma}(\beta/\gamma)$ .$\gamma/\sqrt{\beta^{2}+\gamma^{2}}$ p,  

Proof. Without loss of generality, suppose $\pmb{w}=e_{1}$  

Let $z\sim H_{{\pmb w},\gamma}+D_{\mathbb{R}^{n},\beta/\gamma}$ and $\tilde{z}=\gamma z/\sqrt{\beta^{2}+\gamma^{2}}$ p. It is easy to verify that the marginals density of $\widetilde{z}$ on subspace $e_{1}^{\perp}$ will simply be $\rho$ . Hence we focus on calculating the density of $z_{1}$ and $\tilde{z}_{1}$ . The  

density can be computed by convolving the probability densities of $H_{w,\gamma}$ and $D_{\mathbb{R}^{n},\beta/\gamma}$ as follows.  

$$
\begin{array}{r l}&{H_{w,\gamma}*D_{\mathbb{R}^{n},\beta/\gamma}(z_{1})\propto\displaystyle\sum_{k\in\mathbb{Z}}\rho(k/\gamma)\cdot\rho_{\beta/\gamma}(z_{1}-k/\gamma)}\\ &{\qquad\qquad\qquad\qquad=\rho\sqrt{\beta^{2}+\gamma^{2}}/\gamma^{\big(z_{1}\big)}\cdot\displaystyle\sum_{k\in\mathbb{Z}}\rho_{\beta/\sqrt{\beta^{2}+\gamma^{2}}}\Big(k/\gamma-\frac{\gamma^{2}}{\beta^{2}+\gamma^{2}}z_{1}\Big)}\\ &{\qquad\qquad\qquad=\rho(\tilde{z}_{1})\cdot\displaystyle\sum_{k\in\mathbb{Z}}\rho_{\tilde{\beta}}\Big(k-\tilde{\gamma}\tilde{z}_{1}\Big)~,}\end{array}
$$  

where the second to last equality follows from Claim 2.4 . This verifies that the resulting distribution is indeed $H_{w,\tilde{\beta},\tilde{\gamma}}$ .  

Claim 7.1 implies an homogeneous CLWE distribution with $\beta>0$ is equivalent to a noiseless homogeneous CLWE distribution with independent Gaussian noise added. We will first analyze the noiseless case and then derive the covariance of noisy (i.e., $\beta>0$ ) case by adding independent Gaussian noise and rescaling.  

Lemma 7.2. Let $\Sigma\ \succ\ 0$ be the covariance matrix of the $n$ -dimensional noiseless homogeneous CLWE distribution $H_{w,\gamma}$ with $\gamma\geq1$ . Then,  

$$
\left\|\Sigma-\frac{1}{2\pi}I_{n}\right\|\geq\gamma^{2}\exp(-\pi\gamma^{2})\;,
$$  

where $\|\cdot\|$ denotes the spectral norm.  

Proof. Without los nerality, $w\,=\,e_{1}$ $H_{\mathbf{w},\gamma}\,=\,D_{L}\,\times\,D_{\mathbb{R}^{n-1}}$ where $L$ is the onedimensional lattice (1 $(1/\gamma)\mathbb{Z}$ . Then, Σ = diag( $\Sigma=\mathrm{diag}(\mathbb{E}_{x\sim D_{L}}[x^{2}],\frac{1}{2\pi},\dots,\frac{1}{2\pi})$ ∼), so it suffices to show that  

$$
\Big|\sum_{x\sim D_{L}}[x^{2}]-\frac{1}{2\pi}\Big|\geq\gamma^{2}\exp(-\pi\gamma^{2})\ .
$$  

Define $g(x)=x^{2}\cdot\rho(x)$ . The Fourier transform of $\rho$ is itself; the Fourier transform of $g$ is given by  

$$
\widehat{g}(y)=\Big({\frac{1}{2\pi}}-y^{2}\Big)\rho(y)\;.
$$  

By definition and Poisson’s summation formula (Lemma 2.5 ), we have  

$$
\begin{array}{r l}{\lefteqn{\mathbb{E}_{x\sim D_{L}}[x^{2}]=\frac{g(L)}{\rho(L)}}}\\ &{\quad=\frac{\operatorname*{det}(L^{*})\cdot\widehat{g}(L^{*})}{\operatorname*{det}(L^{*})\cdot\rho(L^{*})}=\frac{\widehat{g}(L^{*})}{\rho(L^{*})}\;,}\end{array}
$$  

where $L^{*}=((1/\gamma)\mathbb{Z})^{*}=\gamma\mathbb{Z}$ . Combining this with the expression for $\widehat{g}$ , we have  

$$
\begin{array}{r l r}{\lefteqn{\left|\L{\mathbb{E}}_{x\sim D_{L}}[x^{2}]-\frac{1}{2\pi}\right|=\frac{\sum_{y\in L^{*}}y^{2}\rho(y)}{1+\rho(L^{*}\setminus\{0\})}}}\\ &{}&{\geq\gamma^{2}\exp(-\pi\gamma^{2})\ ,}\end{array}
$$  

where we use the fact that for $\gamma\geq1$ ,  

$$
\rho(\gamma\mathbb{Z}\setminus\{0\})\le\rho(\mathbb{Z}\setminus\{0\})<2\sum_{k=1}^{\infty}\exp(-\pi k)=\frac{2\exp(-\pi)}{1-\exp(-\pi)}<1\;.
$$  

Combining Claim 7.1 and Lemma 7.2 , we get the following corollary for the noisy case.  

Corollary 7.3. Let $\Sigma\succ0$ be the covariance matrix of $n$ -dimensional homogeneous CLWE distribution $H_{w,\beta,\gamma}$ with $\gamma\geq1$ and $\beta>0$ . Then,  

$$
\left\|\Sigma-\frac{1}{2\pi}I_{n}\right\|\geq\gamma^{2}\exp(-\pi(\beta^{2}+\gamma^{2}))\mathrm{~,~}
$$  

where $\|\cdot\|$ denotes the spectral norm.  

Gaussian noise of width Proof. Using Claim 7.1 , we can view samples from $\beta^{\prime}/\gamma^{\prime}$ added and rescaled by $H_{w,\beta,\gamma}$ $\gamma^{\prime}/\sqrt{\beta^{\prime2}+\gamma^{\prime2}}$ pas samples from , where $H_{w,\gamma^{\prime}}$ $\beta^{\prime},\gamma^{\prime}$ with independent are given by  

$$
\begin{array}{l}{{\beta^{\prime}=\beta\sqrt{1+(\beta/\gamma)^{2}}\;,}}\\ {{\gamma^{\prime}=\sqrt{\beta^{2}+\gamma^{2}}\;.}}\end{array}
$$  

Let $\Sigma$ be the covariance of $H_{w,\beta,\gamma}$ and let $\Sigma_{0}$ be the covariance of $H_{{\pmb w},\gamma^{\prime}}$ . Since the Gaussian noise added to $H_{w,\gamma^{\prime}}$ is independent and $\beta^{\prime}/\gamma^{\prime}=\beta/\gamma$ ,  

$$
\Sigma=\frac{1}{1+(\beta/\gamma)^{2}}\Big(\Sigma_{0}+\frac{(\beta/\gamma)^{2}}{2\pi}I_{n}\Big)\ .
$$  

Hence,  

$$
\begin{array}{l}{\displaystyle\left\|\Sigma-\frac{1}{2\pi}I_{n}\right\|=\frac{1}{1+(\beta/\gamma)^{2}}\Big\|\Big(\Sigma_{0}+\frac{(\beta/\gamma)^{2}}{2\pi}I_{n}\Big)-\frac{1+(\beta/\gamma)^{2}}{2\pi}I_{n}\Big\|}\\ {\displaystyle=\frac{1}{1+(\beta/\gamma)^{2}}\Big\|\Sigma_{0}-\frac{1}{2\pi}I_{n}\Big\|}\\ {\displaystyle\geq\gamma^{2}\exp(-\pi(\beta^{2}+\gamma^{2}))\;.}\end{array}
$$  

where the last inequality follows from Lemma 7.2 .  

We use the following lemma, which provides an upper bound on the error in estimating the covariance matrix by samples. The sub-gaussian norm of a random variable $Y$ is defined as $\|Y\|_{\psi_{2}}=$ $\operatorname*{inf}\{t\;>\;0\;\;|\;\;\mathbb{E}[\exp(Y^{2}/t^{2})]\;\le\;2\}$ and that of an $n$ -dimensional random vector $\mathbf{\deltay}$ is defined as $\begin{array}{r}{\|\pmb{y}\|_{\psi_{2}}=\operatorname*{sup}_{\pmb{u}\in\mathbb{S}^{n-1}}\|\langle\pmb{y},\pmb{u}\rangle\|_{\psi_{2}}}\end{array}$ .  

Lemma 7.4 ([ Ver18 , Theorem 4.6.1]) .Let $A$ be an $m\times n$ matrix whose rows $A_{i}$ are independent, mean zero, sub-gaussian isotropic random vectors in R$\mathbb{R}^{n}$ . Then for any $u\geq0$ we have  

$$
\Big\|\frac{1}{m}A^{T}A-I_{n}\Big\|\leq K^{2}\operatorname*{max}(\delta,\delta^{2})\ \ w h e r e\ \delta=C\Big(\sqrt{\frac{n}{m}}+\frac{u}{\sqrt{m}}\Big)\ ,
$$  

with probability at least $1-2e^{-u^{2}}$ for some constant $C>0$ . Here, $K=\operatorname*{max}_{i}\|A_{i}\|_{\psi_{i}}$ .  

Combining Corollary 7.3 and Lemma 7.4 , we have the following theorem for distinguishing homogeneous CLWE distribution and Gaussian distribution.  

Theorem 7.5. Let $\gamma=n^{\varepsilon}$ , where $\varepsilon<1/2$ is a constant, and let $\beta=\beta(n)\in(0,1)$ . Then, there exists an $\exp(O(n^{2\varepsilon}))$ )-time algorithm that solves hCLWE $\beta,\gamma$ .  

Proof. Our algorithm takes ${\boldsymbol{\mathit{m}}}$ samples from the unknown input distribution $P$ and computes the sample covariance matrix $\Sigma_{m}=(1/m)A^{T}A$ , where $A$ ’s rows are the samples, and its eigenvalues $\mu_{1},\ldots,\mu_{n}$ . Then, it determines whether $P$ is a homogeneous CLWE distribution or not by testing that  

$$
\left|\mu_{i}-{\frac{1}{2\pi}}\right|\leq{\frac{1}{2}}\cdot\gamma^{2}\exp(-\pi(\beta^{2}+\gamma^{2}))\;{\mathrm{~for~all~}}i\in[n]\;.
$$  

The running time of this algorithm is $O(n^{2}m)=\exp(O(n^{2\varepsilon}))$ . To show correctness, we first con(after rescaling by sider the case $P=D_{\mathbb{R}^{n}}$ $1/(2\pi)$ . The standard Gaussian distribution satisfies the conditions of Lemma ). Hence, the eigenvalues of $\Sigma_{m}$ will be within distance $O({\sqrt{n/m}})$ p) from 7.4 $1/(2\pi)$ with high probability.  

Now consider the case $P\,=\,H_{w,\beta,\gamma}$ . We can assume $\pmb{w}\,=\,e_{1}$ without loss of generality since eigenvalues are invariant under rotations. Denote by $\mathbf{\deltay}$ a random vector distributed according to $H_{w,\beta,\gamma}$ and $\sigma^{2}=\mathbb{E}_{\mathbf{y}\sim H_{w,\beta,\gamma}}[y_{1}^{2}]$ ]. The covariance of $P$ is given by  

$$
\Sigma=\binom{\sigma^{2}}{\mathbf{0}}_{\quad\frac{1}{2\pi}I_{n-1}}\mathbf{\Sigma}.
$$  

Now consider the sample covariance $\Sigma_{m}$ of $P$ and denote by $\begin{array}{r}{\sigma_{m}^{2}\,=\,{\pmb w}^{T}\Sigma_{m}{\pmb w}\,=\,(1/m)\sum_{i=1}^{m}A_{i1}^{2}}\end{array}$ P.Since $A_{i1}$ ’s are sub-gaussian random variables [ MP12 , Lemma 2.8], $\sigma_{m}^{2}-\sigma^{2}$ −is a sum of mindependent, mean-zero, sub-exponential random variables. For Corollary 2.8.3] implies that $|\sigma_{m}^{2}-\sigma^{2}|=O(\sqrt{n/m})$ p) with high probability. By Corollary $m=\omega(n)$ ), Bernstein’s inequality [ 7.3 Ver18 , we ,know that  

$$
\left|\sigma^{2}-\frac{1}{2\pi}\right|\geq\gamma^{2}\exp(-\pi(\beta^{2}+\gamma^{2}))\;.
$$  

Hence, if we choose $m=\exp(c\gamma^{2})$ ) with some sufficiently large constant $c$ , then $\Sigma_{m}$ will have an eigenvalue that is noticeably far from $1/(2\pi)$ with high probability.  

# 8 SQ Lower Bound for Homogeneous CLWE  

Statistical Query (SQ) algorithms [ Kea98 ] are a restricted class of algorithms that are only allowed to query expectations of functions of the input distribution without directly accessing individual samples. To be more precise, SQ algorithms access the input distribution indirectly via the STAT $(\tau)$ oracle, which given a query function $f$ and data distribution $D$ , returns a value contained in the interval $\mathbb{E}_{x\sim D}[f(x)]+[-\tau,\tau]$ for some precision parameter $\tau$ .  

In this section, we prove SQ hardness of distinguishing homogeneous CLWE distributions from the standard Gaussian. In particular, we show that SQ algorithms that solve homogeneous CLWE require super-polynomial number of queries even with super-polynomial precision. This is formalized in Theorem 8.1 .  

Theorem 8.1. Let $\beta=\beta(n)\in(0,1)$ and $\gamma=\gamma(n)\geq\sqrt{2}$ √. Then, any (randomized) SQ algorithm with precision $\tau\geq4\cdot\exp(-\pi\cdot\gamma^{2}/4)$ that successfully solves hCLWE $\beta,\gamma$ with probability $\eta>1/2$ requires at least $(2\eta-1)\cdot\exp(c n)\cdot\tau^{2}\beta^{2}/(4\gamma^{2})$ statistical queries of precision $\tau$ for some constant $c>0$ .  

Note that when $\gamma=\Omega({\sqrt{n}})$ and $\gamma/\beta=\mathrm{poly}(n)$ , even exponential precision $\tau=\exp(-O(n))$ results in a query lower bound that grows as $\exp({\tilde{\Omega}}(n))$ . This establishes an unconditional hardness result for SQ algorithms in the parameter regime $\gamma=\Omega({\sqrt{n}})$ , which is consistent with our computational hardness result based on worst-case lattice problems. The uniform spacing in homogeneous CLWE distributions gives us tight control over their pairwise correlation (see definition in ( 16 )), which leads to a simple proof of the SQ lower bound.  

We first provide some necessary background on the SQ framework. We denote by $B(\mathcal{U},D)$ the decision problem in which the input di ion $P$ either equals $D$ or belo gs to $\boldsymbol{\mathcal{U}}$ , and the goal of the algorithm is to identify whether P$P=D$ or $P\in\mathcal{U}$ . For our purposes, Dwill be the standard Gaussian $D_{\mathbb{R}^{n}}$ and $\boldsymbol{\mathcal{U}}$ will be a finite set of homogeneous CLWE distributions. Abusing notation, we denote by $D(x)$ the density of $D$ . Following [ Fel+17 ], we define the pairwise correlation between two distributions $P,Q$ relative to $D$ as  

$$
\chi_{D}(P,Q):=\mathbb{E}_{x\sim D}\left[\left(\frac{P(x)}{D(x)}-1\right)\cdot\left(\frac{Q(x)}{D(x)}-1\right)\right]=\mathbb{E}_{x\sim D}\left[\frac{P(x)Q(x)}{D(x)^{2}}\right]-1\;.
$$  

Lemma 8.2 below establishes a lower bound on the number of statistical queries required to solve $B(\mathcal{U},D)$ in terms of pairwise correlation between distributions in $\boldsymbol{\mathcal{U}}$ .  

Lemma 8.2 ([ Fel+17 , Lemma 3.10]) .Let $D$ be a distribution and $\mathcal{U}$ be a set of distributions both over a domain $X$ such that for any $P,Q\in\mathcal{U}$  

$$
|\chi_{D}(P,Q)|\leq\left\{\delta\begin{array}{l l}{{\,\,\,\,i f\,P=Q}}\\ {{\varepsilon}}&{{\,o t h e r w i s e}}\end{array}\right.~.
$$  

Let $\tau\geq\sqrt{2\varepsilon}$ √. Then, any (randomized) $S Q$ algorithm that solves $B(\mathcal{U},D)$ with success probability $\eta>1/2$ requires at least $(2\eta-1)\cdot|\mathcal{U}|\cdot\tau^{2}/(2\delta)$ queries to $\mathrm{STAT}(\tau)$ .  

The following proposition establishes a tight upper bound on the pairwise correlation between homogeneous CLWE distributions. To deduce Theorem take a set of unit vectors $\boldsymbol{\mathcal{U}}$ such that any two distinct vectors 8.1 from Lemma $\pmb{v},\pmb{w}\in\mathcal{U}$ satisfy 8.2 and Proposition $|\langle\pmb{v},\pmb{w}\rangle|\leq1/\sqrt{2}$ √8.3 2, and , we identify it with the set of homogeneous CLWE distributions $\{H_{\pmb{w},\beta,\gamma}\}_{\pmb{w}\in\mathcal{U}}$ . A standard probabilistic argument shows that such a $\boldsymbol{\mathcal{U}}$ can be as large as $\exp(\Omega(n))$ , which proves Theorem 8.1 .  

Proposition 8.3. Let $\pmb{v},\pmb{w}\in\mathbb{R}^{n}$ be unit vectors and let $H_{v},H_{w}$ be $n$ -dimensional homogeneous CLWE distributions with parameters $\gamma\geq1,\beta\in(0,1)$ , and hidden direction $\pmb{v}$ and $\mathbf{\nabla}w$ , respectively. Then, for any $\alpha\geq0$ that satisfies $\gamma^{2}(1-\alpha^{2})\geq1$ ,  

$$
|\chi_{D}(H_{v},H_{w})|\leq\left\{2(\gamma/\beta)^{2}\atop8\exp(-\pi\cdot\gamma^{2}(1-\alpha^{2}))~~~~i f\,|\langle v,w\rangle|\leq\alpha\right.\,.
$$  

Proof. We will show that computing $\chi_{D}(H_{v},H_{w})$ reduces to evaluating the Gaussian mass of two lattices $L_{1}$ and $L_{2}$ defined below. Then, we will tightly bound the Gaussian mass using Lemma 2.5 and Lemma 2.10 , which will result in upper bounds on $|\chi_{D}(H_{v},H_{w})|$ . We define $L_{1}$ and $L_{2}$ by specifying their bases $B_{1}$ and $B_{2}$ , respectively.  

$$
\begin{array}{l}{{{\cal B}_{1}=\displaystyle\frac{1}{\sqrt{\beta^{2}+\gamma^{2}}}\left(\begin{array}{c c}{{1}}&{{0}}\\ {{0}}&{{1}}\end{array}\right)~,}}\\ {{{\cal B}_{2}=\displaystyle\frac{1}{\sqrt{\beta^{2}+\gamma^{2}}}\left(\begin{array}{c c}{{1}}&{{0}}\\ {{\displaystyle-\frac{\alpha\gamma^{2}}{\zeta\sqrt{\beta^{2}+\gamma^{2}}}}}&{{\displaystyle\frac{\sqrt{\beta^{2}+\gamma^{2}}}{\zeta}\right)~,}}\end{array}
$$  

where $\zeta=\sqrt{(\beta^{2}+\gamma^{2})-\alpha^{2}\gamma^{4}/(\beta^{2}+\gamma^{2})}$ p. Then the basis of the dual lattice $L_{1}^{*}$ and $L_{2}^{*}$ is $B_{1}^{-T}$ and $B_{2}^{-T}$ , respectively. Note that $\lambda_{2}(L_{1})^{2}=1/(\beta^{2}+\gamma^{2})$ and that the two columns of $B_{2}$ have the same norm, and so  

$$
\begin{array}{r l}&{\lambda_{2}(L_{2})^{2}\leq\displaystyle\frac{1}{\beta^{2}+\gamma^{2}}\cdot\operatorname*{max}\left\lbrace1+\frac{\alpha^{2}\gamma^{4}}{\zeta^{2}(\beta^{2}+\gamma^{2})},\frac{\beta^{2}+\gamma^{2}}{\zeta^{2}}\right\rbrace}\\ &{\qquad\qquad=\displaystyle\frac{1}{\zeta^{2}}}\\ &{\qquad\qquad\leq\frac{1}{\gamma^{2}(1-\alpha^{2})}\:.}\end{array}
$$  

Now define the density ratio $a(t):=H(t)/D(t)$ , where $D$ is the standard Gaussian and $H$ is the marginal distribution of homogeneous CLWE with parameters $\beta,\gamma$ along the hidden direction. We immediately obtain  

$$
a(t)=\frac{1}{Z}\sum_{k\in\mathbb{Z}}\rho_{\beta/\gamma}(t-k/\gamma)\ ,
$$  

where $\begin{array}{r}{Z=\int_{\mathbb{R}}\rho(t)\cdot\sum_{k\in\mathbb{Z}}\rho_{\beta/\gamma}(t-k/\gamma)d t}\end{array}$ . By Eq. ( 3 ), $Z$ is given by ∈  

$$
Z=\frac{\beta}{\sqrt{\beta^{2}+\gamma^{2}}}\cdot\rho\left(\frac{1}{\sqrt{\beta^{2}+\gamma^{2}}}\mathbb{Z}\right)\,.
$$  

Moreover, we can express $Z^{2}$ in terms of the Gaussian mass of $\left(L_{1}\right)$ as  

$$
Z^{2}=\frac{\beta^{2}}{\beta^{2}+\gamma^{2}}\cdot\rho(L_{1})\;.
$$  

$\chi_{D}(H_{v},H_{w})$ can be expressed in terms of $a(t)$ as  

$$
\chi_{D}\big(H_{v},H_{w}\big)=\underset{\pmb{x}\sim D}{\mathbb{E}}\Big[a\big(\langle\pmb{x},\pmb{w}\rangle\big)\cdot a\big(\langle\pmb{x},\pmb{v}\rangle\big)\Big]-1\ .
$$  

Without loss of generality, assume $\pmb{v}~=~e_{1}$ and $\pmb{w}\,=\,\alpha\pmb{e}_{1}\,+\xi\pmb{e}_{2}$ , where ξ$\xi\,=\,\sqrt{1-\alpha^{2}}$ √.We first compute the pairwise correlation for $\pmb{v}\neq\pmb{w}$ . For notational convenience, we denote by $\varepsilon=$ $8\cdot\exp(-\pi\cdot\gamma^{2}(1-\alpha^{2}))$ ).  

$$
\begin{array}{r l}{\log(I_{t},H_{m})+1=}&{\underset{=}{\underline{{\mathcal{E}}}}\left[(\tau(x_{1})-\tau\left(x(x_{1}+\ell\sigma_{2})\right)\right]}\\ &{=\frac{1}{2}\sum_{i,j\in\mathcal{I}_{m}}\int\int_{\mathbb{R}^{n}(\mathbb{R}^{n})}\|\exp\left((\tau\alpha x_{1}+\ell\sigma_{2})-\ell\right)\cdot\varphi(x_{1})\cdot\varphi(x_{2})\,d x\|}\\ &{=\frac{\beta}{2}-\sqrt{\binom{\lambda}{\mathcal{E}}}\frac{\int_{\mathbb{R}^{n}(\mathbb{R}^{n})}\sum_{i,j\in\mathcal{I}_{m}}\int_{\mathbb{R}^{n}(\mathbb{R}^{n})}|\cdot\varphi(x_{1})\cdot\varphi(x_{2})\cdot\varphi(x_{1})\cdot\varphi(x_{2})\cdot\varphi(x_{1})}{\sqrt{\binom{\lambda}{\mathcal{E}}+\binom{\lambda}{\mathcal{I}_{m}}}}}\\ &{=\frac{1}{2^{K}}\cdot\frac{\beta}{\sqrt{\binom{\lambda}{\mathcal{E}}}\cdot\binom{\lambda}{i,j\in\mathcal{I}_{m}}}\cdot\frac{\beta\sqrt{\binom{\lambda}{\mathcal{E}}+\binom{\lambda}{j}}\cdot\sum_{i,j\in\mathcal{I}_{m}}\int_{\mathbb{R}^{n}(\mathbb{R}^{n})}\cdot\varphi\left(x_{1}-\ell\right)\cdot\varphi\left(x_{2}-\ell\sigma_{2}\right)}{\sqrt{\binom{\lambda}{\mathcal{E}}+\binom{\lambda}{\mathcal{I}_{m}}}}}\\ &{=\frac{\sqrt{\binom{\lambda}{\mathcal{E}}+\frac{1}{2}}\cdot\sum_{i,j\in\mathcal{I}_{m}}\sum_{i\in\mathcal{I}_{m}}\left(\beta\right)\cdot\left(\lambda\right)\cdot\varphi\left(\ell-\gamma^{2}\alpha\left(x_{1}+\ell\sigma_{2}^{-}\right)\right)}{\sqrt{\binom{\lambda}{\mathcal{E}}+\binom{\lambda}{\mathcal{I}_{m}}}}}\\ &{=\frac{\sqrt{\binom{\lambda}{\mathcal{E}}+\frac{1}{2}}\cdot\frac{\beta\left(L_{1}\right)}{\sqrt{\binom{\lambda}{\mathcal{E}}}}}{\sqrt{\binom{\lambda}{\mathcal{E}}+\frac{1} 
$$  

In ( 21 ), we used the Poisson summation formula (Lemma 2.5 ). The last line follows from ( 18 ) and Lemma 2.10 , which implies that for any 2-dimensional lattice $L$ satisfying $\lambda_{2}(L)\leq1$ ,  

$$
\rho(L^{*}\setminus\{\mathbf{0}\})\le8\exp(-\pi/\lambda_{2}(L)^{2})\;.
$$  

Now consider the case $v=w$ . Using ( 17 ), we get an upper bound $\lambda_{2}(L_{2})\leq1/\beta$ when $\alpha=1$ .It follows that $\lambda_{2}((\beta/\gamma)L_{2})\leq1/\gamma\leq1$ . Hence,  

$$
\begin{array}{r l}&{\chi_{D}(H_{v},H_{v})+1=\frac{\sqrt{\beta^{2}+\gamma^{2}}}{\zeta}\cdot\frac{\rho(L_{2})}{\rho(L_{1})}}\\ &{\qquad\qquad\leq\frac{\sqrt{\beta^{2}+\gamma^{2}}}{\zeta}\cdot\frac{\rho((\beta/\gamma)L_{2})}{\rho(L_{1})}}\\ &{\qquad\qquad=\frac{\sqrt{\beta^{2}+\gamma^{2}}}{\zeta}\cdot\frac{\mathrm{det}((\gamma/\beta)L_{2}^{*})}{\mathrm{det}(L_{1}^{*})}\cdot\frac{\rho((\gamma/\beta)L_{2}^{*})}{\rho(L_{1}^{*})}}\\ &{\qquad\qquad=\frac{\gamma^{2}}{\beta^{2}}\cdot\frac{\rho((\gamma/\beta)L_{2}^{*})}{\rho(L_{1}^{*})}}\\ &{\qquad\qquad\leq2(\gamma/\beta)^{2}\,.}\end{array}
$$  

where we used Lemma 2.5 in ( 23 ) and in ( 24 ), we used ( 22 ) and the fact that $\lambda_{2}((\beta/\gamma)L_{2})\leq1$ to deduce $\rho((\gamma/\beta)L_{2}^{*}\setminus\{\mathbf{0}\})\le1$ \ { }≤1.  

# 9 Extension of Homogeneous CLWE to $m\geq1$ Hidden Directions  

In this section, we generalize the hardness result to the setting where the homogeneous CLWE distribution has $m\geq1$ hidden directions. The proof is a relatively standard hybrid argument.  

Definition 9.1 (${\boldsymbol{\mathit{m}}}$ -Homogeneous CLWE distribution) .For $0\leq m\leq n$ , matrix $W\in\mathbb{R}^{n\times m}$ with orthonormal columns $w_{1},\dots,w_{m}$ , and $\beta,\gamma\,>\,0$ , define the $m$ -homogeneous CLWE distribution $H_{W,\beta,\gamma}$ over $\mathbb{R}^{n}$ to have density at $\textit{\textbf{y}}$ proportional to  

$$
\rho(\pmb{y})\cdot\prod_{i=1}^{m}\sum_{k\in\mathbb{Z}}\rho_{\beta}(k-\gamma\langle\pmb{y},\pmb{w}_{i}\rangle)~.
$$  

Note that the 0-homogeneous CLWE distribution is just $D_{\mathbb{R}^{n}}$ regardless of $\beta$ and $\gamma$ .  

Definition 9.2. For parameters $\beta,\gamma\;>\;0$ and $1\,\leq\,m\,\leq\,n$ , the average-case decision problem $\mathrm{hCLWE}_{\beta,\gamma}^{(m)}$ is to distinguish the following two distributions over $\mathbb{R}^{n}$ :$(\boldsymbol{1})$ the $m$ -homogeneous CLWE distribution $H_{W,\beta,\gamma}$ for some matrix $W\in\mathbb{R}^{n\times m}$ (which is fixed for all samples) with orthonormal columns chosen uniformly from the set of all such matrices, or (2) $D_{\mathbb{R}^{n}}$ .  

Lemma 9.3. For any $\beta,\gamma\,>\,0$ and positive integer $m\,=\,m(n)$ such that $m\,\leq\,n$ and $n-m=$ $\Omega(n^{c})$ for some constant $c>0$ , if there exists an efficient algorithm that solves $\mathrm{hCLWE}_{\beta,\gamma}^{(m)}$ with non-negligible advantage, then there exists an efficient algorithm that solves $\mathrm{hCLWE}_{\beta,\gamma}$ with nonnegligible advantage.  

Proof. Suppose $\mathcal{A}$ is an efficient algorithm that solves $\mathrm{hCLWE}_{\beta,\gamma}^{(m)}$ with non-negligible advantage in dimension $n$ .Then consider the following algorithm $\boldsymbol{\beta}$ that uses $\mathcal{A}$ as an oracle and solves hCLWE $\beta,\gamma$ in dimension $n^{\prime}=n-m+1$ .  

1. Input: $n^{\prime}$ -dimensional samples, drawn from either hCLWE $\beta,\gamma$ or $D_{\mathbb{R}^{n^{\prime}}}$ ;  
2. Choose $0\leq i\leq m-1$ uniformly at random;   
3. Append $m{-}1=n{-}n^{\prime}$ coordinates to the given samples, where the first $i$ appended coordinates are drawn from $H_{I_{i},\beta,\gamma}$ (with $I_{i}$ denoting the rank$i$ identity matrix) and the rest of the coordinates are drawn from $D_{\mathbb{R}^{m-i-1}}$ ;  
4. Rotate the augmented samples using a uniformly random rotation from the orthogonal group $O(n)$ ;  
5. Call $\mathcal{A}$ with the samples and output the result.  

As $n=O(n^{\prime1/c})$ ,$\boldsymbol{\beta}$ is an efficient algorithm. Moreover, the samples passed to $\mathcal{A}$ are effectively drawn from either $\mathrm{hCLWE}_{\beta,\gamma}^{(i+1)}$ or $\mathrm{hCLWE}_{\beta,\gamma}^{(i)}$ . Therefore the advantage of $\boldsymbol{\beta}$ is at least $1/m$ fraction of the advantage of $\mathcal{A}$ , which would be non-negligible (in terms of $n$ , and thus also in terms of $n^{\prime}$ )as well.  

Combining Corollary 4.2 and Lemma 9.3 , we obtain the following corollary.  

Corollary 9.4. For any $\beta\,=\,\beta(n)\,\in\,(0,1)$ and $\gamma=\gamma(n)\,\geq\,2{\sqrt{n}}$ such that $\gamma/\beta$ is polynomially bounded, and positive integer $m=m(n)$ such that $m\leq n$ and $n-m=\Omega(n^{c})$ for some constant $c>0$ , there is a polynomial-time quantum reduction from $\mathrm{DGS}_{2\sqrt{2n}\eta_{\varepsilon}(L)/\beta}$ t$o\ \mathrm{hCLWE}_{\beta,\gamma}^{(m)}$ .  

# References  

[AD97] M. Ajtai and C. Dwork. A public-key cryptosystem with worst-case/average-case equivalence. STOC . 1997, pp. 284–293.  

[AG11] S. Arora and R. Ge. New algorithms for learning in presence of errors. ICALP . 2011, pp. 403–415.   
[AK05] S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. Ann. Appl. Probab. 15: 1A (2005), pp. 69–92.   
[AR05] D. Aharonov and O. Regev. Lattice problems in NP ∩CoNP. J. ACM 52: 5 (2005), pp. 749–765.   
[Bab86] L. Babai. On Lov´asz’ lattice reduction and the nearest lattice point problem. Combinatorica 6: 1 (1986), pp. 1–13.   
[Bra+13] Z. Brakerski, A. Langlois, C. Peikert, O. Regev, and D. Stehl´e. Classical hardness of learning with errors. STOC . 2013, pp. 575–584.   
[Bub+19] S. Bubeck, Y. T. Lee, E. Price, and I. Razenshteyn. Adversarial examples from computational constraints. ICML . Vol. 97. 2019, pp. 831–840.   
[BV08] S. C. Brubaker and S. Vempala. Isotropic PCA and affine-invariant clustering. FOCS .2008, pp. 551–560.   
[Das99] S. Dasgupta. Learning mixtures of Gaussians. FOCS . 1999, p. 634.   
[Dia16] I. Diakonikolas. Learning structured distributions. Handbook of Big Data . 2016, pp. 267– 284.   
[DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures. FOCS . 2017, pp. 73– 84.   
[DKS18] I. Diakonikolas, D. M. Kane, and A. Stewart. List-decodable robust mean estimation and learning mixtures of spherical Gaussians. STOC . 2018, pp. 1047–1060.   
[DMR18] L. Devroye, A. Mehrabian, and T. Reddad. The total variation distance between highdimensional Gaussians. 2018. arXiv: 1810.08693 .  
[DS07] S. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. JMLR 8 (2007), pp. 203–226.   
[Fel+17] V. Feldman, E. Grigorescu, L. Reyzin, S. S. Vempala, and Y. Xiao. Statistical algorithms and a lower bound for detecting planted cliques. J. ACM 64: 2 (2017).   
[HL18] S. B. Hopkins and J. Li. Mixture models, robustness, and sum of squares proofs. STOC .2018, pp. 1021–1034.   
[Kea98] M. Kearns. Efficient noise-tolerant learning from statistical queries. J. ACM 45: 6 (1998), pp. 983–1006.   
[KKK19] S. Karmalkar, A. Klivans, and P. Kothari. List-decodable linear regression. NeurIPS .2019, pp. 7425–7434.   
[KSS18] P. K. Kothari, J. Steinhardt, and D. Steurer. Robust moment estimation and improved clustering via sum of squares. STOC . 2018, pp. 1035–1046.   
[LLL82] A. K. Lenstra, H. W. Lenstra, and L. Lov´asz. Factoring polynomials with rational coefficients. en. Mathematische Annalen 261: 4 (1982), pp. 515–534.   
[LM09] V. Lyubashevsky and D. Micciancio. On bounded distance decoding, unique shortest vectors, and the minimum distance problem. CRYPTO . 2009, pp. 577–594.   
[Min10] H. Minkowski. Geometrie der Zahlen. B.G. Teubner, 1910.   
[Moi18] A. Moitra. Algorithmic aspects of machine learning. Cambridge University Press, 2018.   
[MP12] D. Micciancio and C. Peikert. Trapdoors for lattices: simpler, tighter, faster, smaller. EUROCRYPT . 2012, pp. 700–718.   
[MR07] D. Micciancio and O. Regev. Worst-case to average-case reductions based on Gaussian measures. SIAM J. Comput. 37: 1 (2007), pp. 267–302.   
[MV10] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. FOCS . 2010, pp. 93–102.   
[Pea94] K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A 185 (1894), pp. 71–110.   
[Pei10] C. Peikert. An efficient and parallel Gaussian sampler for lattices. CRYPTO . 2010, pp. 80–97.   
[Pei16] C. Peikert. A decade of lattice cryptography. Foundations and Trends in Theoretical Computer Science 10: 4 (2016), pp. 283–424.   
[PRS17] C. Peikert, O. Regev, and N. Stephens-Davidowitz. Pseudorandomness of ring-LWE for any ring and modulus. STOC . 2017, pp. 461–473.   
[Reg04] O. Regev. New lattice-based cryptographic constructions. J. ACM 51: 6 (2004), pp. 899– 942.   
[Reg05] O. Regev. On lattices, learning with errors, random linear codes, and cryptography. STOC . 2005, pp. 84–93.   
[RS09] R. Rubinfeld and R. A. Servedio. Testing monotone high-dimensional distributions. Random Structures & Algorithms 34: 1 (2009), pp. 24–44.   
[RV17] O. Regev and A. Vijayaraghavan. On learning mixtures of well-separated Gaussians. FOCS . 2017, pp. 85–96.   
[RY20] P. Raghavendra and M. Yau. List decodable learning via sum of squares. SODA . 2020, pp. 161–180.   
[Sze+14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. ICLR . 2014.   
[Ver18] R. Vershynin. High-dimensional probability: an introduction with applications in data science. Cambridge University Press, 2018.   
[VW02] S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. FOCS . 2002, p. 113.  