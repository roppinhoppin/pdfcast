[
    {
        "type": "text",
        "text": "NEURAL MECHANICS :SYMMETRY AND BROKEN CON -SERVATION LAWS IN DEEP LEARNING DYNAMICS ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Daniel Kunin\\*, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka\\* †",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Stanford University $\\dagger$ Physics & Informatics Laboratories, NTT Research, Inc. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "A BSTRACT ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network’s architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether’s theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow ,a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 I NTRODUCTION ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Just like the fundamental laws of classical and quantum mechanics taught us how to control and optimize the physical world for engineering purposes, a better understanding of the laws governing neural network learning dynamics can have a profound impact on the optimization of artificial neural networks. This raises a foundational question: what, if anything, can we quantitatively understand about the learning dynamics of large-scale, non-linear neural network models driven by real-world datasets and optimized via stochastic gradient descent with a finite batch size, learning rate, and with or without momentum? In order to make headway on this extremely difficult question, existing works have made major simplifying assumptions on the network, such as restricting to identity activation functions Saxe et al. (2013), infinite width layers Jacot et al. (2018), or single hidden layers Saad & Solla (1995). Many of these works have also ignored the complexity introduced by stochasticity and discretization by only focusing on the learning dynamics under gradient flow. In the present work, we make the first step in an orthogonal direction. Rather than introducing unrealistic assumptions on the model or learning dynamics, we uncover restricted, but meaningful, combinations of parameters with simplified dynamics that can be solved exactly without introducing a major assumption (see Fig. 1). To find the parameter combinations, we use the lens of symmetry to show that if the training loss doesn’t change under some transformation of the parameters, then the gradient and Hessian for those parameters have associated geometric constraints. We systematically apply this approach to modern neural networks to derive exact integral expressions and verify our predictions empirically on large scale models and datasets. We believe our work is the first step towards a foundational understanding of neural network learning dynamics that is not based in simplifying assumptions, but rather the simplifying symmetries embedded in a network’s architecture. Our main contributions are: ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1. We leverage continuous differentiable symmetries in the loss to unify and generalize geometric constraints on neural network gradients and Hessians (section 3).   \n2. We prove that each of these differentiable symmetries has an associated conservation law under the learning dynamics of gradient flow (section 4).   \n3. We construct a more realistic continuous model for stochastic gradient descent by modeling weight decay, momentum, stochastic batches, and finite learning rates (section 5).   \n4. We show that under this more realistic model the conservation laws of gradient flow are broken, yielding simple ODEs governing the dynamics for the previously conserved parameter combinations (section 6).   \n5. We solve these ODEs to derive exact learning dynamics for the parameter combinations, which we validate empirically on VGG-16 trained on Tiny ImageNet with and without batch normalization (section 6). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 RELATED WORK ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The goal of this work is to construct a theoretical framework to better understand the learning dynamics of state-of-the-art neural networks trained on real-world datasets. Existing works have made progress towards this goal through major simplifying assumptions on the architecture and learning rule. Saxe et al. (2013; 2019) and Lampinen & Ganguli (2018) considered linear neural networks with specific orthogonal initializations, deriving exact solutions for the learning dynamics under gradient flow. The theoretical tractability of linear networks has further enabled analyses on the properties of loss landscapes Kawaguchi (2016), convergence Arora et al. (2018a); Du & Hu (2019), and implicit acceleration by overparameterization Arora et al. (2018b). Saad & Solla (1995) and Goldt et al. (2019) studied single hidden layer architectures with non-linearities in a studentteacher setup, deriving a set of complex ODEs describing the learning dynamics. Such shallow neural networks have also catalyzed recent major advances in understanding convergence Figure 1: Neuron level dynamics are simpler than parameter dynamics. We plot the perparameter dynamics (left) and per-channel squared Euclidean norm dynamics (right) for the convolutional layers of a VGG-16 model (with batch normalization) trained on Tiny ImageNet with SGD with learning rate $\\eta\\:=\\:0.1$ , weight decay $\\lambda=10^{-4}$ , and batch size $S=256$ . While the parameter dynamics are noisy and chaotic, the neuron dynamics are smooth and patterned. ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/15e96b5fd49c9a768069c017262eead3ffe536c4d4098921538fd4e1bded589d.jpg",
        "img_caption": [
            "(a) Parameter Dynamics (b) Neuron Dynamics "
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "properties of neural networks Du et al. (2018b); Mei et al. (2018). Jacot et al. (2018) considered infinitely wide neural networks with non-linearities, demonstrating that the network’s prediction becomes linear in its parameters. This setting allows for an insightful mathematical formulation of the network’s learning dynamics as a form of kernel regression where the kernel is defined by the initialization (though see also Fort et al. (2020)). Arora et al. (2019) extended these results to convolutional networks and Lee et al. (2019) demonstrated how this understanding also allows for predictions of parameter dynamics. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In the present work, we make the first step in an orthogonal direction. Instead of introducing unrealistic assumptions, we discover restricted combinations of parameters for which we can find exact solutions, as shown in Fig. 1. We make this fundamental contribution by constructing a framework harnessing the geometry of the loss shaped by symmetry and realistic continuous equations of learning. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Geometry of the loss. A wide range of literature has discussed constraints on gradients originating from specific architectural building blocks of networks. For the first part of our work, we simplify, unify, and generalize the literature through the lens of symmetry. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The earliest works understanding the importance of invariances in neural networks come from the loss landscape literature Baldi & Hornik (1989) and the characterization of critical points in the presence of explicit regularization Kunin et al. (2019). More recent works have studied implicit regularization originating from linear Arora et al. (2018b) and homogeneous Du et al. (2018a) activations, finding that gradient geometry plays an important role in constraining the learning dynamics. A different line of research studying the generalization capacity of networks has noticed similar gradient structures Liang et al. (2019). Beyond theoretical studies, geometric properties of the gradient and Hessian have been applied to optimize Neyshabur et al. (2015), interpret Bach et al. (2015), and prune Tanaka et al. (2020) neural networks. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Gradient properties introduced by batch Ioffe & Szegedy (2015), weight Salimans & Kingma (2016) and layer Ba et al. (2016) normalization have been intensely studied. Van Laarhoven (2017); Zhang et al. (2018) showed that normalization layers are scale invariant, but have an implicit role in controlling the effective learning rate. Cho & Lee (2017); Hoffer et al. (2018); Chiley et al. (2019); Li & Arora (2019); Wan et al. (2020) have leveraged the scale invariance of batch normalization to understand geometric properties of the learning dynamics. Most recently, Li et al. (2020) studied the role of gradient noise in reconciling the empirical dynamics of batch normalization with the theoretical predictions given by continuous models of gradient descent. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Equations of learning. To make experimentally testable predictions on learning dynamics, we introduce a continuous model for stochastic gradient descent (SGD). There exists a large body of works studying this subject using stochastic differential equations (SDEs) in the continuous-time limit Mandt et al. (2015; 2017); Li et al. (2017); Smith & Le (2017); Chaudhari & Soatto (2018); Jastrz˛ebski et al. (2017); Zhu et al. (2018); An et al. (2018). Each of these works involves making specific assumptions on the loss or the noise in order to derive stationary distributions. More careful treatment of stochasticity led to fluctuation dissipation relationships at steady state without such assumptions Yaida (2018). In our analysis, we apply a more recent approach Li et al. (2017); Barrett & Dherin (2020), inspired by finite difference methods, that augments SDE model with higher-order terms to account for the effect of a finite step size and curvature in the learning trajectory. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3 SYMMETRIES IN THE LOSS SHAPE GRADIENT AND HESSIAN GEOMETRIES ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "While we initialize neural networks randomly, their gradients and Hessians at all points in training, no matter the loss or dataset, obey certain geometric constraints. Some of these constraints have been noticed previously as a form of implicit regularization, while others have been leveraged algorithmically in applications from network pruning to interpretability. Remarkably, all these geometric constraints can be understood as consequences of numerous differentiable symmetries in the loss introduced by neural network architectures. A set of parameters observes a differentiable symmetry in the loss if the loss doesn’t change under a certain differentiable transformation of these parameters. This invariance introduces associated geometric constraints on the gradient and Hessian. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Consider a function $f(\\theta)$ where $\\theta\\in\\mathbb{R}^{m}$ . This function possesses a differentiable symmetry if it is invariant under the differentiable action $\\psi$ Figure 2: Visualizing symmetry. We visualize the vector fields associated with simple network components that have translation, scale, and rescale symmetry. In (a) we consider the vector field associated with a neuron $\\sigma\\left(\\left[w_{1}\\quad w_{2}\\right]^{\\mathsf{T}}x\\right)$ \u0000\u0001where $\\sigma$ is the softmax function. In (b) we consider the vector field associated with a neuron BN ${\\bigl(}[w_{1}\\quad w_{2}]\\,[x_{1}\\quad x_{2}]^{\\mathsf{T}}{\\bigr)}$ \u0001where BN is the batch normalization function. In (c) we consider the vector field associated with a linear path $w_{2}w_{1}x$ .",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/1aa0c83930f5b11a08abd9f4ef2bba3bbf711d71d767327b1da25c8722ff5e9e.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "$G$ he para vector $\\theta$ , i.e., if $\\theta\\,\\mapsto\\,\\psi(\\theta,\\alpha)$ where $\\alpha\\,\\in\\,G$ , then $F\\left(\\theta,\\alpha\\right)\\;=$ $f\\left(\\psi(\\theta,\\alpha)\\right)=f(\\theta)$ for all $(\\theta,\\alpha)$ . The existence of a symmetry enforces a geometric structure on the gradient, $\\nabla F$ . Evaluating the gradient at the identity element $I$ of $G$ (so that for all $\\theta$ ,$\\psi(\\theta,I)=\\theta;$ )yields the result, ",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\nabla f,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where to the vector field $\\langle~,~\\rangle$ denote $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ |er product. This equality implies that that gen es the symmetry, for all θhe gradient . The symmetry also enforces a $\\nabla f$ is perpendicular geometric structure on the Hessian, $\\mathbf{H}F$ . Evaluating the Hessian at the identity element yields the ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "result, ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n{\\bf H}f[\\partial_{\\theta}\\psi|_{\\alpha=I}]\\partial_{\\alpha}\\psi|_{\\alpha=I}+[\\partial_{\\theta}\\partial_{\\alpha}\\psi|_{\\alpha=I}]\\nabla f=0,\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "which constrains the Hessian $\\mathbf{H}f$ . See appendix A for the derivation of these properties and other geometric consequences of symmetry. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "We ow consider the specific setting of a neural network parameterized by $\\theta\\in\\mathbb{R}^{m}$ , the training loss modern network architectures. L${\\mathcal{L}}(\\theta)$ , and three families of symmetries (translation, scale, and rescale) that commonly appear in ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Trans on symmetry. Translation symmetry defined by the gr $\\mathbb{R}$ ion $\\theta\\mapsto\\theta+\\alpha\\mathbb{1}_{\\mathcal{A}}$ Lturn implies the loss gradient here thus possesses translation $\\mathbb{1}_{\\mathcal{A}}$ A is the indicator vector for some s $\\partial_{\\theta}{\\mathcal{L}}=g$ Ly if is orthogonal to the indicator vector L$\\mathcal{L}(\\theta)=\\mathcal{L}(\\theta+\\alpha\\mathbb{1}_{A})$ $\\boldsymbol{\\mathcal{A}}$ LA for all ters {$\\{\\theta_{1},\\ldots,\\theta_{m}\\}$ $\\alpha\\in\\mathbb{R}$ ∈$\\partial_{\\alpha}\\psi|_{\\alpha=I}=\\mathbb{1}_{\\mathcal{A}}$ }|. The loss function A metry in ,",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\langle g,\\mathbb{1}_{\\mathcal{A}}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "and that the Hessian matrix $H=\\partial_{\\theta}^{2}{\\mathcal{L}}$ Lhas the indicator vector in its kernel, ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nH\\mathbb{1}_{A}=0.\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Softmax function. Any network using the softmax function gives rise to translation symmetry for the parameters immediately preceding the function. Let $z=W x+b$ be the input to the softmax function such that bias vector $b$ by a real constant has no effect on the output of the softmax as the shift factors from $\\begin{array}{r}{\\sigma(z)_{i}=\\frac{e^{z_{i}}}{\\sum_{j}e^{z_{j}}}}\\end{array}$ P. Notice that shifting any column of the weight matrix $w_{i}$ or the both the numerator and denominator canceling its effect. Thus, the loss function is invariant w.r.t. this translation, yielding the gradient constraints toy model where $w_{i}\\in\\mathbb{R}^{2}$ .$\\begin{array}{r}{\\langle\\frac{\\partial\\mathcal{L}}{\\partial w_{i}},\\mathbb{1}\\rangle=\\langle\\frac{\\partial\\mathcal{L}}{\\partial b},\\mathbb{1}\\rangle=0}\\end{array}$ , visualized in Fig. 2 for the Scale symmetry. Scale symmetry is defined by the group $\\mathrm{GL_{1}^{+}(\\mathbb{R})}$ an $\\theta\\mapsto\\alpha_{\\mathcal{A}}\\odot\\theta$ where $\\alpha_{\\mathcal{A}}\\,=\\,\\alpha\\mathbb{1}_{\\mathcal{A}}+\\mathbb{1}_{\\mathcal{A}^{\\circ}}$ . The loss function possesses scale symmetry if L$\\mathcal{L}(\\theta)\\;=\\;\\mathcal{L}(\\alpha_{\\mathcal{A}}\\odot\\theta)$ LA ⊙for all $\\alpha\\in\\mathrm{GL}_{1}^{+}(\\mathbb{R})$ the parameter vector itself ∈. This symmetry immediately implies the loss gradient is everywhere perpendicular to $\\partial_{\\alpha}\\psi|_{\\alpha=I}=\\theta\\odot\\mathbb{1}_{A}=\\theta_{A}$ ,",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\langle g,\\theta_{\\cal A}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "and relates to the Hessian matrix, where $[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]\\partial_{\\theta}\\mathcal{L}=\\mathrm{diag}(\\mathbb{1}_{A})g=g_{A},$ , as ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nH\\theta_{\\mathcal{A}}+g_{\\mathcal{A}}=0.\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Batch normalization. Batch normalization leads to scale invariance during training. Let $z=w^{\\top}x+b$ be the input to a neuron with batch normalization such that $\\begin{array}{r}{\\mathrm{BN}(z)\\,=\\,\\frac{z-\\mathrm{E}[z]}{\\sqrt{\\mathrm{Var}(z)}}}\\end{array}$ where $\\operatorname{E}[z]$ is the sample mean and $\\operatorname{Var}(z)$ is the sample variance given a batch of data. Notice that scaling $w$ and $b$ by a non-zero real constant has no effect on the output of the batch normalization as it factors from $z,\\operatorname{E}[z]$ and their gradients satisfy , and $\\operatorname{Var}(z)$ canceling its effect. Thus, these parameters observe scale symmetry in the loss $\\begin{array}{r}{\\left\\langle\\frac{\\bar{\\partial}\\mathcal{L}}{\\partial w},w\\right\\rangle+\\left\\langle\\frac{\\partial\\mathcal{L}}{\\partial b},b\\right\\rangle=0}\\end{array}$ , as has been previously noted by Ioffe & Szegedy (2015); Van Laarhoven (2017), and visualized in Fig. 2 for the toy model where $w,b\\in\\mathbb{R}$ .",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Rescale symmetry. Rescale symmetry is defined by the group $\\mathrm{GL_{1}^{+}(\\mathbb{R})}$ and action $\\theta\\mapsto\\alpha_{A_{1}}\\odot$ $\\alpha_{A_{2}}^{-1}\\odot\\theta$ symmetry if A wh L$\\mathcal{L}(\\theta)=\\mathcal{L}(\\alpha_{\\mathcal{A}_{1}}\\odot\\alpha_{\\mathcal{A}_{2}}^{-1}\\odot\\theta)$ $\\mathcal{A}_{1}$ $\\boldsymbol{A_{2}}$ ⊙e perpendicular to the sign inverted parameter vector A for all int sets $\\alpha\\in\\mathrm{GL_{1}^{+}(R)}$ ∈s. The loss function possesses rescale . This symmetry immediately implies $\\partial_{\\alpha}\\psi|_{\\alpha=I}\\,=$ $\\theta_{\\mathcal{A}_{1}}-\\theta_{\\mathcal{A}_{2}}=\\theta\\odot\\left(\\mathbb{1}_{\\mathcal{A}_{1}}-\\mathbb{1}_{\\mathcal{A}_{2}}\\right)$ ,",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\langle g,\\theta_{A_{1}}-\\theta_{A_{2}}\\rangle=0\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "and relates to the Hessian matrix, where $[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]\\partial_{\\theta}\\mathcal{L}=\\mathrm{diag}\\big(\\mathbb{1}_{\\mathcal{A}_{1}}-\\mathbb{1}_{\\mathcal{A}_{2}}\\big)g=g_{\\mathcal{A}_{1}}-g_{\\mathcal{A}_{2}}$ , as ",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r}{H(\\theta_{\\mathcal{A}_{1}}-\\theta_{\\mathcal{A}_{2}})+g_{\\mathcal{A}_{1}}-g_{\\mathcal{A}_{2}}=0.}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Homogeneous activation. For networks with continuous, homogeneous activation functions $\\phi(z)\\;\\bar{=}\\;\\phi^{\\prime}(z)z$ (e.g. ReLU, Leaky ReLU, linear), this symmetry emerges at every hidden neuron by considering all incoming and outgoing parameters to the neuron. For example, consider a hidden neuron with ReLU activation $\\phi(\\bar{z})=\\operatorname*{max}\\{0,z\\}$ , such that $w_{2}\\phi(w_{1}^{\\mathsf{T}}x+b)$ is the computational path through this neuron. Scaling $w_{1}$ and bby a real constant and $w_{2}$ by its inverse has no effect on the computational path as the constants can be passed through the ReLU activation $\\begin{array}{r}{\\left<\\frac{\\partial\\mathcal{L}}{\\partial w_{1}},\\bar{w_{1}}\\right>+\\left<\\frac{\\partial\\mathcal{L}}{\\partial b},b\\right>-\\left<\\frac{\\partial\\mathcal{L}}{\\partial w_{2}},w_{2}\\right>=0.}\\end{array}$ canceling their effects. Thus, these parameters observe rescale symmetry and their gradients satisfy EDE, as has been previously noted by Du et al. (2018a); Liang (2019); Tanaka et al. (2020), and visualized in Fig. 2 for the toy model where $w_{1},w_{2}\\in\\mathbb{R}$ and $b=0$ .",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4 SYMMETRY LEADS TO CONSERVATION LAWS UNDER GRADIENT FLOW ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We now explore how geometric constraints on gradients and Hessians, arising as a consequence of symmetry, impact the learning dynamics given by stochastic gradient descent (SGD). We will consider a model parameterized by $\\theta$ , a training dataset $\\{x_{1},\\ldots,x_{N}\\}$ of size $N$ , and a training loss $\\begin{array}{r}{\\mathcal{L}(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\boldsymbol{\\theta},\\boldsymbol{x}_{i})}\\end{array}$ Pwith corresponding gradient $\\textstyle g(\\theta)={\\frac{\\partial{\\mathcal{L}}}{\\partial\\theta}}$ .",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "The gradient descent update with learning rate $\\eta$ is $\\bar{\\theta}^{(n+1)}=\\theta^{(n)}-\\bar{\\eta}g(\\theta^{(n)})$ , which is a forward Euler discretization with step size $\\eta$ of the ordinary differential equation (ODE) ",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d\\theta}{d t}=-g(\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "In the limit as $\\eta\\rightarrow0$ , gradient descent exactly matches the dynamics of this ODE, which is commonly referred to as gradient flow Kushner & Yin (2003). Equipped with a continuous model for the learning dynamics, we now ask how do the dynamics interact with the geometric properties introduced by symmetries? ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Symmetry leads to conservation. Strikingly similar to Noether’s theorem, which describes a fundamental relationship between symmetry and conservation for physical systems governed by Lagrangian dynamics, here we show that symmetries of a network architecture have corresponding conserved quantities through training under gradient flow. ",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/221c57fa903016b4e1e24530745ca8a9b49b21080056846d0821282222c73fec.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Figure 3: Visualizing conservation. Associated with each symmetry is a conserved quantity constraining the gradient flow dynamics to a surface. For translation symmetry (a) the flow is constrained to a hyperplane where the intercept is conserved. For scale symmetry (b) the flow is constrained to a sphere where the radius is conserved. For rescale symmetry (c) the flow is constrained to a hyperbola where the axes are conserved. The color represents the value of the conserved quantity, where blue is positive and red is negative, and the black lines are level sets. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Theorem 1. Symmetry and conservation laws in neural networks. Every differentiable symmetry $\\psi(\\alpha,\\theta)$ of the loss that satisfies $\\langle\\theta,[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]g(\\theta)\\rangle=0$ has the corresponding conservation law, ",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r}{\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0,}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "through learning under gradient flow. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "To prove Theorem 1, consider projecting the gradient flow learning dynamics in equation (9) onto the generat ctor field $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ associated with a symme own in section 3, the gradient of the loss a differential equation, which can be simplified to equation (10). See appendix B for a complete $g(\\theta)$ is always perpendicular to the vector field $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ |. Thus, the rojection yields derivation. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Application of this general theorem to the translation, scale, and rescale symmetries, identified in section 3, yields the following conservation law of learning, ",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}{\\mathrm{Translation:}}&{\\langle\\theta_{A}(t),\\mathbb{1}\\rangle=\\langle\\theta_{A}(0),\\mathbb{1}\\rangle}\\\\ {\\mathrm{Scale:}}&{|\\theta_{A}(t)|^{2}=|\\theta_{A}(0)|^{2}}\\\\ {\\mathrm{Rescale:}}&{|\\theta_{A_{1}}(t)|^{2}-|\\theta_{A_{2}}(t)|^{2}=|\\theta_{A_{1}}(0)|^{2}-|\\theta_{A_{2}}(0)|^{2}}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Each of these equations define a conserved constant of learning through training. For parameters with translation symmetry, their sum $(\\langle\\theta_{A}(t),\\mathbb{1}\\rangle)$ is conserved, effectively cons eir dynamics to a hyperplane. For parameters with scale symmetry, their euclidean norm ( $(|\\theta_{\\mathcal{A}}(t\\bar{)}|^{2})$ |A |) is conserved, effectively constraining their dynamics to a sphere Ioffe & Szegedy (2015); Van Laarhoven (2017). For parameters with rescale symmetry, their difference in squared euclidean norm $(|\\theta_{A_{1}}(t)|^{2}-|\\theta_{A_{2}}(t)|^{2})$ is conserved, effectively constraining their dynamics to a hyperbola Du et al. (2018a). In Fig. 3 we visualize the level sets of these conserved quantities for the toy models discussed in Fig. 2. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5 A R EALISTIC CONTINUOUS MODEL FOR STOCHASTIC GRADIENT DESCENT ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "In section 4 we combined the geometric constraints introduced by symmetries with gradient flow to derive conservation laws for simple combinations of parameters during training. However, empirically we know these laws are broken, as demonstrated in Fig. 1. What causes this discrepancy? Gradient flow is too simple of a continuous model for realistic SGD training. It fails to incorporate the effect of weight decay introduced by explicit regularization, momentum introduced by commonly used hyperparameters, stochasticity introduced by random batches, and discretization introduced by a finite learning rate. Here, we construct a more realistic continuous model for stochastic gradient descent. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Modeling weight decay. Explicit regularization through the addition of an $L_{2}$ penalty on the parameters, with regularization constant $\\lambda$ , is very common practice when training modern deep learning models. This is generally implemented not by modifying the training loss, but rather directly modifying the optimizer’s update equation. For stochastic gradient descent, the result leads to the updated continuous model ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d\\theta}{d t}=-g(\\theta)-\\lambda\\theta.\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Modeling momentum. Momentum is a common extension to SGD that uses an exponentially moving average of gradients to update parameters rather than a single gradient evaluation Rumelhart et al. (1986). The method introduces two additional hyperparameters, a damping coefficient $\\alpha$ and a $\\beta$ ies the t date equation, $\\theta^{(n+1)}=\\theta^{(n)}-\\eta v^{(n+1)}$ where $v^{(n+1)}=\\beta v^{(n)}+(1-\\alpha)g(\\theta^{(\\bar{n})})$ αeffectively reduces the learning rate and −. When $\\alpha=\\beta=0$ βcontrols how past gradients are used in future updates , we regain classic gradient descent. In general, resulting in a form of “inertia” accelerating and smoothing the descent trajectory. Rearranging the two-step update equation, we find that gradient descent with momentum is a first-order discretization with step size $\\eta(1-\\alpha)$ of the $\\mathrm{ODE^{1}}$ ,",
        "page_idx": 5
    },
    {
        "type": "equation",
        "text": "$$\n(1-\\beta){\\frac{d\\theta}{d t}}=-g(\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "$\\begin{array}{r}{\\frac{1}{S}\\sum_{i\\in B}{\\nabla\\ell({\\boldsymbol{\\theta}},{\\dot{\\boldsymbol{x}_{i}}})},}\\end{array}$ we can model the batch gradient as an average of Modeling stochasticity. Pfrom the indices . When the batch size is much s Stochastic {$\\{1,\\ldots,N\\}$ }${\\hat{g}}_{B}(\\theta)$ forming the unbiased gradient estim Si.i.d. samples from a noisy version of the true aller than the size of the dataset, arise when we consider a batch $S\\ll N$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ ≪${\\hat{g}}_{B}(\\theta)\\;=$ , then $S$ gradient $g(\\theta)$ . Usi central limit theorem, ssume ${\\hat{g}}_{B}(\\theta)\\mathrm{~-~}g(\\theta)$ is a Gaussian random variable with mean $\\mu=0$ and covariance matrix $\\Sigma(\\theta)$ . However, because both the batch gradient and true gradient observe the same geometric properties introduced by symmetry, the noise has a special low-rank structure. As we showed in section 3, the gradient of the loss, regardless of the batch, is orthogonal to the generator vector field $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ associated with a symmetry. This implies the stochastic noise must also observe the same property. In order for this relationship to hold for arbitrary noise, then $\\Sigma(\\theta)\\partial_{\\alpha}\\psi|_{\\alpha=I}=0$ . In other words, the differential symmetry inherent in neural network architectures projects the noise introduced by stochastic gradients onto low rank subspaces, leaving the gradient flow dynamics in these directions unchanged 2 .",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Modeling discretization. The effect of discretization when modeling continuous dynamics is a well studied problem in the numerical analysis of partial differential equations. One tool commonly used in this setting, is modified equation analysis Warming & Hyett (1974), which determines how to better model discrete steps with a continuous differential equation by introducing higher order spatial or temporal derivatives. We present two methods based on modified equation analysis, which modify gradient flow to account for the effect of discretization. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Modified loss. Gradient descent always moves in the direction of steepest descent on a loss function finite nature of the learning rate, it fails to re$\\mathcal{L}$ at each step, however, due to the main on the continuous steepest descent path given by gradient flow. Li et al. (2017); Feng et al. (2019) and most recently Barrett & Dherin (2020), demonstrate that the gradient descent trajectory closely follows the steepest descent path of a modified loss fun ion $\\bar{\\tilde{c}}$ . The divergence between these trajectories fundamentally depends on the learning rate ηand the curvature $H$ . As derived in Barrett & Dherin (2020), and summarized in appendix D, this divergence is given by the gradient correction $-{\\textstyle\\frac{1}{2}}H\\overline{{g}}$ , which Thus, the modified loss is is the gradient of the squared norm $\\begin{array}{r}{\\widetilde{\\mathcal{L}}=\\mathcal{L}+\\frac{\\eta}{4}|\\nabla\\mathcal{L}|^{2}}\\end{array}$ e$-\\textstyle{\\frac{\\eta}{4}}\\left|\\nabla{\\mathcal{L}}\\right|^{2}$ and L| .the modified gradient flow ODE is ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d\\theta}{d t}=-g(\\theta)-\\frac{\\eta}{2}H(\\theta)g(\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "See Fig. 4 for an illustrative example of this method applied to a quadratic loss in $\\mathbb{R}^{2}$ .",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/1db3c82228ccdc3dd434c8b7549b55fddb41f0334fab520a3a900ffb73e7b0ea.jpg",
        "img_caption": [
            "Figure 4: Modeling discretization. We visualize the trajectories of gradient descent and momentum (black dots), gradient flow with and without momentum (blue lines), and the modified dy$w^{\\mathsf{T}}\\left[\\!\\!\\begin{array}{c c}{{2.5}}&{{-1.5}}\\\\ {{-1.5}}&{{2}}\\end{array}\\!\\!\\right]w$ namics (red lines) on the quadratic loss '. On the left we visualize gradi$\\mathcal{L}(w)\\stackrel{!}{=}$ ent dynamics using modified loss. On the right −we visualize momentum dynamics using modified flow. In both settings the modified continuous dynamics visually track the discrete dynamics better than the original continuous dynamics. See appendix $\\mathrm{D}$ for further details. ",
            "(a) Modified Loss "
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Modified flow. Rather than modifying gradient   \nflow with higher order “spatial” derivatives of   \nthe loss function, here we introduce higher order temporal derivatives. We start by assuming the existence of a continuous trajectory $\\theta(t)$ that weaves through the discrete steps taken by gradient descent and then identify the differential equation that generates the trajectory. Rearranging the update equation for gradient descent, $\\bar{\\theta}_{t+1}=\\bar{\\theta_{t}}-\\bar{\\eta}g(\\theta_{t})$ , and assuming $\\theta(t)\\stackrel{=}{=}\\theta_{t}$ and $\\theta(t\\!+\\!\\eta)\\!=\\!\\overline{{\\theta}}_{t+1}$ , gives the equality $\\begin{array}{r}{-g(\\theta_{t})=\\frac{\\theta(t+\\eta)-\\theta(t)}{\\eta}}\\end{array}$ , which Taylor expanding the right side results in the differential equation −$\\begin{array}{r}{-g(\\theta_{t})=\\frac{d\\theta}{d t}+\\frac{\\eta}{2}\\frac{d^{2}\\theta}{d t^{2}.}+O(\\eta^{2})}\\end{array}$ . Notice that in the limit as $\\eta\\rightarrow0$ we regain gradient flow. For small $\\eta$ , we obtain a modified version of gradient flow with an additional second-order term, ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d\\theta}{d t}=-g(\\theta)-\\frac{\\eta}{2}\\frac{d^{2}\\theta}{d t^{2}}.\n$$",
        "text_format": "latex",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "This approach to modifying first-order differential equation with higher order temporal derivatives was applied by Kovachki & Stuart (2019) to construct a more realistic continuous model for momentum, as illustrated in Fig. 4. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "6 COMBINING SYMMETRY AND MODIFIED GRADIENT FLOW TO DERIVE EXACT LEARNING DYNAMICS ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "As shown in section 4, each symmetry results in a conserved quantity under gradient flow. We now study how weight decay, momentum, stochastic gradients, and finite learning rates all interact to break these conservation laws. Remarkably, even when using a more realistic continuous model for stochastic gradient descent, as discussed in section 5, we can derive exact learning dynamics for the previously conserved quantities. To do this we (i) consider a realistic continuous model for SGD, (ii) project these learning dynamics onto the generator vector fields $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ associated with each symmetry, (iii) harness the geometric constraints from section 3 to derive simplified ODEs, and (iv) solve these ODEs to obtain exact dynamics for the previously conserved quantities. For simplicity, we first consider the continuous model of SGD incorporating weight decay and modified loss, but not momentum and stochasticity 3 . In this setting, the exact dynamics, as fully derived in appendix E, for the parameter combinations tied to the symmetries are, ",
        "page_idx": 6
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{l}{{\\langle\\theta(t),\\mathbb{1}_{A}\\rangle=e^{-\\lambda t}\\langle\\theta(0),\\mathbb{1}_{A}\\rangle}}\\\\ {{\\displaystyle|\\theta_{A}(t)|^{2}=e^{-2\\lambda t}|\\theta_{A}(0)|^{2}+\\eta\\int_{0}^{t}e^{-2\\lambda(t-\\tau)}\\left|g_{A}\\right|^{2}d\\tau}}\\\\ {{\\displaystyle|\\theta_{A_{1}}(t)|^{2}-|\\theta_{A_{2}}(t)|^{2}=}}\\\\ {{\\displaystyle e^{-2\\lambda t}(|\\theta_{A_{1}}(0)|^{2}-|\\theta_{A_{2}}(0)|^{2})+\\eta\\int_{0}^{t}e^{-2\\lambda(t-\\tau)}\\left(\\left|g_{\\theta_{A_{1}}}\\right|^{2}-\\left|g_{\\theta_{A_{2}}}\\right|^{2}\\right)d\\tau}}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Notice how these equations are equivalent to the conservation laws derived in section 4 when $\\eta=\\lambda=0$ . Remarkably, even in typical hyperparameter settings (weight decay, stochastic batches, finite learning rates), these solutions match nearly perfectly with empirical results 4 from modern neural networks (VGG-16) trained on real-world datasets (Tiny ImageNet), as shown in Fig. 5. We will now discuss each equation individually. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Translation dynamics. For parameters with translation symmetry, equation 18 implies that the sum of these parameters $(\\langle\\theta_{A}(t),\\Bar{\\mathbb{1}}\\rangle)$ decays exponentially to zero at a rate proportional to the weight decay. Equation 18 does not directly depend on the learning rate $\\eta$ nor any information of the dataset or task. This is due to the lack of curvature in the gradient field for these parameters (as shown in Fig. 2). This implies that at initialization we can deterministically predict the trajectory for the parameter sum as simple exponential functions with a rate defined by the weight decay. The first row in Fig. 5 demonstrates this qualitatively, as all trajectories are smooth exponential functions that converge faster for increasing levels of weight decay. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Scale dynamics. For parameters with scale symmetry, equation 19 implies that the norm for these parameters $(|\\theta_{A}|^{2})$ is the sum of an exponentially decaying memory of the norm at initialization and an exponentially weighted integral of gradient norms accumulated through training. Compared to the translation dynamics, the scale dynamics do depend on the data through the gradient norms accumulated throughout training. Without weight decay $\\lambda=0$ , the first term stays constant and the second term grows monotonically. With weight decay $\\lambda>0$ , the first term decays monotonically to zero, while the second term can decay or grow, but always stays positive. The second row in Fig. 5 demonstrates these qualitative relationships. Without weight decay the norms increase monotonically as predicted and with weight decay the dynamics are non-monotonic and present more complex behavior. To better understand the forces driving these complex dynamics, we can examine the time derivative of equation 19, ",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/33e3f567f7bb1b2a0167caec48555fcd2a4a0d09b9ef8569aad3f4b4b4ec0217.jpg",
        "img_caption": [
            "Figure 5: Exact dynamics of VGG-16 on Tiny ImageNet. We plot the column sum of the final linear layer (top row) and the difference between squared channel norms of the fifth and fourth convolutional layer (bottom row) of a VGG-16 model without batch normalization. We plot the squared channel norm of the second convolution layer (middle row) of a VGG-16 model with batch normalization. Both models are trained on Tiny ImageNet with SGD with learning rate $\\eta=0.1$ , weight decay $\\lambda$ , batch size $S=256$ , for 100 epochs . Colored lines are empirical and black dashed lines are the theoretical predictions from equations (18), (19), and (20). See appendix J for more details on the experiments. "
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d}{d t}|\\theta_{A}(t)|^{2}=-2\\lambda|\\theta_{A}(t)|^{2}+\\eta\\left|g_{A}\\right|^{2}.\n$$",
        "text_format": "latex",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "From this equation we see that there is a competition between a centripetal effect due to weight decay $(-2\\lambda|\\theta_{\\mathcal{A}}(t)|^{2})$ and a centrifugal effect due to discretization $(\\eta\\left|g_{\\dot{A}}\\right|^{2})$ . The centripetal effect due to weight decay is a direct consequence of its regularizing influence, pulling the parameters towards the origin. The centrifugal effect due to discretization originates from the spherical geometry of the gradient field in parameter space – because scale symmetry implies the gradient is always orthogonal to the parameter itself, each discrete update with a finite learning rate effectively pushes the parameters away from the origin. At the stationary state of the dynamics, these forces will balance leading the dynamics of these parameters to be constrained to the surface of a high-dimensional sphere. In particular, at stationarity, then $\\begin{array}{r}{\\frac{d}{d t}|\\theta(t)|^{2}=0}\\end{array}$ , which rearranging equation (21) gives the condition $\\begin{array}{r}{\\omega(t)\\equiv\\left|\\frac{d\\theta}{d t}\\right|/|\\theta|=\\sqrt{\\frac{2\\lambda}{\\eta}}}\\end{array}$ q. Consistent with the results of Wan et al. (2020), this implies that at stationarity the angular speed $\\omega(t)$ of the weights is constant and governed only by the learning rate $\\eta$ and weight decay constant $\\lambda$ . When considering stochasticity explicitly, as explained in appendix $\\boldsymbol{\\mathrm F}$ ,then these dynamics also depend on the covariance of the gradient noise $\\Sigma(\\theta)$ and batch size $S$ .",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Rescale dynamics. For parameters with rescale symmetry, equation (20) is the sum of an exponentially decaying memory of the difference in norms at initialization and an exponentially weighted integral of difference in gradient norms accumulated through training. Similar to the scale dynamics, the rescale dynamics do depend on the data through the gradient norms, however unlike the scale dynamics we have no guarantee that the integral term is always positive. This leads to quite sophisticated, complex dynamics, consistent with the third row in Fig. 5. Despite the complexity, our theory, nevertheless, quantitatively matches the empirics. The only apparent pattern from the empirics is that for large enough weight decay, the regularization dominates any complexity introduced by the gradient norms and the difference in parameter norms decays exponentially to zero. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Harmonic oscillation with momentum. We ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "will now consider a continuous model of SGD with momentum. As discussed in appendix G, we consider the continuous model incorporating weight decay, momentum, stochasticity, and modified flow. Under this model, the solutions we obtain take the form of driven harmonic oscillators where the driving force is given by the gradient norms, the friction is defined by the momentum constant, the spring coefficient is defined by the regularization rate, and the mass is defined by the the learning rate and momentum constant. For most standard hyperparameter choices, these solutions are in the overdamped setting and align well with the first-order solutions for SGD without momentum up to a time rescaling, as shown in the left and middle panel of Fig. 6. However, for large values of beta we can push the solution into the underdamped regime where we would expect harmonic oscil",
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/cd487794b6a155595d774dd8e5e2f0cb05a52b492706ee17021654f5a33b39b7.jpg",
        "img_caption": [
            "Figure 6: Momentum leads to harmonic oscillation. We plot the column sum of the final linear layer of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with SGD with learning rate $\\eta\\:=\\:0.1$ , weight decay $\\lambda=5\\times10^{-3}$ h size $S=256$ and momentum $\\beta\\,\\in\\,\\{0,0.9,0.99\\}$ and black dashed lines are the theoretical predic∈{ }. Colored lines are empirical tions from equations (34). "
        ],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "lation and indeed, we can empirically verify our predictions, even at scale for VGG-16 trained on Tiny ImageNet, as in right panel of Fig. 6. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "7 CONCLUSION ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Despite being the central guiding principle in the exploration of the physical world Anderson (1972); Gross (1996), symmetry has been underutilized in understanding the mechanics of neural networks. In this paper, we constructed a unifying theoretical framework harnessing the geometric properties of symmetry and more realistic continuous equations for learning dynamics that model weight decay, momentum, stochasticity, and discretization. We use this framework to derive exact dynamics for meaningful combinations of parameters, which we experimentally verified on large scale neural networks and datasets. For example, in the case of a VGG-16 model with batch normalization trained on Tiny-ImageNet (one of the model/dataset combinations we considered in section 6) there are 12 ,751 distinct parameter combinations whose dynamics we can analytically describe. Overall, this work provides a first step towards understanding the mechanics of learning in neural networks without unrealistic simplifying assumptions. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "A CKNOWLEDGEMENTS ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "We thank Daniel Bear, Lauren Gillespie, Kyogo Kawaguchi, Brett Larsen, Eshed Margalit, Alain Studer, Sho Sugiura, Umberto Maria Tomasini, and Atsushi Yamamura for helpful discussions. This work was funded in part by the IBM-Watson AI Lab. D.K. thanks the Stanford Data Science Scholars program for support. J.S. thanks the Mexican National Council of Science and Technology (CONACYT) for support. S.G. thanks the James S. McDonnell and Simons Foundations, NTT Research, and an NSF CAREER Award for support. D.L.K.Y thanks the McDonnell Foundation (Understanding Human Cognition Award Grant No. 220020469), the Simons Foundation (Collaboration on the Global Brain Grant No. 543061), the Sloan Foundation (Fellowship FG-2018-10963), the National Science Foundation (RI 1703161 and CAREER Award 1844724), and the DARPA Machine Common Sense program for support and the NVIDIA Corporation for hardware donations. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "REFERENCES ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Jing An, Jianfeng Lu, and Lexing Ying. Stochastic modified equations for the asynchronous stochastic gradient descent. Information and Inference: A Journal of the IMA , 2018. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Philip W Anderson. More is different. Science , 177(4047):393–396, 1972. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. arXiv preprint arXiv:1810.02281 , 2018a. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. arXiv preprint arXiv:1802.06509 , 2018b. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. arXiv preprint arXiv:1812.03981 , 2018c. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In Advances in Neural Information Processing Systems , pp. 8141–8150, 2019. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one , 10(7), 2015. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks , 2(1):53–58, 1989. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "David GT Barrett and Benoit Dherin. Implicit gradient regularization. arXiv preprint arXiv:2009.11162 , 2020. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA) , pp. 1–10. IEEE, 2018. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de la Fuente, Vishal Subbiah, and Michael James. Online normalization for training neural networks. In Advances in Neural Information Processing Systems , pp. 8433–8443, 2019. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in Neural Information Processing Systems , pp. 5225–5235, 2017. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. arXiv preprint arXiv:1901.08572 , 2019. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems ,pp. 384–395, 2018a. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054 , 2018b.   \nYuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu. Uniform-in-time weak error analysis for stochastic gradient descent algorithms via diffusion approximation. arXiv preprint arXiv:1902.00635 , 2019.   \nStanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. Adv. Neural Inf. Process. Syst. , 33, 2020.   \nSebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborová. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In Advances in Neural Information Processing Systems , pp. 6981–6991, 2019.   \nDavid J Gross. The role of symmetry in fundamental physics. Proceedings of the National Academy of Sciences , 93(25):14256–14259, 1996.   \nElad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate normalization schemes in deep networks. In Advances in Neural Information Processing Systems ,pp. 2160–2170, 2018.   \nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.   \nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems , pp. 8571–8580, 2018.   \nStanisław Jastrz˛ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623 ,2017.   \nKenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information processing systems , pp. 586–594, 2016.   \nNikola B Kovachki and Andrew M Stuart. Analysis of momentum methods. arXiv preprint arXiv:1906.04285 , 2019.   \nDaniel Kunin, Jonathan M Bloom, Aleksandrina Goeva, and Cotton Seed. Loss landscapes of regularized linear autoencoders. arXiv preprint arXiv:1901.08168 , 2019.   \nHarold Kushner and G George Yin. Stochastic approximation and recursive algorithms and applications , volume 35. Springer Science & Business Media, 2003.   \nAndrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer learning in deep linear networks. In International Conference on Learning Representations (ICLR) ,2018.   \nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In Advances in neural information processing systems , pp. 8572–8583, 2019.   \nQianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic gradient algorithms. In International Conference on Machine Learning , pp. 2101–2110, 2017.   \nZhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. arXiv preprint arXiv:1910.07454 , 2019.   \nZhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing Systems , 33, 2020.   \nZhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic differential equations (sdes). arXiv preprint arXiv:2102.12470 , 2021.   \nTengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. In The 22nd International Conference on Artificial Intelligence and Statistics , pp. 888–896, 2019.   \nStephan Mandt, Matthew D Hoffman, and David M Blei. Continuous-time limit of stochastic gradient descent revisited. NIPS-2015 , 2015.   \nStephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate bayesian inference. The Journal of Machine Learning Research , 18(1):4873–4907, 2017.   \nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018.   \nBehnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems , pp. 2422–2430, 2015.   \nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature , 323(6088):533–536, 1986.   \nDavid Saad and Sara Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. Advances in neural information processing systems , 8:302–308, 1995.   \nTim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Advances in neural information processing systems , pp. 901–909, 2016.   \nAndrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120 , 2013.   \nAndrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A. , May 2019.   \nSamuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451 , 2017.   \nWeijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling nesterov’s accelerated gradient method: Theory and insights. In Advances in neural information processing systems , pp. 2510–2518, 2014.   \nHidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467 , 2020.   \nTwan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint arXiv:1706.05350 , 2017.   \nRuosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics of deep neural networks with batch normalization and weight decay. arXiv preprint arXiv:2006.08419 , 2020.   \nRF Warming and BJ Hyett. The modified equation approach to the stability and accuracy analysis of finite-difference methods. Journal of computational physics , 14(2):159–179, 1974.   \nSho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint arXiv:1810.00004 , 2018.   \nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay regularization. arXiv preprint arXiv:1810.12281 , 2018.   \nZhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects. arXiv preprint arXiv:1803.00195 , 2018. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "A SYMMETRY AND GEOMETRY ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Here we derive in detail the geometric properties of the loss landscape introduced by symmetry, as discussed in section 3. Consider a nction $f(\\theta)$ here $\\theta\\in\\mathbb{R}^{m}$ . This functio possesse if it is nt un $\\psi$ $G$ on the eter vector θ, i.e., if $\\theta\\mapsto\\psi(\\theta,\\dot{\\alpha})$ 7→ $\\nabla f$ where ∇constraints on of a neural network, yields fifteen distinct equations describing the geometrical relationships between and Hess tionship betwe $\\alpha\\in G$ ∈, then ∇$\\nabla f$ and $\\mathbf{H}f$ $F\\left(\\theta,\\alpha\\right)=f\\left(\\psi(\\theta,\\alpha)\\right)=f(\\theta)$ he gradient $\\mathbf{H}f$ . Considering these general formulae when he original function ∇$\\nabla F$ and Hessi $f(\\theta)$ $\\mathbf{H}F$ . This relations of the composition r all $(\\theta,\\alpha)$ . Symme $f(\\theta)={\\mathcal{L}}(\\theta)$ $F(\\theta,\\alpha)$ Lwith the gradient , the training loss rces a geometric escribed by five architectural symmetries and the loss landscapes, some of which have been identified individually in existing literature (Table 1). ",
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/923947c7359ba88d4d65b1dc281272fed496a9776ce6c3f4a12b79eddd9d0259.jpg",
        "table_caption": [],
        "table_footnote": [],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "1. Ioffe & Szegedy (2015) has motivated the effectiveness of Batch Normalization by its scale invariant property. In particular, they have noted that Batch Normalization will stabilize back-propagation because “The scale does not affect the layer Jacobian nor, consequently, the gradient propagation. Moreover, larger weights lead to smaller gradients, and Batch Normalization will stabilize the parameter growth.”   \n2. Van Laarhoven (2017) has then shown that the role of $L_{2}$ regularization when combined with batch Ioffe & Szegedy (2015), weight Salimans & Kingma (2016) or layer Ba et al. (2016) normalization is not to regularize the function, but to effectively control the learning rate.   \n3. Zhang et al. (2018) has thoroughly studied mechanisms of weight decay regularization and derived various geometric properties of loss landscapes along the way. Arora et al. (2018c) has theoretically analyzed the automatic tuning property of learning rate in networks with Batch Normalization.   \n4. Li et al. (2020) studied the interaction of weight decay and batch normalization in the setting of stochastic gradients.   \n5. Neyshabur et al. (2015) has identified that SGD is not rescale equivariant even when network outputs are rescale invariant. This is a problem because gradient descent performs very poorly on unbalanced networks due to the lack of equivariance. Motivated by the issue, they have introduced a new optimizer, Path-SGD, that is rescale equivariant.   \n6. Arora et al. (2018b) proved that the weights of linear artificial neural networks satisfy strong balancedness property.   \n7. Du et al. (2018a) studied implicit regularization in networks with homogeneous activation functions. To do that, they showed conservation law of parameters with rescale invariance.   \n8. Liang et al. (2019) have proposed a new capacity measure to study generalization that respects rescale invariance of networks. Along the way, they showed geometric properties of gradients and Hessians for networks with rescale invariance. However, their results were restricted to a layer without biases.   \n9. Tanaka et al. (2020) have proved gradient properties of parameters with rescale invariance at neuron level including biases. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "If a function $f$ posses a symmetry, then there exists a geometric constraint on the relationship between the gradients $\\nabla F$ and $\\nabla f$ at all $(\\theta,\\alpha)$ ,",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla F=\\left(\\partial_{\\theta}F\\right)=\\left(\\partial_{\\psi}F\\partial_{\\theta}\\psi\\right)=\\left(\\nabla f\\right).\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The top element of the gradient relationship, $\\partial_{\\theta}F$ , evaluated at any $(\\theta,\\alpha)$ , yields the property ",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n\\partial_{\\theta}\\psi\\;\\nabla f|_{\\psi(\\theta,\\alpha)}=\\nabla f|_{\\theta}\\,,\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "which describes how the symmetry transformation affects the function’s gradients despite leaving the output unchanged. The bottom element of the gradient relationship, $\\partial_{\\alpha}F$ , evaluated at the identity element of $G$ yields the property ",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\nabla f,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "which implies th gradient $\\nabla f$ is perpendicul tor field $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ that generates the symmetry, for all θ. In the specific setting when $f(\\theta)={\\mathcal{L}}(\\theta)$ L, the training loss of a neural network, these gradient properties are summarized in Table 2 for the translation, scale, and rescale symmetries described in section 3. ",
        "page_idx": 13
    },
    {
        "type": "table",
        "img_path": "images/87b0eaca0e352fe0c3eab39a0b07189be88ab19cabb0312a83094b645c59a933.jpg",
        "table_caption": [],
        "table_footnote": [],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Notice that the first row of Table 2 implies that symmetry transformations affect learning dynamics governed by gradient descent for scale and rescale symmetries, while it does not for translation symmetry. These observations are in agreement with Van Laarhoven (2017) who has shown that effective learning rate is inversely proportional to the norm of parameters immediately preceding the batch normalization layers and Neyshabur et al. (2015) who have noticed that SGD is not invariant to the rescale symmetry that the network output respects and proposed Path-SGD to fix the discrepancy. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "A.2 HESSIAN GEOMETRY ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "If a function $f$ posses a symmetry, then there also exists a geometric constraint on the relationship between the Hessian matrices $\\mathbf{H}F$ and $\\mathbf{H}f$ at all $(\\theta,\\alpha)$ ,",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r}{\\mathbf{H}F=\\left(\\begin{array}{l l}{\\partial_{\\theta}^{2}F}&{\\partial_{\\theta}\\partial_{\\alpha}F}\\\\ {\\partial_{\\alpha}\\partial_{\\theta}F}&{\\partial_{\\alpha}^{2}F}\\end{array}\\right)}\\\\ {=\\left(\\begin{array}{l l}{\\partial_{\\psi}F\\partial_{\\theta}^{2}\\psi+\\partial_{\\psi}^{2}F(\\partial_{\\theta}\\psi)^{2}}&{\\partial_{\\psi}^{2}F\\partial_{\\theta}\\psi\\partial_{\\alpha}\\psi+\\partial_{\\psi}F\\partial_{\\theta}\\partial_{\\alpha}\\psi}\\\\ {\\partial_{\\psi}^{2}F\\partial_{\\theta}\\psi\\partial_{\\alpha}\\psi+\\partial_{\\psi}F\\partial_{\\theta}\\partial_{\\alpha}\\psi}&{(\\partial_{\\alpha}\\psi)^{\\mathsf{T}}\\partial_{\\psi}^{2}F\\partial_{\\alpha}\\psi+(\\partial_{\\psi}F)^{\\mathsf{T}}\\partial_{\\alpha}^{2}\\psi}\\end{array}\\right)=\\left(\\begin{array}{l l}{\\mathbf{H}f}&{0}\\\\ {0}&{0}\\end{array}\\right).}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The first diagonal element, $\\partial_{\\theta}^{2}F$ , evaluated at any $(\\theta,\\alpha)$ , yields the property ",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n\\partial_{\\theta}^{2}\\psi\\left.\\nabla f\\right|_{\\psi(\\theta,\\alpha)}+(\\partial_{\\theta}\\psi)^{2}\\left.\\mathbf{H}f\\right|_{\\psi(\\theta,\\alpha)}=\\mathbf{H}f\\right|_{\\theta},\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "which describes how the symmetry transformation affects the function’s Hessian despite leaving the output unchanged. The off-diagonal elements, $\\partial_{\\theta}\\partial_{\\alpha}F=\\partial_{\\alpha}\\partial_{\\theta}F$ , evaluated at the identity element of $G$ yields the property ",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n{\\bf H}f[\\partial_{\\theta}\\psi|_{\\alpha=I}]\\partial_{\\alpha}\\psi|_{\\alpha=I}+[\\partial_{\\theta}\\partial_{\\alpha}|_{\\alpha=I}]\\psi\\nabla f=0,\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "which implies the geometry of gradient and Hessian are connected through the action of the symmetry. Lastly, the second diagonal element, $\\partial_{\\alpha}^{2}F$ , represents an equality, evaluated at the identity element of $G$ yields the property ",
        "page_idx": 13
    },
    {
        "type": "equation",
        "text": "$$\n(\\partial_{\\alpha}\\psi|_{\\alpha=I})^{\\boldsymbol{\\mathsf{T}}}\\mathbf{H}f(\\partial_{\\alpha}\\psi|_{\\alpha=I})+\\langle\\nabla f,\\partial_{\\alpha}^{2}\\psi|_{\\alpha=I}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "which combines the geometric relationships in equation 23 and equation 25. In the specific setting when $f(\\theta)={\\mathcal{L}}(\\theta)$ , the training loss of a neural network, these Hessian properties are summarized in Table 2 for the translation, scale, and rescale symmetries described in section 3. ",
        "page_idx": 14
    },
    {
        "type": "table",
        "img_path": "images/b5792ebd41e3a09fb52a5c30e3728db8ee02a0452a3da246702703d8110bd0d0.jpg",
        "table_caption": [
            "Table 3: Geometric properties of the Hessian. The Hessian matrix of a neural network with either translation, scale or rescale symmetry observe certain geometric properties no matter the dataset or step in training. "
        ],
        "table_footnote": [],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The translation, scale, and rescale symmetries identified in section 3 is not an exhaustive list of the symmetries present in neural network architectures. For example, more general rescale symmetries can be defined by the group quadratic activation functions. A stronger form of rescale symmetry also occurs for linear networks $\\mathrm{GL_{1}^{+}(\\mathbb{R})}$ and action $\\theta\\mapsto\\alpha_{\\mathcal{A}_{1}}^{k_{1}}\\odot\\alpha_{\\mathcal{A}_{2}}^{-k_{2}}\\odot\\theta$ A A , which occur in networks with under the action of the group $G L_{k}^{+}(\\mathbb{R})$ of $k\\times k$ invertible matrices, as noticed previously by Arora et al. (2018a); Du et al. (2018a). Interestingly, some of the gradient and Hessian properties for scale symmetry can also be easily proven as consequences of Euler’s Homogeneous Function Theorem when $k=0$ .",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "BCONSERVATION LAWS ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Here we repeat Theorem 1 and provide a detailed derivation. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Theorem. Symmetry and conservation laws in neural networks. Every differentiable symmetry $\\psi(\\alpha,\\theta)$ $\\begin{array}{r}{\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0}\\end{array}$ of the loss that satisfies through learning under gradient flow. $\\langle\\theta,[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]g(\\theta)\\rangle=0$ has the corresponding conservation law ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Proof. Project the gradient flow learning dynamics, $\\begin{array}{r}{\\frac{d\\theta}{d t}=-g(\\theta)}\\end{array}$ , onto the vector field that generates the symmetry $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ , evaluated at the identity element, ",
        "page_idx": 14
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\frac{d\\theta}{d t},\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=\\langle-g(\\theta),\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0.\n$$",
        "text_format": "latex",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "We can factor the left side of this equation as ",
        "page_idx": 14
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{l}{\\displaystyle\\langle\\frac{d\\theta}{d t},\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle-\\langle\\theta,\\displaystyle\\frac{d}{d t}\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle}\\\\ {\\displaystyle=\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle-\\langle\\theta,\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}\\frac{d\\theta}{d t}\\rangle}\\\\ {\\displaystyle=\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle+\\langle\\theta,\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}g(\\theta)\\rangle}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "By assumption, $\\langle\\theta,[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]g(\\theta)\\rangle=0$ , implying $\\begin{array}{r}{\\frac{d}{d t}\\langle\\theta,\\partial_{\\alpha}\\psi|_{\\alpha=I}\\rangle=0}\\end{array}$ .",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "The condition consider in section 3. For translation symmetry, $\\langle\\theta,[\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}]g(\\theta)\\rangle=0$ holds $\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}=0$ |. For scale symmetry, n, scale, and rescale $\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}=$ |$I$ and $\\langle\\theta_{A},g(\\theta_{A})\\rangle\\;=\\;0$ . For rescale symmetry, $\\partial_{\\alpha}\\partial_{\\theta}\\psi|_{\\alpha=I}\\;=\\;{\\binom{I}{0}}\\;\\;\\;\\;0\\,\\,\\,\\,$ and $\\langle\\theta_{A_{1}},g(\\theta_{A_{1}})\\rangle\\mathrm{~-~}$ $\\langle\\theta_{A_{2}},g(\\theta_{A_{2}})\\rangle=0.$ .",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "CLIMITING DIFFERENTIAL EQUATIONS FOR LEARNING RULES ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Here we identify ordinary differential equations whose first-order discretization give rise to the gradient descent and classical momentum algorithms. These differential equations can be understood as the limiting dynamics for their respective discrete algorithms as the learning rate $\\eta\\rightarrow0$ .",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "C.1 GRADIENT DESCENT ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Gradient descent with learning rate $\\eta$ is given by the update equation ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n\\theta_{k+1}=\\theta_{k}-\\eta g(\\theta_{k}),\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "and initial condition $\\theta_{0}$ . Rearranging the difference between consecutive updates gives ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{\\theta_{k+1}-\\theta_{k}}{\\eta}=-g(\\theta_{k}).\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "This is a discretization with step size $\\eta$ of the first order ODE ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n{\\frac{d}{d t}}\\theta=-g(\\theta),\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "where we used the forward Euler discretization $\\begin{array}{r}{\\frac{d}{d t}\\theta_{k}=\\frac{\\theta_{k+1}-\\theta_{k}}{\\eta}}\\end{array}$ . This ODE is commonly referred to as gradient flow .",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "C.2 CLASSICAL MOMENTUM ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Classical momentum with learning rate $\\eta$ , damping coefficient $\\alpha$ , and momentum parameter $\\beta$ , is given by the update equation 5 ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{l}{{v_{k+1}}=\\beta{v_{k}}+(1-\\alpha)g(\\theta_{k}),}\\\\ {{\\theta_{k+1}}=\\theta_{k}-\\eta{v_{k+1}},}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "and initial conditions $v_{0}=0$ and some $\\theta_{0}$ . The difference between consecutive updates is ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}{\\theta_{k+1}-\\theta_{k}=-\\eta v_{k+1}}&{}\\\\ {=-\\eta\\beta v_{k}-\\eta(1-\\alpha)g(\\theta_{k})}\\\\ {=\\beta\\left(\\theta_{k}-\\theta_{k-1}\\right)-\\eta(1-\\alpha)g(\\theta_{k}).}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Rearranging this equation we get ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{\\theta_{k+1}-\\theta_{k}}{\\eta(1-\\alpha)}-\\beta\\frac{\\theta_{k}-\\theta_{k-1}}{\\eta(1-\\alpha)}=-g(\\theta_{k}).\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "This is a discretization with step size $\\eta(1-\\alpha)$ of the first order ODE ",
        "page_idx": 15
    },
    {
        "type": "equation",
        "text": "$$\n(1-\\beta)\\frac{d}{d t}\\theta=-g(\\theta),\n$$",
        "text_format": "latex",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "where we used the forward Euler discretization $\\begin{array}{r}{\\frac{d}{d t}\\theta_{k}=\\frac{\\theta_{k+1}-\\theta_{k}}{\\eta(1-\\alpha)}}\\end{array}$ and backward Euler discretization $\\begin{array}{r}{\\frac{d}{d t}\\theta_{k}\\,=\\,\\frac{\\theta_{k}-\\theta_{k-1}}{\\eta(1-\\alpha)}}\\end{array}$ . We will refer to this equation as momentum flow −. A more detailed derivation for this ODE under Nesterov variants of classical momentum can be found in Su et al. (2014) and −Kovachki & Stuart (2019). ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "DMODIFIED EQUATION A NALYSIS ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Gradient descent always moves in the direction of steepest descent on a loss function, however, due to the finite nature of the learning rate, it fails to remain on the continuous steepest descent path. The divergence between the discrete and continuous trajectories fundamentally depends on the learning rate and the curvature of the loss. It is thus natural to assume there exists a more realistic continuous model for SGD that incorporates both these terms in a non-trivial way. How can we better model the discrete dynamics of gradient descent with a continuous differential equation? ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Intuition from finite difference methods. To answer this question we will take inspiration from tools developed for finite difference methods. Finite difference methods are a class of numerical techniques for approximating derivatives in the analysis of partial differential equations (PDE). These approximations are applied iteratively to construct numerical solutions to a PDE given some initial conditions. However, this discretization process introduces numerical artifacts, which can lead to a significant difference between the numerical solution and the true solution for the PDE. Modified equation analysis is a method for understanding this difference by modeling the numerical artifacts as higher-order spatial or temporal derivatives modifying the original PDE. This approach can be used to construct modified continuous dynamics that better approximate discrete dynamics, as illustrated in Fig. 7. ",
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/25a650adcbe508c317420780f53c105276de8823aa2fad1a21ab9c94da8b354d.jpg",
        "img_caption": [
            "Figure 7: the vector field Circular motion. $f(x)={\\left[\\begin{array}{l l}{0}&{\\!\\!\\!-1}\\\\ {1}&{0}\\end{array}\\right]}:$ Consider 'xand the discrete dynamics $\\overline{{x_{t+1}}}\\overline{{=x_{t}}}+\\eta f(x_{t})$ (black dots), the continuous dynamics ${\\dot{x}}=f(x)$ (blue line), and the modified continuous dynamics $\\begin{array}{r}{\\dot{x}\\,=\\,f(x)+\\frac{\\eta}{2}x}\\end{array}$ .We visualize the trajectories given by these dynamics using the initial condition $\\overline{{x_{0}}}^{\\overline{{\\mathbf{\\alpha}}}}=\\mathbf{\\alpha}\\left[\\mathbf{\\alpha}_{1}\\mathbf{\\alpha}_{0}\\right]^{\\overline{{\\mathbf{\\alpha}}}}$ (white circle) and a step size $\\eta~=~0.1$ .As we can see, the modified continuous trajectory better matches the discrete trajectory. "
        ],
        "img_footnote": [],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "D.1 MODIFIED LOSS ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Taking inspiration from modified equation analysis, Li et al. (2017); Feng et al. (2019) and most recently Barrett & Dherin (2020), demonstrate that the trajectory given by gradient descent closely follows the steepest descent path of a modified loss fu explained in Barrett & Dherin (2020), assume there exists a modified vector field with corrections tion $\\widetilde{\\mathcal{L}}$ , rather than the original loss $\\mathcal{L}$ .$g_{i}$ in powers of the learning rate to the original vector field gthat the discrete dynamics follow. In other words, rather than considering the dynamics given by gradient flow, modified differential equation, $\\scriptstyle{\\frac{d}{d t}}\\theta\\;=\\;-g(\\theta)$ −, we consider the ",
        "page_idx": 16
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d}{d t}\\theta=-g(\\theta)+\\eta g_{1}(\\theta)+\\eta^{2}g_{2}(\\theta)+\\ldots.\n$$",
        "text_format": "latex",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Truncating the modified vector field up to the order $\\eta$ and using backward error analysis we can derive that the first-order correction $\\dot{g}_{1}~=~-{\\textstyle\\frac{1}{2}}H g$ , which is the gradient of the squared norm $-\\textstyle{\\frac{\\eta}{4}}\\left|\\nabla{\\mathcal{L}}\\right|^{2}$ loss $\\begin{array}{r}{\\widetilde{\\mathcal{L}}=\\mathcal{L}+\\frac{\\eta}{4}|\\nabla\\mathcal{L}|^{2}}\\end{array}$ .runcated modified differential equation is simply gradient flow on a modified Convex quadratic loss. eTo illustrate modified loss, we will consider the trajectories of gradient descent wt, gradient flow $\\hat{w}(t)$ , and modified gradient flow $\\tilde{w}(t)$ on the convex quadratic loss $\\textstyle{\\mathcal{L}}(w)\\;=\\;{\\frac{1}{2}}w^{\\mathsf{\\bar{T}}}A w$ ,here $A~\\succ~0$ is some sitive definite matrix, as shown in Fig. 4. For a finite learning rate ηand initial condition $w_{0}$ , gradient descent is given by the update formula $w_{t+1}\\,=\\,w_{t}\\,-\\,\\eta A w_{t}$ equation. For the initial condition . Gradient flo $w_{0}$ , the resulting initial value problem can be solved exactly s defined as $\\begin{array}{r}{\\frac{d}{d t}\\hat{w}\\,=\\,-A\\hat{w}}\\end{array}$ −, a linear first-order differential giving the gradient flow trajectory $\\hat{w}(t)=S^{-1}e^{-\\Lambda t}\\bar{S}w_{0}$ , where $A=S^{-1}\\Lambda S$ is the diagonalization of the curvacture matrix. For learning ra $\\eta$ is $\\begin{array}{r}{\\tilde{\\mathcal{L}}(w)\\,=\\,\\frac{1}{2}w^{\\intercal}A w+\\frac{\\eta}{4}w^{\\intercal}A^{2}w}\\end{array}$ and modified gradient flow is defined as $\\begin{array}{r}{\\frac{d}{d t}\\tilde{w}=-A\\tilde{w}-\\frac{\\eta}{2}A^{2}\\tilde{w}}\\end{array}$ . For the initial condition $w_{0}$ , the resulting initial value problem can be solved exactly giving the modified gradient flow trajectory $\\tilde{w}(t)=\\overline{{S^{-1}}}e^{-\\left(\\Lambda+\\frac{\\eta}{2}\\Lambda^{\\hat{2}}\\right)t}S w_{0}.$ .",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Rather than modify gradient flow with higher order “spatial” derivatives of the loss function, here we introduce higher order temporal derivatives. We start by assuming the existence of a continuous trajectory $\\theta(t)$ that weaves through the discrete steps taken by SGD and then identify the differential equation that generates the trajectory. Rearranging the update equation for SGD, $\\begin{array}{r}{\\theta_{t+1}\\dot{=}\\theta_{t}\\!-\\!\\eta g_{B}(\\theta_{t})}\\end{array}$ ,and assuming $\\theta(t)\\,=\\,\\theta_{t}$ and $\\theta(t+\\eta)=\\theta_{t+1}$ , gives the equality $\\begin{array}{r}{-g_{B}(\\theta_{t})\\,=\\,\\frac{\\theta(t+\\eta)-\\theta(t)}{\\eta}}\\end{array}$ , which Taylor expanding the righ Notice that in the limit as $\\eta\\rightarrow0$ →results in the differential equation we regain gradient flow. For small −$\\begin{array}{r}{-g_{\\mathcal{B}}(\\theta_{t})=\\frac{d\\theta}{d t}+\\frac{\\eta}{2}\\frac{d^{2}\\theta}{d t^{2}}+O(\\eta^{2})}\\end{array}$ $\\eta,\\eta\\ll1$ ≪, we obtain a modified .version of gradient flow with an additional second-order term. This approach of modifying first-order differential equations with higher order temporal derivatives was applied by Kovachki & Stuart (2019) to modify momentum flow, capturing the harmonic motion of momentum. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Convex quadratic loss. To illustrate modified flow, we will consider the trajectories of momentum $w_{t}$ entum $\\hat{w}(t)$ , and modified momentum flow $\\tilde{w}(t)$ on the convex quadratic loss $\\mathcal{L}(w)=$ ${\\textstyle\\frac{1}{2}}w^{\\intercal}A w$ , wher $A\\succ0$ is some positive defi te matrix, as shown in F finite learning rate η, dampening $\\alpha\\,=\\,0$ , momentum constant β, and initial conditions $v_{0}=0,w_{0}$ , then momentum is given by the pair of recursi $v_{t+1}\\,=\\,\\beta v_{t}\\,+\\,A w_{t}$ and $w_{t+1}=\\,w_{t}\\,-\\,\\eta v_{t+1}$ .Momentum flow is d ned as $\\textstyle(1-\\beta){\\frac{d}{d t}}\\hat{w}=-A\\hat{w}$ , a linear first-order differential equation. For a given initialization $w_{0}$ , the resulting initial value problem can be solved exactly as in the case of gradient flow. Modified momentum flow is defined as second-order differential equation. For a given initialization $\\begin{array}{r}{\\frac{\\eta}{2}(1+\\beta)\\frac{d^{2}}{d t^{2}}\\tilde{w}+(1-\\beta)\\frac{d}{d t}\\tilde{w}=-A\\tilde{w}}\\end{array}$ $w_{0}$ and the assumed initial condition −−, a linear $\\begin{array}{r}{\\frac{d}{d t}\\tilde{w}(0)=0}\\end{array}$ , then the resulting initial value problem can be solved exactly as a system of damped harmonic oscillators. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "EDERIVING THE EXACT LEARNING DYNAMICS OF SGD ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We consider a continuous model of SGD without momentum incorporating weight decay (equation 14) and modified loss (equation 16), such that ",
        "page_idx": 17
    },
    {
        "type": "equation",
        "text": "$$\n\\widetilde{\\mathcal{L}}=\\mathcal{L}_{\\lambda}+\\frac{\\eta}{4}|\\nabla\\mathcal{L}_{\\lambda}|^{2}\n$$",
        "text_format": "latex",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where $\\begin{array}{r}{\\mathcal{L}_{\\lambda}=\\mathcal{L}+\\frac{\\lambda}{2}\\vert\\theta\\vert^{2}}\\end{array}$ is the regularized loss. The gradient of the modified loss is, ",
        "page_idx": 17
    },
    {
        "type": "equation",
        "text": "$$\n\\nabla\\widetilde{\\mathcal{L}}=\\nabla\\mathcal{L}_{\\lambda}+\\frac{\\eta}{2}H_{\\lambda}\\nabla\\mathcal{L}_{\\lambda}=\\left(1+\\frac{\\eta\\lambda}{2}\\right)g+\\left(\\lambda+\\frac{\\eta\\lambda^{2}}{2}\\right)\\theta+\\frac{\\eta}{2}\\left(H g+\\lambda H\\theta\\right),\n$$",
        "text_format": "latex",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where we used $\\nabla{\\mathcal{L}}_{\\lambda}=g+\\lambda\\theta$ and $H_{\\lambda}=H+\\lambda I$ . Thus, the equation of learning we consider is ",
        "page_idx": 17
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{d\\theta}{d t}=-\\nabla{\\widetilde{\\mathcal{L}}}(\\theta)=-\\left(1+\\frac{\\eta\\lambda}{2}\\right)g-\\left(\\lambda+\\frac{\\eta\\lambda^{2}}{2}\\right)\\theta-\\frac{\\eta}{2}\\left(H g+\\lambda H\\theta\\right).\n$$",
        "text_format": "latex",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "To incorporate the effect of stochasticity (equation 31) in this equation of learning we could replace the full-batch gradient $g$ and Hessian $H$ with their stochastic batch counterparts ${\\hat{g}}{B}$ and ${\\hat{H}}_{B}$ respectively. However, careful treatment of these terms using stochastic calculus is needed when integrating the resulting stochastic differential equation, which we discuss at the end of this section. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Translation dynamics. In the case of parameters with translation symmetry, the effect of discretization essentially leaves the dynamics for the constant of learning unchanged. Combining the geometric properties of gradient $(\\langle g,\\bar{\\mathbb{1}}_{\\mathscr{A}}\\rangle=0)$ ) and Hessian $(H\\mathbb{1}_{\\mathcal{A}}=0)$ ) introduced by translation symmetry with the equation of learning (equation 27) gives the differential equation, ",
        "page_idx": 17
    },
    {
        "type": "equation",
        "text": "$$\n\\left\\langle\\frac{d\\theta}{d t}+\\nabla\\widetilde{\\mathcal{L}},\\mathbb{1}_{A}\\right\\rangle=\\left(\\lambda+\\frac{d}{d t}\\right)\\langle\\theta,\\mathbb{1}_{A}\\rangle=0,\n$$",
        "text_format": "latex",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where we used the simplification, ",
        "page_idx": 17
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla\\widetilde{\\mathcal L},\\mathbb{1}_{A}\\rangle=\\left(1+\\frac{\\eta\\lambda}{2}\\right)\\overbrace{\\langle g,\\mathbb{1}_{A}\\rangle}^{\\langle g,\\mathbb{1}_{A}\\rangle=0}+\\left(\\lambda+\\frac{\\eta\\lambda^{2}}{2}\\right)\\langle\\theta,\\mathbb{1}_{A}\\rangle+\\frac{\\eta}{2}(\\overbrace{\\langle{H\\mathrm{d}_{\\mathcal{P}}\\!\\cdot\\!\\mathbb{1}_{A}}\\rangle}^{H\\mathbb{1}_{A}=0}+\\overbrace{\\lambda\\mathrm{/}{H\\theta},\\mathbb{1}_{A}\\rangle}^{H\\mathbb{1}_{A}=0})}\\\\ &{\\quad\\quad\\quad\\quad=\\left(\\lambda+\\frac{\\eta\\lambda^{\\mathcal{Y}}}{\\mathit{/2}}\\right)\\langle\\theta,\\mathbb{1}_{A}\\rangle,}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "and ignored the ${\\cal O}(\\eta\\lambda^{2})$ term as we set the weight decay constant $\\lambda$ to be as small as the learning rate $\\eta$ in practice. The solution to this differential equation is ",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\theta(t),\\mathbb{1}_{A}\\rangle=e^{-\\lambda t}\\langle\\theta(0),\\mathbb{1}_{A}\\rangle.\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Scale dynamics. In the case of parameters with scale symmetry, the effect of discretization does distort the original geometry. A finite learning rate leads to a centrifugal force that monotonically increases the previously conserved quantity $\\bar{(|\\theta|^{2})}$ , while weight decay acts as force decreasin Hessian ( gives the following differential equation, $'H\\theta_{A}=-g_{A})$ A −A ) introduced by scale symmetry with the equation of learning (equation 27) . Combining the geometric constraints on the gradient ( $(\\langle g,\\theta_{\\mathcal{A}}\\bar{\\rangle}=0)$ ⟨A ⟩) and ",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n\\left\\langle\\frac{d\\theta}{d t}+\\nabla\\widetilde{\\mathcal{L}},\\theta_{A}\\right\\rangle=\\left(\\lambda+\\frac{1}{2}\\frac{d}{d t}\\right)\\lvert\\theta_{A}\\rvert^{2}-\\frac{\\eta}{2}\\left\\lvert g_{A}\\right\\rvert^{2}=0,\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "where we used the simplifications ⟨$\\begin{array}{r}{\\langle\\frac{d\\theta}{d t},\\theta_{A}\\rangle=\\frac{1}{2}\\frac{d}{d t}|\\theta_{A}|^{2}}\\end{array}$ ,",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla\\widetilde{\\mathcal L},\\theta_{A}\\rangle=\\left(1+\\displaystyle\\frac{\\eta\\lambda}{2}\\right)\\!\\!\\frac{\\langle\\varrho,\\theta_{A}\\rangle\\!=\\!0}{\\mathcal{\\hat{L}}\\!\\varrho\\!\\cdot\\!\\theta\\!\\mathcal{A}\\!\\rangle}+\\left(\\lambda+\\displaystyle\\frac{\\eta\\lambda^{2}}{2}\\right)\\langle\\theta,\\theta_{A}\\rangle+\\frac{\\eta}{2}(\\overbrace{\\langle H g,\\theta_{A}\\rangle}^{\\langle g,H\\theta_{A}\\rangle=-|g^{2}|}+\\lambda\\overbrace{\\langle H\\theta\\!,\\!\\phi_{A}\\rangle}^{-\\langle g,\\theta_{A}\\rangle=0})}\\\\ &{\\quad\\quad\\quad\\quad=\\left(\\lambda+\\displaystyle\\frac{\\eta\\lambda^{2}}{2}\\right)|\\theta_{A}|^{2}-\\frac{\\eta}{2}|g_{A}|^{2},}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "and ignored the ${\\cal O}(\\eta\\lambda^{2})$ term. The solution to this differential equation is ",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n|\\theta_{A}(t)|^{2}=e^{-2\\lambda t}|\\theta_{A}(0)|^{2}+\\eta\\int_{0}^{t}e^{-2\\lambda(t-\\tau)}\\left|g_{A}\\right|^{2}d\\tau.\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Rescale dynamics. In the case of parameters with rescale symmetry, the effect of discretization also distorts the original geometry. However, unlike in the sphere, the force originating from discretization can both increase or decrease the pr ity $(|\\theta_{\\mathcal{A}_{1}}(t)|^{\\check{2}}-|\\theta_{\\mathcal{A}_{2}}^{\\smile}(t)|^{2})$ $g_{A_{2}}\\,=\\,0_{.}$ following differential equation, A etric properties of gradient ( ) introduced by rescale symmetry with the equation of learning (equation 27) gives the $(\\left\\langle g,\\theta_{\\dot{A_{1}}}\\right\\rangle-\\left\\langle g,\\theta_{\\dot{A_{2}}}\\right\\rangle=0)$ ⟨A ⟩−⟨ A ⟩) and Hessian ( $(H\\theta_{A_{1}}-H\\theta_{A_{2}}+g_{A_{1}}-$ A −A A −",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n\\left\\langle\\frac{d\\theta}{d t}+\\nabla\\widetilde{\\mathcal{L}},\\theta_{A_{1}}\\right\\rangle-\\left\\langle\\frac{d\\theta}{d t}+\\nabla\\widetilde{\\mathcal{L}},\\theta_{A_{2}}\\right\\rangle=\\left(\\lambda+\\frac{1}{2}\\frac{d}{d t}\\right)\\left(|\\theta_{A_{1}}(t)|^{2}-|\\theta_{A_{2}}(t)|^{2}\\right)-\\frac{\\eta}{2}\\left(|g_{A_{1}}|^{2}-|g_{A_{2}}|^{2}\\right)\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "where we used the simplification, ",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla_{\\theta}\\widetilde{\\mathcal{L}},\\theta_{A_{1}}\\right\\rangle-\\left\\langle\\nabla\\widetilde{\\mathcal{L}},\\theta_{A_{2}}\\right\\rangle=\\left(1+\\frac{\\eta\\lambda}{2}\\right)\\frac{\\left\\langle g,\\theta_{A_{1}}\\right\\rangle-\\left\\langle g,\\theta_{A_{2}}\\right\\rangle=0}{\\left(\\mathcal{L},\\theta_{A_{1}}\\right)^{2}-\\left\\langle g,\\theta_{A_{2}}\\right\\rangle}+\\left(\\lambda+\\frac{\\eta\\lambda^{2}}{2}\\right)(|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\frac{\\eta}{2}\\left(\\!\\begin{array}{c c}{\\left(g,-g_{A_{1}}+g_{A_{2}}\\right)\\!=\\!-|g_{A_{1}}|^{2}\\!+\\!|g_{A_{2}}|^{2}}&{-\\langle g,\\theta_{A_{1}}\\rangle\\!+\\!\\langle g,\\theta_{A_{2}}\\rangle=0}\\\\ {\\left\\langle g,H\\theta_{A_{1}}-H\\theta_{A_{2}}\\right\\rangle\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\times\\underset{\\omega}{\\underbrace{\\prod\\theta_{A_{1}}\\!-\\!|H\\theta_{A_{1}}\\rangle-H\\theta_{A_{2}}^{2}}}\\end{array}\\!\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\left(\\lambda+\\frac{\\eta\\lambda^{\\prime}}{\\mathcal{L}^{2}}\\right)(|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2})-\\frac{\\eta}{2}(|g_{A_{1}}|^{2}-|g_{A_{2}}|^{2}),}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "and ignored the ${\\cal O}(\\eta\\lambda^{2})$ term. This is the same differential equation as in equation (29), just with a different forcing term. Thus, the solution to this differential equation is ",
        "page_idx": 18
    },
    {
        "type": "equation",
        "text": "$$\n|\\theta_{A_{1}}(t)|^{2}-|\\theta_{A_{2}}(t)|^{2}=e^{-2\\lambda t}(|\\theta_{A_{1}}(0)|^{2}-|\\theta_{A_{2}}(0)|^{2})+\\eta\\int_{0}^{t}e^{-2\\lambda(t-\\tau)}\\left(|g_{A_{1}}|^{2}-|g_{A_{2}}|^{2}\\right)d\\tau.\n$$",
        "text_format": "latex",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "FMODELING STOCHASTICITY ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Stochas indices batch size is mu as an average of {$\\{1,\\ldots,N\\}$ Si.i.d. samples from a noisy version of the true gradient smaller than the size of the dataset, }forming the unbiased gradient es s${\\hat{g}}_{B}(\\theta)$ arise when we consider a batch $S\\ll N$ ≪$\\begin{array}{r}{\\hat{g}_{\\mathcal{B}}(\\theta)=\\frac{1}{S}\\sum_{i\\in\\mathcal{B}}\\nabla\\ell(\\theta,x_{i})}\\end{array}$ $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ , then we B$S$ $g(\\theta)$ Pmodel the batch gradient . Using the central limit ly from the . When the theore ${\\hat{g}}_{B}(\\theta)-g(\\theta)$ is a Gaussian random variable with mean $\\mu=0$ and covariance matrix $\\begin{array}{r}{\\Sigma(\\theta)=\\frac{1}{S}G(\\theta)\\dot{G}(\\theta)^{\\intercal}}\\end{array}$ S. Under this assumption, the stochastic gradient update can be written as $\\begin{array}{r}{\\theta^{(n+1)}=\\theta^{(n)}-\\eta g(\\theta^{(n)})+\\frac{\\eta}{\\sqrt{S}}G(\\theta)\\xi}\\end{array}$ , where $\\xi$ is a standard normal random variable. This update is an Euler-Maruyama discretization with step size $\\eta$ of the stochastic differential equation ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\nd\\theta=-g(\\theta)d t+\\sqrt{\\frac{\\eta}{S}}G(\\theta)d W_{t},\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "where $W_{t}$ is a standard Wiener process. Equation (31) has been derived as a model for SGD in many previous works Mandt et al. (2015). In order to simplify the analysis, many of these works have then made additional assumptions on the covariance matrix $\\Sigma(\\theta)={\\dot{G}}(\\theta)G({\\dot{\\theta}})^{\\intercal}$ , such as $\\Sigma(\\theta)=H(\\theta)$ where $H(\\theta)$ is the Hessian matrix Jastrz˛ebski et al. (2017), $\\Sigma(\\theta)=C$ where $C$ is some constant matrix Mandt et al. (2015), and $\\Sigma(\\theta)=I$ where $I$ is the identity matrix Chaudhari & Soatto (2018). However, without any additional assumptions, the differential symmetries intrinsic to neural network architectures add fundamental constraints on $\\Sigma$ .",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "As we showed in section 3, the gradient of the loss, regardless of the batch, is orthogonal to the generator vector field $\\partial_{\\alpha}\\psi|_{\\alpha=I}$ associated with a symmetry. This implies the stochastic noise must also observe the same property, $\\langle-{\\textstyle\\frac{1}{\\sqrt{S}}}G(\\theta)\\xi,\\partial_{\\alpha}\\dot{\\psi_{|\\alpha=I}}\\rangle=0$ . In order for this relationship to hold for arbitrary noise 6 $\\xi$ , then $G(\\theta)^{\\top}\\partial_{\\alpha}\\psi|_{\\alpha=I}=0$ . In other words, the differential symmetry inherent in neural network architectures projects the noise introduced by stochastic gradients onto low rank subspaces. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Scale Symmetry with Stochasticity. In the previous section, we performed our analysis using continuous-time model with deterministic gradients to facilitate calculations, and replaced them with stochastic batch gradients upon discretization when we evaluated the results empirically. Instead, we can also directly model the stochastic noise arising from batch gradient with an Itô process to perform analogous analysis in continuous-time as pointed out by Li et al. (2021) in a recent follow-up work. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "First, we assume that the time evolution of network parameters by the Itô process $\\begin{array}{r}{d\\theta=\\mu d t+\\sqrt{\\frac{\\eta}{S}}G(\\theta)d W_{t}}\\end{array}$ p, where $\\theta(t)$ during SGD training is described ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\n\\mu=-g-\\lambda\\theta-\\frac{\\eta}{2}(H g+\\lambda H\\theta)+O(\\eta^{2})+O(\\eta\\lambda)+O(\\lambda^{2}).\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "$|\\theta_{A}(t)|^{2}$ |A |yields the Itô process, g a set of parameters $\\boldsymbol{\\mathcal{A}}$ respect scale symmetry, then applying Itô’s lemma to the function ",
        "page_idx": 19
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l}&{d|\\theta_{A}(t)|^{2}=\\left\\{2\\theta_{A}^{\\top}\\mu+\\displaystyle\\frac{\\eta}{S}\\,\\mathrm{Tr}[G(\\theta_{A})^{T}G(\\theta_{A})]\\right\\}d t+2\\sqrt{\\displaystyle\\frac{\\eta}{S}}\\theta^{\\top}G(\\theta_{A})d W_{t}}\\\\ &{\\qquad\\qquad=\\left\\{-2\\lambda|\\theta_{A}|^{2}-\\eta|g_{A}|^{2}+\\displaystyle\\frac{\\eta}{S}\\,\\mathrm{Tr}[G(\\theta_{A})^{T}G(\\theta_{A})]\\right\\}d t.}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Notice this is a deterministic ODE equivalent to the previously derived ODE with an additional forcing term accounting for the variance of the noise. We can perform an analogous analysis for the case of translation and rescale symmetry. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "We can also consider the effect of stochasticity without the complexity of stochastic calculus by considering the dynamics in the discrete setting. As explained in appendix I, this is possible for the case without momentum, but becomes much more complicated once we consider momentum as well. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "GDERIVING THE EXACT LEARNING DYNAMICS OF SGD WITH MOMENTUM ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "We consider a continuous model of SGD with momentum incorporating weight decay (equation 14), momentum (equation 15), stochasticity (equation 31), and modified flow (equation 17). As discussed in appendix $\\mathbf{C}$ , we can model the effect of momentum by considering the forward Euler discretization $\\textstyle{\\frac{\\theta_{k+1}-\\theta_{k}}{\\eta(1-\\alpha)}}$ artifacts −$\\scriptstyle{\\frac{\\eta(1-\\alpha)}{2}}{\\frac{d^{2}}{d t^{2}}}\\theta$ and the backward Euler discretization and $\\scriptstyle{\\frac{\\eta(1-\\alpha)}{2}}\\beta{\\frac{d^{2}}{d t^{2}}}\\theta$ respectively, as explained by the modified flow analysis in $-\\beta\\frac{\\theta_{k}\\!-\\!\\theta_{k-1}}{\\eta(1\\!-\\!\\alpha)}$ −. These terms introduce the numerical appendix D. Incorporating these elements with weight decay gives the equation of learning, ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{\\eta(1-\\alpha)}{2}(1+\\beta)\\frac{d^{2}}{d t^{2}}\\theta+(1-\\beta)\\frac{d}{d t}\\theta+\\lambda\\theta=-g(\\theta).\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Incorporating the effect of stochasticity into this equation of learning gives the Langevin equation, ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\frac{\\eta(1-\\alpha)}{2}(1+\\beta)d v=-(1-\\beta)v d t-\\lambda\\theta d t-g(\\theta)d t+\\sqrt{\\frac{\\eta}{S}}G(\\theta)d W_{t},\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "where $W_{t}$ is a standard Weiner process. Compared to the modified loss route (described in the previous section), it is much more natural and simple to account for the effect of stochasticity with modified flow. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "n dynamics. Com metric properties of gradient $(\\langle g,\\mathbb{1}_{\\mathcal{A}}\\rangle\\,=\\,0)$ ), Hessian $(H\\mathbb{1}_{\\mathcal{A}}=0)$ equation of learning (equation 32) gives the differential equation, A ), and stochasticity ( $(G(\\theta)\\mathbb{+}\\mathbb{1}_{A}=0)$ A ) introduced by translation symmetry with the Langevin ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\left(\\frac{\\eta(1-\\alpha)}{2}(1+\\beta)\\frac{d^{2}}{d t^{2}}+(1-\\beta)\\frac{d}{d t}+\\lambda\\right)\\langle\\theta,\\mathbb{1}_{A}\\rangle=0.\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This is the differential equation for a harmonic oscillator with γ$\\begin{array}{r l r}{\\gamma}&{{}=}&{\\frac{1-\\beta}{\\eta\\left(1-\\alpha\\right)\\left(1+\\beta\\right)}}\\end{array}$ and $\\omega\\ =$   \n$\\scriptstyle{\\sqrt{\\frac{2\\lambda}{\\eta(1-\\alpha)(1+\\beta)}}}$ . Assuming the initial condition $\\begin{array}{r}{\\frac{d}{d t}\\langle\\theta(t),\\mathbb{1}_{A}\\rangle|_{t=0}\\,=\\,0}\\end{array}$ , then the general solution −  \n(as derived in appendix $\\mathrm{H}$ ) is ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\theta(t),\\mathbb{1}_{A}\\rangle=\\left\\{\\begin{array}{l l}{e^{-\\gamma t}\\left(\\cosh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)+\\frac{\\gamma}{\\sqrt{\\gamma^{2}-\\omega^{2}}}\\sinh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)\\right)\\langle\\theta(0),\\mathbb{1}_{A}\\rangle}&{\\gamma>\\omega}\\\\ {e^{-\\gamma t}(1+\\gamma t)\\langle\\theta(0),\\mathbb{1}_{A}\\rangle}&{\\gamma=\\omega}\\\\ {e^{-\\gamma t}\\left(\\cos\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)+\\frac{\\gamma}{\\sqrt{\\omega^{2}-\\gamma^{2}}}\\sin\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)\\right)\\langle\\theta(0),\\mathbb{1}_{A}\\rangle}&{\\gamma<\\omega}\\end{array}\\right.\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "cs. Combining th straints on the gradient $(\\langle g,\\theta_{\\mathcal{A}}\\rangle\\,=\\,0)$ ), Hessian $(H\\theta_{\\mathcal{A}}=-g_{\\mathcal{A}})$ equation of learning (equation 32) gives the following differential equation A −A ), and stochasticity ( $(\\bar{G}(\\theta_{\\mathcal{A}})\\mathsf{\\bar{r}}\\theta_{\\mathcal{A}}=0)$ A A ) introduced by scale symmetry with the Langevin 7 ,",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n\\left(\\frac{\\eta(1-\\alpha)(1+\\beta)}{4}\\frac{d^{2}}{d t^{2}}+\\frac{(1-\\beta)}{2}\\frac{d}{d t}+\\lambda\\right)|\\theta_{A}|^{2}=\\frac{\\eta(1-\\alpha)(1+\\beta)}{2}\\left|\\frac{d\\theta_{A}}{d t}\\right|^{2},\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This is the differential equation for a driven harmonic oscillator with $\\begin{array}{r}{\\gamma\\;=\\;\\frac{1-\\beta}{\\eta(1-\\alpha)(1+\\beta)}}\\end{array}$ ,$\\omega=$ $\\scriptstyle{\\sqrt{\\frac{4\\lambda}{\\eta(1-\\alpha)(1+\\beta)}}}$ , and $\\begin{array}{r}{f(t)\\,=\\,2\\left|\\frac{d\\theta_{A}}{d t}\\right|^{2}}\\end{array}$ Assuming the initial condition $\\begin{array}{r}{\\frac{d}{d t}|\\theta_{A}|^{2}\\big|_{t=0}=0}\\end{array}$ , then the general solution (derived in appendix H) is ",
        "page_idx": 20
    },
    {
        "type": "equation",
        "text": "$$\n|\\theta_{A}(t)|^{2}=\\left\\{\\begin{array}{l l}{|\\theta_{A}(t)|_{h}^{2}+\\int_{0}^{t}e^{-\\gamma(t-\\tau)}\\left(\\displaystyle\\frac{\\sinh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}(t-\\tau)\\right)}{\\sqrt{\\gamma^{2}-\\omega^{2}}}\\right)2\\left|\\frac{d\\theta_{A}}{d t}(\\tau)\\right|^{2}d\\tau}&{\\gamma>\\omega}\\\\ {|\\theta_{A}(t)|_{h}^{2}+\\int_{0}^{t}e^{-\\gamma(t-\\tau)}(t-\\tau)2\\left|\\frac{d\\theta_{A}}{d t}(\\tau)\\right|^{2}d\\tau}&{\\gamma=\\omega}\\\\ {|\\theta_{A}(t)|_{h}^{2}+\\int_{0}^{t}e^{-\\gamma(t-\\tau)}\\left(\\displaystyle\\frac{\\sinh\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}(t-\\tau)\\right)}{\\sqrt{\\omega^{2}-\\gamma^{2}}}\\right)2\\left|\\frac{d\\theta_{A}}{d t}(\\tau)\\right|^{2}d\\tau}&{\\gamma<\\omega}\\end{array}\\right.\n$$",
        "text_format": "latex",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "7 The derivation of this ODE uses ⟨$\\begin{array}{r}{\\langle\\frac{d^{2}\\theta_{A}}{d t^{2}},\\theta_{A}\\rangle=\\frac{1}{2}\\frac{d^{2}}{d t^{2}}|\\theta_{A}|^{2}-|\\frac{d\\theta_{A}}{d t}|^{2}}\\end{array}$ |||.",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "where $|\\theta_{A}(t)|_{h}^{2}$ is the solution to the homogeneous harmonic oscillator ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\n|\\theta_{A}(t)|_{h}^{2}=\\left\\{\\begin{array}{l l}{e^{-\\gamma t}\\left(\\cosh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)+\\frac{\\gamma}{\\sqrt{\\gamma^{2}-\\omega^{2}}}\\sinh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)\\right)|\\theta_{A}(0)|^{2}}&{\\gamma>\\omega}\\\\ {e^{-\\gamma t}(1+\\gamma t)|\\theta_{A}(0)|^{2}}&{\\gamma=\\omega}\\\\ {e^{-\\gamma t}\\left(\\cos\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)+\\frac{\\gamma}{\\sqrt{\\omega^{2}-\\gamma^{2}}}\\sin\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)\\right)|\\theta_{A}(0)|^{2}}&{\\gamma<\\omega}\\end{array}\\right.\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "e geometric prope $\\langle g,\\theta_{A_{1}}-\\theta_{A_{2}}\\rangle=0$ , Hessian $(H(\\theta_{A_{1}}-\\theta_{A_{2}})+g_{A_{1}}-g_{A_{2}}\\bar{=}\\,0)$ by rescale symmetry with the Langevin equation of learning (equation 32) gives the differential A −A A −A ), and stochasticity ( $(G(\\theta_{\\mathcal{A}_{1}})^{\\top}\\theta_{\\mathcal{A}_{1}}-\\bar{G}(\\theta_{\\mathcal{A}_{2}})^{\\top}\\theta_{\\mathcal{A}_{2}}\\stackrel{\\cdot\\cdot}{=}0)$ A A −A A ) introduced equation, ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\n\\left(\\frac{\\eta(1-\\alpha)(1+\\beta)}{4}\\frac{d^{2}}{d t^{2}}+\\frac{(1-\\beta)}{2}\\frac{d}{d t}+\\lambda\\right)\\left(|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2}\\right)=\\frac{\\eta(1-\\alpha)(1+\\beta)}{2}\\left(\\left|\\frac{d\\theta_{A_{1}}}{d t}\\right|^{2}-\\left|\\frac{d\\theta_{A_{2}}}{d t}+\\lambda\\right|^{2}\\right).\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "$\\begin{array}{r}{2\\left(|\\frac{d\\theta_{A_{1}}}{d t}|^{2}-|\\frac{d\\theta_{A_{2}}}{d t}|^{2}\\right)}\\end{array}$ This is the same harmonic oscillator given by equation (35) with the different forcing term \u0011. The general solution is given by equation (36) and (37) replacing $|\\theta|^{2}$ $f(t)=$ and $|\\frac{d\\theta}{d t}|^{2}$ by $|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2}$ and |$|\\frac{d\\theta_{A_{1}}}{d t}|^{2}-|\\frac{d\\theta_{A_{2}}}{d t}|^{2}$ respectively. ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "HGENERAL SOLUTIONS FOR ODE S",
        "text_level": 1,
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "H.1EXPONENTIALGROWTH",
        "text_level": 1,
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Here we will solve for the general solution of the homogenous first-order linear differential equation, ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\n\\left({\\frac{d}{d t}}+\\lambda\\right)x(t)=0.\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Assume a solution of the form $x(t)=e^{\\alpha t}$ . Plugging this in gives the auxiliary equation $\\alpha+\\lambda=0$ .Thus, the general solution to the differential equation with initial condition $x(0)$ is ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\lambda t}x(0).\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Now we will solve the inhomogenous differential equation, ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\n\\left({\\frac{d}{d t}}+\\lambda\\right)x(t)=f(t).\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Multiply both sides by equation simplies to $\\begin{array}{r}{\\frac{d}{d t}\\left(e^{\\lambda t x(t)}\\right)\\,=\\,e^{\\lambda t}f(t)}\\end{array}$ $e^{\\lambda t}$ \u0000and factor the left hand side using the product rule such that the differential \u0001. Integrate this equation and using the fundamental theorem of calculus rearrange to get the solution ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\lambda t}x(0)+\\int_{0}^{t}e^{-\\lambda(t-\\tau)}f(\\tau)d\\tau.\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "H.2 HARMONIC OSCILLATOR ",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Here we will solve the general solution for a harmonic oscillator, ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\n\\left(\\frac{d^{2}}{d t^{2}}+2\\gamma\\frac{d}{d t}+\\omega^{2}\\right)x(t)=0.\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Assume a solution of the form $x(t)=e^{\\alpha t}$ . Plugging this in gives the auxiliary equation $\\alpha^{2}+2\\gamma\\alpha+$ $\\omega^{2}=0$ with solutions $\\alpha_{\\pm}=-\\gamma\\pm\\sqrt{\\gamma^{2}-\\omega^{2}}$ −. Thus, the general solution to the oscillator equation with initial conditions $x(0)$ and $\\begin{array}{r}{\\frac{d x}{d t}(0)=0}\\end{array}$ is ",
        "page_idx": 21
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\gamma t}\\left(C_{1}e^{\\sqrt{\\gamma^{2}-\\omega^{2}}t}+C_{2}e^{-\\sqrt{\\gamma^{2}-\\omega^{2}}t}\\right)\n$$",
        "text_format": "latex",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "where $C_{1},C_{2}$ are constants ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nC_{1}=\\frac{\\gamma+\\sqrt{\\gamma^{2}-\\omega^{2}}}{2\\sqrt{\\gamma^{2}-\\omega^{2}}}x(0),\\qquad C_{2}=\\frac{-\\gamma+\\sqrt{\\gamma^{2}-\\omega^{2}}}{2\\sqrt{\\gamma^{2}-\\omega^{2}}}x(0).\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Using hyperbolic functions the solution simplifies as ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\gamma t}\\left(\\cosh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)+\\frac{\\gamma}{\\sqrt{\\gamma^{2}-\\omega^{2}}}\\sinh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)\\right)x(0).\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "The form of this general solution implicitly assumes $\\gamma>\\omega$ , the overdamped setting. When $\\gamma=\\omega$ ,the critically damped setting, then the solution reduces to ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\gamma t}(C_{1}+C_{2}t),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "where $C_{1}=x(0)$ and $C_{2}=\\gamma x(0)$ . When $\\gamma<\\omega$ , the underdamped setting, then the solution reduces to ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=e^{-\\gamma t}\\left(C_{1}\\cos\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)+C_{2}\\sin\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)\\right),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "where $C_{1}=x(0)$ and $\\begin{array}{r}{C_{2}=\\frac{\\gamma}{\\sqrt{\\omega^{2}-\\gamma^{2}}}x(0)}\\end{array}$ .",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "H.3 DRIVEN HARMONIC OSCILLATOR ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Here we will solve the general solution for a driven harmonic oscillator, ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\n\\left({\\frac{d^{2}}{d t^{2}}}+2\\gamma{\\frac{d}{d t}}+\\omega^{2}\\right)x(t)=f(t).\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "First notice that if $x_{h}(t)$ is a solution to the homogenous harmonic oscillator and $x_{d}(t)$ a specific solution to the driven harmonic oscillator, then ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\n\\boldsymbol{x}(t)=\\boldsymbol{x}_{h}(t)+\\boldsymbol{x}_{d}(t),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "is the general solution to the driven harmonic oscillator. We will use the Fourier transform to solve for $\\bar{x_{d}(t)}$ .",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Let $\\hat{x}_{d}(\\tau)=(\\nabla\\tilde{\\mathcal{L}}x_{d})(\\tau)$ and $\\hat{f}(\\tau)=(\\nabla\\tilde{\\mathcal{L}}f)(\\tau)$ be the Fourier transforms of $x_{d}(t)$ and $f(t)$ respectively. Applying the Fourier transform to the driven harmonic oscillator equation and rearranging gives ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\n\\hat{x}_{d}(\\tau)=\\left(-\\tau^{2}+2\\gamma i\\tau+\\omega^{2}\\right)^{-1}\\hat{f}(\\tau),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "which implies by the inverse Fourier transform that $x_{d}(t)$ is the convolution, ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nx_{d}(t)=\\left(G*f\\right)(t),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "where $G(t)$ is Green’s function (the driven solution $x_{d}(t)$ for the dirac delta forcing function $\\delta_{0}$ ), ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nG(t)=\\Theta(t)\\frac{e^{-\\gamma t}}{2\\sqrt{\\gamma^{2}-\\omega^{2}}}\\left(e^{\\sqrt{\\gamma^{2}-\\omega^{2}}t}-e^{-\\sqrt{\\gamma^{2}-\\omega^{2}}t}\\right),\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "which again using hyperbolic functions simplifies as ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nG(t)=\\Theta(t)e^{-\\gamma t}\\frac{\\sinh\\left(\\sqrt{\\gamma^{2}-\\omega^{2}}t\\right)}{\\sqrt{\\gamma^{2}-\\omega^{2}}}.\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "This form of Green’s function is again implicitly assuming $\\gamma>\\omega$ . When $\\gamma=\\omega$ , the function simplifies to ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nG(t)=\\Theta(t)e^{-\\gamma t}t,\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "and when $\\gamma<\\omega$ , the function simplifies to ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nG(t)=\\Theta(t)e^{-\\gamma t}\\frac{\\sin\\left(\\sqrt{\\omega^{2}-\\gamma^{2}}t\\right)}{\\sqrt{\\omega^{2}-\\gamma^{2}}}.\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Noticing that both $G$ and $f$ are only supported on $[0,\\infty)$ , their convolution can be simplified and the general solution for the driven harmonic oscillator is ",
        "page_idx": 22
    },
    {
        "type": "equation",
        "text": "$$\nx(t)=x_{h}(t)+\\int_{0}^{t}G(t-\\tau)f(\\tau)d\\tau.\n$$",
        "text_format": "latex",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "I DERIVING DYNAMICS IN THE DISCRETE SETTING ",
        "text_level": 1,
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "In section 4 we identified certain parameter combinations associated with network symmetries that are conserved under gradient flow. However, as we explained in section 5, these conservation laws are not observed empirically. To remedy this discrepancy we constructed more realistic continuous models for SGD, incorporating weight decay, momentum, stochasticity, and finite learning rates. In section 6 we derived the exact dynamics for the parameter combinations under this more realistic setting, demonstrating near perfect alignment with the empirical dynamics. What would happen if we instead derived the dynamics for the parameter combinations directly in the discrete setting of SGD? Here, we will identify these discrete dynamics and discuss the relationship between the discrete equations and the continuous solutions. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Gradient descent with learning rate $\\eta$ and weight decay constant $\\lambda$ is given by the update equation $\\theta^{(n+1)}=(1-\\eta\\lambda)\\theta^{(n)}-\\eta g(\\theta^{(n)})$ , and initial condition $\\theta^{(0)}$ . Using this update equation the sum of the parameters after $n+1$ steps can be “unrolled” as, ",
        "page_idx": 23
    },
    {
        "type": "equation",
        "text": "$$\n\\langle\\theta^{(n+1)},\\mathbb{1}_{A}\\rangle=(1-\\eta\\lambda)^{n+1}\\langle\\theta^{(0)},\\mathbb{1}_{A}\\rangle+\\eta\\sum_{i=0}^{n}(1-\\eta\\lambda)^{n-i}\\langle g(\\theta^{(i)}),\\mathbb{1}_{A}\\rangle.\n$$",
        "text_format": "latex",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Similarly, the squared Euclidean norm of the parameters after $n+1$ steps can be “unrolled” as, ",
        "page_idx": 23
    },
    {
        "type": "equation",
        "text": "$$\n\\vartheta_{A}^{(n+1)}|^{2}=(1-\\eta\\lambda)^{2(n+1)}|\\theta_{A}^{(0)}|^{2}+\\eta^{2}\\sum_{i=0}^{n}(1-\\eta\\lambda)^{2(n-i)}|g_{A}(\\theta^{(i)})|^{2}-2\\eta\\sum_{i=0}^{n}(1-\\eta\\lambda)^{2(n-i)+1}\\langle g(\\theta^{(i)}),\\theta^{(i)}\\rangle.\n$$",
        "text_format": "latex",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Combining these unrolled equations, with the gradient properties of symmetry discussed in section 3, gives the discrete dynamics for the parameter combinations, ",
        "page_idx": 23
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array}{r l r}{\\mathrm{nslation:}}&{\\left\\langle\\theta^{(n)},\\mathbb{1}_{A}\\right\\rangle=(1-\\eta\\lambda)^{n}\\left\\langle\\theta^{(0)},\\mathbb{1}_{A}\\right\\rangle}&{\\quad{\\mathrm{(35)}}}\\\\ {\\mathrm{Scale:}}&{\\left\\vert\\theta_{A}^{(n)}\\right\\vert^{2}=(1-\\eta\\lambda)^{2n}\\left\\vert\\theta_{A}^{(0)}\\right\\vert^{2}+\\eta^{2}\\sum_{i=0}^{n-1}(1-\\eta\\lambda)^{2(n-1-i)}\\left\\vert g_{A}^{(i)}\\right\\vert^{2}}&{\\quad{\\mathrm{(44)}}}\\\\ {\\mathrm{Rescale:}}&{\\left\\vert\\theta_{A_{1}}^{(n)}\\right\\vert^{2}-\\left\\vert\\theta_{A_{2}}^{(n)}\\right\\vert^{2}=}&{\\quad{\\mathrm{(41)}}}\\\\ &{(1-\\eta\\lambda)^{2n}\\left(\\left\\vert\\theta_{A_{1}}^{(0)}\\right\\vert^{2}-\\left\\vert\\theta_{A_{2}}^{(0)}\\right\\vert^{2}\\right)+\\eta^{2}\\underset{i=0}{\\overset{n-1}{\\sum_{i=0}^{n-1}}}(1-\\eta\\lambda)^{2(n-1-i)}\\left(\\left\\vert g_{A_{1}}^{(i)}\\right\\vert^{2}-\\left\\vert g_{A_{2}}^{(i)}\\right\\vert^{2}\\right)}\\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Notice the striking similarity between the continuous equations (18), (19), (20) presented in section 6 and the discrete equations (39), (40), (41). The exponential function with decay rate $\\lambda$ from the continuous solutions are replaced by a power of the base $(1-\\eta\\lambda)$ in the discrete setting. The integral of exponentially weighted gradient norms from the continuous solutions are replaced by a Riemann sum of power weighted gradient norms in the discrete setting. This is further confirmation that the continuous solutions we derived, and the modified gradient flow equation of learning used, well approximate the actual empirics. While the equations derived in the discrete setting remove any uncertainty about the exactness of the theoretical predictions, they provide limited qualitative understanding for the empirical learning dynamics. This is especially true if we consider the learning dynamics with momentum. In this setting, the process of “unrolling” is much more complicated and the harmonic nature of the empirics, easily derived in the continuous setting, is hidden in the discrete algebra. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "JEXPERIMENTAL DETAILS ",
        "text_level": 1,
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "An open source version of our code, used to generate all the figures in this paper, is available at github.com/danielkunin/neural-mechanics. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Dataset. While we ran some initial experiments on Cifar-100, the dataset used in all the empirical figures in this documents was Tiny Imagenet. It is used for image categorization an consists of 100,000 training images at a resolution of $64\\times64$ spanning 200 classes. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Model. We use standard VGG-16 models for all out experiments with the following modifications: ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "•The last three fully connected layers at the end have been adjusted for an input at the Tiny ImageNet resolution $(64\\times64)$ and thus consist of 2048 ,2048 , and 200 layers respectively. •In addition to the standard arrangement of conv layers for the VGG-16, we consider a variant where we add a batch normalization layer between every convolutional layer and its activation function. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Training hyperparameters. Certain hyperparameters were varied during training. Below we outline the combinations explored. All models were initialized using Kaiming Normal, and no learning rate drops or warmup were used. ",
        "page_idx": 24
    },
    {
        "type": "table",
        "img_path": "images/345fad475b69c833aaa12ffcb24c09500e758159a32f1f1e676920383757f5a9.jpg",
        "table_caption": [
            "Table 4: Training hyperparameters. "
        ],
        "table_footnote": [],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Counting number of symmetries. Here we explain how to count the number of symmetries a VGG-16 model contains. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "•Scale symmetries appear once per channel at every layer preceding a batch normalization layer. For our VGG-16 model with batch norm: $2{\\cdot}64+2{\\cdot}128{+}3{\\cdot}256{+}3{\\cdot}512{+}3{\\cdot}512{+}3=$ 4 ,227 .  \n•Rescale symmetries appear once per channel where there are afine transforms between layers as well as once per input neuron to a fully connected layer. Note that the sizes of the fully connected layers depend on input image size and the number of classes. For our s is: $(2\\cdot64+2\\cdot128+$   \n$3\\cdot256+3\\cdot512+3\\cdot512+3)+(2048+1024+1024)=8,323$ ···.  \n•Translation symmetries appear once per input value to the softmax function, which for the case of classification is always equal to the number of classes, plus the bias term. For our case this is $200+1=201$ .",
        "page_idx": 24
    },
    {
        "type": "table",
        "img_path": "images/340f7a7c9289ed65cd3fb6c1f735dae28695e5815543fbd4cbf7954cb1ad67bb.jpg",
        "table_caption": [
            "Table 5: Counting the number of symmetries. "
        ],
        "table_footnote": [],
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Computing the theoretical predictions. Some of the expressions shown in equations (18), (19), and (20) in section 6 are not trivial to compute as they involve an integral of an exponentially weighted gradient term. To tackle this problem, we wrote custom optimizers in PyTorch with additional buffers to approximate the integral via a Riemann sum. At every update step, the argument in the integral term was computed from the batch gradients, scaled appropriately, and was accumulated in the buffer. Note that the above sum needs to be scaled by the learning rate, which is the coarseness of the grid of this Riemann sum. Checkpoints of the model and optimizer states were stored at pre-defined frequencies during training. Our visualizations involve computing the right hand side of equations (18), (19), and (20) from the model states and the left hand side of the same equations from the integral buffers stored in the optimizer states as explained above. These two quantities are referred to as “empirical” and “theoretical” in the figures and are depicted with solid color lines and dotted lines, respectively. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "In this work we made exact predictions for the dynamics of combinations of parameters during training with SGD. Importantly, these predictions were made at the neuron level, but could be aggregated for each layer. Here we plot our predictions at both layer and neuron levels for VGG-16 models trained on Tiny ImageNet. ",
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/cd3ee4481ec4fc2af81375ec67e523a2eb855515f0a468be9aec7292bd7a03ce.jpg",
        "img_caption": [
            "Figure 8: The planar dynamics of VGG-16 on Tiny ImageNet. We plot the column sum of the final linear layer of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with th learning rate $\\eta\\:=\\:0.1$ , weight decay $\\lambda\\,\\in\\,\\{0,10^{-4},5\\times10^{-4},10^{-3}\\}$ , and batch size $S=256$ . Colored lines are empirical column sums of the last layer through training and black dashed lines are the theoretical predictions of equation (18). "
        ],
        "img_footnote": [],
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/bd370e060929565fbea71effaf60c1fa4f69535a0678d252326a5a5527da3f11.jpg",
        "img_caption": [
            "Figure 9: The spherical dynamics of VGG-16 BN on Tiny ImageNet. We plot the squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained on Tiny ImageNe D with learning rate $\\eta=0.1$ , weight decay $\\lambda\\in\\{0,10^{-4},5\\times10^{-4},10^{-3}\\}$ ,and batch size $S=256$ . Colored lines represent empirical layer-wise squared norms through training and the white dashed lines the theoretical predictions given by equation (19). "
        ],
        "img_footnote": [],
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/7f820aff3d61311304dfdbbccfd810e460dd8dbf5f319c3a4958acd7df626d9b.jpg",
        "img_caption": [
            "Figure 10: The hyperbolic dynamics of VGG-16 on Tiny ImageNet. We plot the difference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with SGD with learning rate $\\eta=0.1$ , weight decay $\\lambda\\in\\{0,10^{-4},5\\times10^{-4},10^{-3}\\}$ , and batch size $S=256$ . Colored lines represent empirical differences in consecutive layer-wise squared norms through training and the white dashed lines the theoretical predictions given by equation (20). "
        ],
        "img_footnote": [],
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/56b7b6aca494289519247d9d86d8abf7e23048a225b222387f69d3f8e5fde8a3.jpg",
        "img_caption": [
            "Figure 11: The planar dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the column sum of the final linear layer of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with Moment te $\\eta=0.1$ , weig $\\lambda\\in\\{0,10^{-4},5\\times10^{-4},10^{-3}\\}$ ,momentum coefficient $\\beta\\,\\in\\,\\{0,0.9,0.{\\bar{9}}9\\}$ ∈{ }, and batch size S$S\\,=\\,128$ . Colored lines are empirical column sums of the last layer through training and black dashed lines are the theoretical predictions of equation (34). "
        ],
        "img_footnote": [],
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/1b8a07ea2e06702d8f4bf301110ed7ee17edd815df4cab59e39ece2330ffba32.jpg",
        "img_caption": [
            "Figure 12: The spherical dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained on Tiny ImageNet with Momentum with learning rate $\\eta\\:=\\:0.1$ , weight decay $\\lambda\\in\\{0,10^{-4},5\\times10^{-4},10^{-3}\\}$ , momentum coefficient $\\beta\\in\\{0,0.9,\\^{\\mathrm{{0.99}}}\\}$ , and batch size $S=128$ .Colored lines represent empirical layer-wise squared norms through training and the white dashed lines the theoretical predictions given by equation (36) and (37) "
        ],
        "img_footnote": [],
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/c6776c85cc8b307264c28ccb21fedce872f9b1c1ee63296ac48c4e33a5344cd7.jpg",
        "img_caption": [
            "Figure 13: The hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the difference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with Momentum with learning rate $\\eta=0.1$ , w ay $\\lambda\\in\\{0,10^{-4},5\\!\\times\\!10^{-4},10^{-3}\\}$ , momentum coefficient $\\beta\\in\\{0,0.9,0.99\\}$ , and batch size $S=128$ . Colored lines represent empirical differences in consecutive layer-wise squared norms through training and the black dashed lines the theoretical predictions given by equation (36) and (37) replacing $|\\theta|^{2}$ and $|\\frac{d\\theta}{d t}|^{2}$ by $|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2}$ and |$|\\frac{d\\theta_{A_{1}}}{d t}|^{2}-|\\frac{d\\theta_{A_{2}}}{d t}|^{2}$ respectively. "
        ],
        "img_footnote": [],
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/9611f1a31744ef185d17a5efc657d30aa03cd6ac5e43e830fb72b6d26fd21f87.jpg",
        "img_caption": [
            "Figure 14: The per-neuron spherical dynamics of SGD on VGG-16 BN on Tiny ImageNet. We plot the per-neuron squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained on Tiny ImageNet with SGD with learning rate $\\eta=0.1$ , weight decay $\\lambda=0$ ,and batch size $S=256$ . Colored lines represent empirical layer-wise squared norms through training and the black dashed lines the theoretical predictions given by equation (19). "
        ],
        "img_footnote": [],
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/3a5e6ab2ba2d6f437bd115fb1890a279a4b59f84b357a96179fb6b697fefcf6e.jpg",
        "img_caption": [
            "Figure 15: The per-neuron hyperbolic dynamics of SGD on VGG-16 on Tiny ImageNet. We plot the per-neuron difference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with SGD with learning rate $\\eta=0.1$ , weight decay $\\lambda=0$ , and batch size $S=256$ . Colored lines represent empirical differences in consecutive layer-wise squared norms through training and the black dashed lines the theoretical predictions given by equation (20). "
        ],
        "img_footnote": [],
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/799b95e6aa7b43e575c17b1ec989deb51c72d12323bca9b24e2da7deead4014a.jpg",
        "img_caption": [
            "Figure 16: The per-neuron spherical dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the per-neuron squared Euclidean norms for convolutional layers of a VGG-16 model (with batch normalization) trained on Tiny ImageNet with Momentum with learning rate $\\eta=0.1$ ,weight decay $\\lambda=0$ , momentum coefficient $\\vec{\\beta^{{\\,}}}\\in\\{0.9,0.99\\}$ , and batch size $S\\,=\\,128$ . Colored lines represent empirical layer-wise squared norms through training and the black dashed lines the theoretical predictions given by equation (36) and (37). "
        ],
        "img_footnote": [],
        "page_idx": 29
    },
    {
        "type": "image",
        "img_path": "images/2a35d3cfbf02820ea917dfe0d3223136046e05e0722a5b184481679e8200adb6.jpg",
        "img_caption": [
            "Figure 17: The per-neuron hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet. We plot the per-neuron difference between the squared Euclidean norms for consecutive convolutional layers of a VGG-16 model (without batch normalization) trained on Tiny ImageNet with Momentum with rate $\\eta=0.1$ , weight decay $\\lambda=0$ , momentum coefficient $\\beta\\in\\{0.9,0.99\\}$ , and batch size $S=128$ . Colored lines represent empirical differences in consecutive layer-wise squared norms through training and the black dashed lines the theoretical predictions given by equation (36) and (37) replacing $|\\theta|^{2}$ and $|\\frac{d\\theta}{d t}|^{2}$ by $|\\theta_{A_{1}}|^{2}-|\\theta_{A_{2}}|^{2}$ and |$|\\frac{d\\theta_{\\mathcal{A}_{1}}}{d t}|^{\\overline{{2}}}-|\\frac{d\\theta_{\\mathcal{A}_{2}}}{d t}|^{2}$ respectively. "
        ],
        "img_footnote": [],
        "page_idx": 29
    }
]