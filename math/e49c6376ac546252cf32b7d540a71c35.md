## Mathematical Statements:

**Definition 1:**  
We define $\rho_{k}$ as the distribution of $X_{k}$ generated at the $k$-th step of SVRG-LD, and similarly $\phi_{k}$ for SARAH-LD.

**Assumption 1:**  
$f_{i}\;(i=1,\ldots,n)$ are twice differentiable, and for all $x,y\in\mathbb{R}^{d}$, $\|\nabla^{2}f_{i}(x)\|\leq L$.  

**Assumption 2 (Log-Sobolev Inequality):**  
Distribution $\nu$ satisfies the Log-Sobolev inequality (LSI) with a constant $\alpha$. That is, for all probability density functions $\rho$ absolutely continuous with respect to $\nu$, the following holds:
$$
H_{\nu}(\rho)\leq\frac{1}{2\alpha}J_{\nu}(\rho),
$$
where $H_{\nu}(\rho) := \mathbb{E}_{\rho}\left[\log\left(\frac{\rho}{\nu}\right)\right]$ is the KL-divergence of $\rho$ with respect to $\nu$, and
$$
J_{\nu}(\rho) := \mathbb{E}_{\rho}\left[\left\Vert \nabla \log\left(\frac{\rho}{\nu}\right)\right\Vert^{2}\right]
$$
is the relative Fisher information of $\rho$ with respect to $\nu$.

**Theorem 1 (Convergence of SVRG-LD):**  
Under Assumptions 1 and 2, $0 < \eta < \frac{\alpha}{16\sqrt{6}L^{2}m\gamma}$, $\gamma \geq 1$ and $B \geq m$, for all $k=1,2,\ldots$, the following holds in the update of SVRG-LD where $\Xi=\frac{(n-B)}{B(n-1)}$:
$$
H_{\nu}(\rho_{k})\leq\mathrm{e}^{-\frac{\alpha\eta}{\gamma}k}H_{\nu}(\rho_{0})+\frac{224\eta\gamma d L^{2}}{3\alpha}\left(2+3\Xi+2m\Xi\right).
$$

**Corollary 1.1:**  
Under the same assumptions as Theorem 1, for all $\epsilon\geq0$, if we choose step size $\eta$ such that $\eta\leq\frac{3\alpha\epsilon}{448\gamma d L^{2}}$, then a precision $H_{\nu}(\rho_{k})\leq\epsilon$ is reached after $k\geq\frac{\gamma}{\alpha\eta}\log\frac{2H_{\nu}\left(\rho_{0}\right)}{\epsilon}$ steps. Especially, if we take $B = m = \sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{6}L^{2}\sqrt{n}\gamma}\wedge\frac{3\alpha\epsilon}{448d L^{2}\gamma}$, then the gradient complexity becomes:
$$
\tilde{O}\left(\left(n+\frac{d n^{\frac{1}{2}}}{\epsilon}\right)\cdot\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right).
$$

**Theorem 2 (Convergence of SARAH-LD):**  
Under Assumptions 1 and 2, $0<\eta<\frac{\alpha}{16\sqrt{2}L^{2}m\gamma}$ and $\gamma\geq1$, for all $k=1,2,\dots$, the following holds in the update of SARAH-LD where $\Xi=\frac{(n-B)}{B(n-1)}$:
$$
H_{\nu}(\phi_{k})\leq\mathrm{e}^{-\frac{\alpha\eta}{\gamma}k}H_{\nu}(\phi_{0})+\frac{32\eta\gamma d L^{2}}{3\alpha}\left(2+\Xi+2m\Xi\right).
$$

**Corollary 2.1:**  
Under the same assumptions as Theorem 2, for all $\epsilon_0$, if we choose step size $\eta$ such that $\eta\,\le\,\frac{3\alpha\epsilon}{64\gamma d L^{2}}\left(2+\Xi+2m\Xi\right)^{-1}$, then a precision $H_{\nu}(\phi_{k})\,\leq\,\epsilon$ is reached after $k\ge\frac{\gamma}{\alpha\eta}\log\frac{2H_{\nu}\left(\phi_{0}\right)}{\epsilon}$ steps. Especially, if we take $B = m = \sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{2}L^{2}\sqrt{n}\gamma}\wedge\frac{3\alpha\epsilon}{320d L^{2}\gamma}$, then the gradient complexity becomes:
$$
\tilde{O}\left(\left(n+\frac{d n^{\frac{1}{2}}}{\epsilon}\right)\cdot\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right).
$$

**Theorem 3 (Non-Convex Optimization Convergence):**  
Using SVRG-LD or SARAH-LD, under Assumptions 1 to 3, $0<\eta<\frac{\alpha}{16\sqrt{6}L^{2}m\gamma}$, $\gamma\geq\frac{4d}{\epsilon}\log\left(\frac{\mathrm{e}L}{M}\right) ee \frac{8d b}{\epsilon^{2}} ee 1 ee \frac{2}{M}$, and $B \geq m$, if we take $B=m=\sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{6}L^{2}\sqrt{n}\gamma}\wedge\frac{3}{1792}\frac{\alpha^{2}\epsilon}{L^{2}d\gamma}$, the gradient complexity to reach a precision of:
$$
\mathbb{E}_{X_{k}}[F(X_{k})]-F(X^{*})\leq\epsilon
$$
is:
$$
\tilde{O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{\alpha}\right)\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right),
$$
where $\alpha$ is a function of $\gamma$, and $X^{*}$ is the global minimum of $F$.

**Corollary 3.1:**  
Under the same assumptions as Theorem 3, taking $\gamma=\begin{array}{r}{i(\epsilon):=\frac{4d}{\epsilon}\log\left(\frac{\mathrm{e}L}{M}\right)\lor\frac{8d b}{\epsilon^{2}}\lor1\lor\frac{2}{M}}\end{array}$, we obtain a gradient complexity of:
$$
\tilde{O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{C_{1}i(\epsilon)}\mathrm{e}^{C_{2}i(\epsilon)}\right)L^{2}\mathrm{e}^{2C_{2}i(\epsilon)}\right)
$$
since $\alpha=\gamma C_{1}\mathrm{e}^{-C_{2}\gamma}$.

**Corollary 3.2:**  
Under the same assumptions as Theorem 3 and Assumptions 4 to 6, taking $\gamma=j(\epsilon):=\frac{4\breve{d}}{\epsilon}\log\left(\frac{\mathrm{e}L}{M}\right)\lor\frac{8d b}{\epsilon^{2}}\lor1\lor\frac{2}{M}\lor C_{\gamma}$, we obtain a gradient complexity of:
$$
\tilde{\cal O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{C_{3}}j(\epsilon)\right)C_{3}^{2}j(\epsilon)^{4}L^{2}\right),
$$
since $\alpha=C_{3}/\gamma$.