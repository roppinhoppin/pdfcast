---
audio_file_path: /audio/89fddc9a42157a08bc49de27242cc674.wav
transcript_path: /transcript/89fddc9a42157a08bc49de27242cc674.txt
pdffile_path: /pdf/89fddc9a42157a08bc49de27242cc674.pdf
date: 2024-11-23 23:20:12 +0900
images: ['images/89fddc9a42157a08bc49de27242cc674/2a5208e531d49876621e4f6e236b39c5d2bae0cef07e947a78a4e37816a0f3b8.jpg', 'images/89fddc9a42157a08bc49de27242cc674/e1216c010caa465260eeacd862afba52ba80f16fc0123d974d32a762d60b30d7.jpg', 'images/89fddc9a42157a08bc49de27242cc674/dc5f51a4a760310f61071702373a22094dd2f48473d7bc304ebaf3e2aa5449a4.jpg', 'images/89fddc9a42157a08bc49de27242cc674/6b2fa112e1760279c87a9879fea44b17da9b49fdeea52c980d2bd12685008ef1.jpg', 'images/89fddc9a42157a08bc49de27242cc674/3474ef74d85729b948a1ca39970b1087c0c129b20046ccb79f60370562a78e3d.jpg', 'images/89fddc9a42157a08bc49de27242cc674/9e25c013eb9bd51e866f406fec283e6bbe6b67641ec4acecd9bbdb0ae04f1dfc.jpg', 'images/89fddc9a42157a08bc49de27242cc674/44958b7b46d1e3200f30c93c59dd15cf895fdcb708604ebf84f6e555ef37fc09.jpg']
description: AI-generated podcast from the PDF file Diochnos et al. - 2018 - Adversarial Risk and Robustness General Definitio_JP / 89fddc9a42157a08bc49de27242cc674
layout: article
title: Diochnos et al. - 2018 - Adversarial Risk and Robustness General Definitio_JP
---

## Transcription
こんにちは！皆さん、「ラーニング・マシーンズ」へようこそ！僕はマシューです。今日のゲストは、バージニア大学のDimitrios I. Diochnosさんです。Dimitriosさん、ようこそ！

ありがとうございます、マシュー。今日は皆さんにお会いできて嬉しいです。

今日は、敵対的機械学習、特にロバストな分類器構築の課題について、Dimitriosさんと深く掘り下げていきたいと思います。2018年のNIPSで発表された「Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution」という論文がベースです。

ええ、その論文について話しましょう。簡単に言うと、わずかな変更で分類器を騙せる入力データ、いわゆる敵対的例の存在が問題になっていますよね。

まさに！例えば、パンダの写真を分類器が正しく認識しているとします。でも、ほんの数ピクセルを変更するだけで、分類器はそれをテナガザルと誤認識してしまう。そんな不安定さが敵対的例の問題です。この論文では、特にデータが均一に分布している場合のロバスト性の限界を調べています。

はい。論文では、敵対的リスクとロバスト性の分類体系を提案しています。分類器と敵対者の間のゲームのルールを定義するようなものです。重要な定義の一つに「error-region robustness」があります。これは、分類器の誤分類領域にデータを押し込むために、平均してどのくらい入力データを変更する必要があるかを測る指標です。

なるほど。他の定義、「prediction-change robustness」や「corrupted-instance robustness」との違いも重要ですね。例えば、「prediction-change robustness」は予測が変わるかどうかだけを見て、それが実際に間違っているかどうかは考慮しませんよね。

そうです。そして、「corrupted-instance robustness」は、元のデータのラベルを回復することに焦点を当てています。論文では、「error-region robustness」に焦点を当てるべきだと主張しています。

なぜ「error-region robustness」が重要なんですか？

例えば、スパムメールを特定のキーワードの存在に基づいて識別する分類器を学習させると考えてみましょう。「error-region robustness」は、検出を回避するスパムを作成するという敵対者の目標を捉えています。一方、他の定義では、スパムの状況に実際には影響を与えない変更によって騙されてしまう可能性があります。

分かりやすいですね！論文の主要な結果は、入力が均一に分布している場合、どんな分類器にもロバスト性に関する根本的な限界があることを示しています。

はい、等周不等式という強力な幾何学的ツールを使って、初期の誤差率がたとえ1％であっても、敵対者は驚くほど少ないビット数（入力次元の平方根のオーダー）を変更するだけで、誤差率を50％または99％にまで引き上げることができることを示しています。

それは衝撃的な結果ですね！完璧なロバスト性は達成できないことが多いという示唆になりますね。

まさにそうです。また、リスクとロバスト性の間に密接な関係があることも示しています。誤差の増加は、分類器を常に間違えさせるために必要な平均的な努力と密接に関連しているのです。

証明方法は？

分類器の誤差領域を高次元空間内の形状と見なすと、等周不等式は、最も「丸い」形状が最も小さい境界を持つことを本質的に述べています。誤差領域をこれらの丸い形状に関連付けることで、敵対者によって誤差領域に到達する容易さに関する限界を導き出しています。

この研究の意義は大きいですね。私たちが達成できるロバスト性の根本的な限界を浮き彫りにしています。均一分布を超えて、摂動を測定するための代替指標を探求することで、よりロバストな分類器の開発につながるでしょう。

はい。実世界のデータセットの「等周特性」を研究し、敵対的攻撃に対する固有の脆弱性を理解することも興味深い方向性です。

今日は本当に興味深いお話ありがとうございました、Dimitriosさん。今日の「ラーニング・マシーンズ」は以上です。また次回お会いしましょう！





