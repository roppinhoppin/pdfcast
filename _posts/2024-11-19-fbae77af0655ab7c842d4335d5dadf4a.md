---
audio_file_path: /audio/fbae77af0655ab7c842d4335d5dadf4a.wav
transcript_path: /transcript/fbae77af0655ab7c842d4335d5dadf4a.txt
pdffile_path: /pdf/fbae77af0655ab7c842d4335d5dadf4a.pdf
date: 2024-11-23 23:27:32 +0900
images: ['images/fbae77af0655ab7c842d4335d5dadf4a/5c6884ce3454c5d8cb70f775d3d5941ca4732ef3ca591183fa93de4c8242c583.jpg', 'images/fbae77af0655ab7c842d4335d5dadf4a/72ade12db38654811da0d7dec5df9eee39418847dc5b5916d90c121cd05f7f88.jpg']
description: AI-generated podcast from the PDF file Kinoshita and Suzuki, 2022 - Improved convergence rate of stochastic gradient Langevin dynamics with variance reduction and its application to optimization_JP / fbae77af0655ab7c842d4335d5dadf4a
layout: article
title: Kinoshita and Suzuki, 2022 - Improved convergence rate of stochastic gradient Langevin dynamics with variance reduction and its application to optimization_JP
---

## Transcription
こんにちは！皆さん、「アルゴリズム冒険記」へようこそ！今日は、機械学習と統計学における重要な課題、複雑な確率分布からの効率的なサンプリングについて深掘りします。

今日は、東京大学の貴志田裕さんと鈴木達志さんの論文、「確率的勾配Langevinダイナミクス の収束速度の改善とその最適化への応用」を取り上げます。貴志田さん、今日はありがとうございます！

ありがとうございます、マシューさん。今日はこの論文についてお話できることを嬉しく思います。

この論文、正直なところ、タイトルだけでちょっと身構えちゃいました（笑）。でも、重要なのは、大量のデータで正確な勾配を計算するのが難しい現状への解決策を提示している点ですよね？

まさにその通りです。大規模データ時代において、正確な勾配の計算は非常にコストがかかります。そこで、この論文では、確率的勾配Langevinダイナミクス（SGLD）の収束速度を向上させることで、効率的なサンプリングと最適化を実現しています。

なるほど！Langevinダイナミクスって、どういうものなんでしょう？僕にはちょっと難しそうに聞こえます…。

簡単に言うと、複雑な地形をボールが転がる様子を想像してみてください。ボールが止まる場所が、確率分布が高いところになります。Langevinダイナミクスは、このボールの動きをシミュレートすることで、確率分布からサンプルを抽出するアルゴリズムなんです。

へぇー！面白いですね！でも、地形が霧で覆われていて、一部分しか見えない状況だとどうなるんですか？

それはまさに、確率的勾配を使う状況を表していますね。データの一部しか使えないので、ノイズが入ってしまい、ボールの動きが遅くなってしまうんです。この論文では、分散削減の手法を用いることで、霧を晴らすようにノイズを減らし、収束を高速化しているんです。

つまり、霧を晴らすことで、より効率的に「確率分布の高いところ」を見つけられるようになるってことですね！

そうです！この論文の重要な成果は、分散削減版SGLDであるSVRG-LDとSARAH-LDが、従来の研究よりも弱い仮定の下で指数関数的に高速に目標分布に収束することを証明した点です。

具体的に、どういう手法を使って高速化しているんでしょうか？

彼らは「対数ソボレフ不等式」という強力なツールを使っています。これは、霧の地形の中で情報がどれくらい速く広がるかを測る尺度のようなものです。

なるほど！そして、この論文では、確率的勾配を使うことによるバイアス（偏り）の問題も巧みに解決しているんですよね？

はい、その通りです。確率的勾配を使うとバイアスが蓄積されてしまうのですが、この論文ではそのバイアスを効果的に制御する方法を示し、勾配計算の複雑さを大幅に削減することに成功しています。

すごいですね！この高速化されたアルゴリズムは、具体的にどんな応用があるんですか？

機械学習におけるベイズ推論や、複雑な関数のグローバルな最小値を求める非凸最適化などに幅広く応用できます。例えば、ベイズ推論では、モデルパラメータの事後分布を推定する際に、このアルゴリズムを使うことで計算時間を大幅に短縮できます。

つまり、より効率的に、より正確な結果を得られるようになるわけですね！この研究は、今後の研究にも大きな影響を与えそうですね。

そうですね。例えば、他のサンプリングアルゴリズムへの応用や、さらに弱い仮定の下での高速収束の保証など、多くの可能性が開かれています。

本当に興味深い研究ですね！今日はありがとうございました！

こちらこそ、ありがとうございました！



## Mathematical Statements:

**Definition 1:**  
We define $\rho_{k}$ as the distribution of $X_{k}$ generated at the kth step of SVRG-LD, and similarly $\phi_{k}$ for SARAH-LD.

**Assumption 1:**  
$f_{i}\;(i=1,\ldots,n)$ are twice differentiable, and $\forall x,y\in\mathbb{R}^{d}$, $\|\nabla^{2}f_{i}(x)\|\leq L$.

**Assumption 2 (Log-Sobolev Inequality):**  
Distribution $\nu$ satisfies the Log-Sobolev inequality $(L S I)$ with a constant $\alpha$. That is, for all probability density functions $\rho$ absolutely continuous with respect to $\nu$, the following holds:
$$ H_{\nu}(\rho)\leq{\frac{1}{2\alpha}}J_{\nu}(\rho), $$
where $$H_{\nu}(\rho) := \mathbb{E}_{\rho}\left[\log\left(\frac{\rho}{\nu}\right)\right]$$ 
of $\rho$ with respect to $\nu$, and $J_{\nu}(\rho) := \mathbb{E}_{\rho}\left[\left\Vert \nabla \log\left(\frac{\rho}{\nu}\right)\right\Vert^{2}\right]$ is the relative Fisher information of $\rho$ with respect to $\nu$.

**Theorem 1 (Convergence of SVRG-LD):**  
Under Assumptions 1 and 2, $0 < \eta < \frac{\alpha}{16\sqrt{6}L^{2}m\gamma}$, $\gamma \geq 1$ and $B \geq m$, for all $k=1,2,\ldots$, the following holds in the update of SVRG-LD where $\Xi=\frac{(n-B)}{B(n-1)}$:
$$ H_{\nu}(\rho_{k})\leq\mathrm{e}^{-\frac{\alpha\eta}{\gamma}k}H_{\nu}(\rho_{0})+\frac{224\eta\gamma d L^{2}}{3\alpha}\left(2+3\Xi+2m\Xi\right). $$

**Corollary 1.1:**  
Under the same assumptions as Theorem 1, for all $\epsilon\geq0$, if we choose step size $\eta$ such that $\eta\leq\frac{3\alpha\epsilon}{448\gamma d L^{2}}$, then a precision $H_{\nu}(\rho_{k})\leq\epsilon$ is reached after $k\geq\frac{\gamma}{\alpha\eta}\log\frac{2H_{\nu}\left(\rho_{0}\right)}{\epsilon}$ steps. Especially, if we take $B = m = \sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{6}L^{2}\sqrt{n}\gamma}\wedge\frac{3\alpha\epsilon}{448d L^{2}\gamma}$, then the gradient complexity becomes:
$$ \tilde{O}\left(\left(n+\frac{d n^{\frac{1}{2}}}{\epsilon}\right)\cdot\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right). $$

**Theorem 2 (Convergence of SARAH-LD):**  
Under Assumptions 1 and 2, $0<\eta<\frac{\alpha}{16\sqrt{2}L^{2}m\gamma}$ and $\gamma\geq1$, for all $k=1,2,\dots$, the following holds in the update of SARAH-LD where $\Xi=\frac{(n-B)}{B(n-1)}$:
$$ H_{\nu}(\phi_{k})\leq\mathrm{e}^{-\frac{\alpha\eta}{\gamma}k}H_{\nu}(\phi_{0})+\frac{32\eta\gamma d L^{2}}{3\alpha}\left(2+\Xi+2m\Xi\right). $$

**Corollary 2.1:**  
Under the same assumptions as Theorem 2, for all $\epsilon\_0$, if we choose step size $\eta$ such that $\eta\,\le\,\frac{3\alpha\epsilon}{64\gamma d L^{2}}\left(2+\Xi+2m\Xi\right)^{-1}$, then a precision $H_{\nu}(\phi_{k})\,\leq\,\epsilon$ is reached after $k\ge\frac{\gamma}{\alpha\eta}\log\frac{2H_{\nu}\left(\phi_{0}\right)}{\epsilon}$ steps. Especially, if we take $B = m = \sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{2}L^{2}\sqrt{n}\gamma}\wedge\frac{3\alpha\epsilon}{320d L^{2}\gamma}$, then the gradient complexity becomes:
$$ \tilde{O}\left(\left(n+\frac{d n^{\frac{1}{2}}}{\epsilon}\right)\cdot\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right). $$

**Theorem 3 (Non-Convex Optimization Convergence):**  
Using SVRG-LD or SARAH-LD, under Assumptions 1 to 3, $0<\eta<\frac{\alpha}{16\sqrt{6}L^{2}m\gamma}$, $\gamma\geq\frac{4d}{\epsilon}\log\left(\frac{\mathrm{e}L}{M}\right) \lor \frac{8d b}{\epsilon^{2}} \lor 1 \lor \frac{2}{M}$, and $B \geq m$, if we take $B=m=\sqrt{n}$ and the largest permissible step size $\eta=\frac{\alpha}{16\sqrt{6}L^{2}\sqrt{n}\gamma}\wedge\frac{3}{1792}\frac{\alpha^{2}\epsilon}{L^{2}d\gamma}$, the gradient complexity to reach a precision of:
$$ \mathbb{E}_{X_{k}}[F(X_{k})]-F(X^{\ast})\leq\epsilon $$
is:
$$ \tilde{O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{\alpha}\right)\frac{\gamma^{2}L^{2}}{\alpha^{2}}\right), $$
where $\alpha$ is a function of $\gamma$, and $X^{*}$ is the global minimum of $F$.

**Corollary 3.1:**  
Under the same assumptions as Theorem 3, taking $\gamma=i(\epsilon):=\frac{4d}{\epsilon}\log\left(\frac{\mathrm{e}L}{M} \right)\lor\frac{8d b}{\epsilon^{2}}\lor1\lor\frac{2}{M}$, we obtain a gradient complexity of:
$$ \tilde{O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{C_{1}i(\epsilon)}\mathrm{e}^{C_{2}i(\epsilon)}\right)L^{2}\mathrm{e}^{2C_{2}i(\epsilon)}\right) $$
since $\alpha=\gamma C_{1}\mathrm{e}^{-C_{2}\gamma}$.

**Corollary 3.2:**  
Under the same assumptions as Theorem 3 and Assumptions 4 to 6, taking $\gamma=j(\epsilon):=\frac{4\breve{d}}{\epsilon}\log\left(\frac{\mathrm{e}L}{M}\right)\lor\frac{8d b}{\epsilon^{2}}\lor1\lor\frac{2}{M}\lor C_{\gamma}$, we obtain a gradient complexity of:
$$ \tilde{\cal O}\left(\left(n+\frac{n^{\frac{1}{2}}}{\epsilon}\cdot\frac{d L}{C_{3}}j(\epsilon)\right)C_{3}^{2}j(\epsilon)^{4}L^{2}\right), $$
since $\alpha=C_{3}/\gamma$.

