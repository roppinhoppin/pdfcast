---
actor_ids:
  - alice
  - bob
audio_file_path: /audio/03b041226d6dcb2c40f171ab973fda97.wav
transcript_path: /transcript/03b041226d6dcb2c40f171ab973fda97.txt
pdffile_path: /pdf/03b041226d6dcb2c40f171ab973fda97.pdf
date: 2024-11-22 16:36:15 +0900
images: ['images/03b041226d6dcb2c40f171ab973fda97/1.png', 'images/03b041226d6dcb2c40f171ab973fda97/2.png', 'images/03b041226d6dcb2c40f171ab973fda97/3.png', 'images/03b041226d6dcb2c40f171ab973fda97/4.png', 'images/03b041226d6dcb2c40f171ab973fda97/5.png', 'images/03b041226d6dcb2c40f171ab973fda97/6.png', 'images/03b041226d6dcb2c40f171ab973fda97/7.png', 'images/03b041226d6dcb2c40f171ab973fda97/8.png', 'images/03b041226d6dcb2c40f171ab973fda97/9.png', 'images/03b041226d6dcb2c40f171ab973fda97/10.png', 'images/03b041226d6dcb2c40f171ab973fda97/11.png', 'images/03b041226d6dcb2c40f171ab973fda97/12.png', 'images/03b041226d6dcb2c40f171ab973fda97/13.png', 'images/03b041226d6dcb2c40f171ab973fda97/14.png', 'images/03b041226d6dcb2c40f171ab973fda97/15.png', 'images/03b041226d6dcb2c40f171ab973fda97/16.png', 'images/03b041226d6dcb2c40f171ab973fda97/17.png']
description: AI-generated podcast from the PDF file Li et al., 2024 - Evolving Subnetwork Training for Large Language Models_EN
layout: article
title: Li et al., 2024 - Evolving Subnetwork Training for Large Language Models_EN
---

## 文字起こし
Hello everyone, and welcome to Cutting-Edge AI! I'm your host, Matthew.

And I'm thrilled to have Hanqi Li with me today. Hanqi is one of the authors of the groundbreaking paper, "Evolving Subnetwork Training for Large Language Models."  Hanqi, welcome to the show!

Thanks for having me, Matthew! It's great to be here.

So, Hanqi, let's dive right in.  Large language models are amazing, but training them is ridiculously expensive.  Your paper tackles this head-on. Can you give our listeners a quick overview of the problem?

Sure.  LLMs are incredibly powerful, but the cost of training them is astronomical. We're talking millions of dollars and hundreds of GPU-years for some of the biggest models. This limits access for many researchers and smaller companies.

Exactly!  It's a real bottleneck for innovation. Your paper proposes Evolving Subnetwork Training, or EST.  Can you explain the core idea behind EST in simple terms?

Essentially, EST is about training only parts of the model at a time, rather than the whole thing.  Imagine a huge city – that's your LLM. EST is like selectively powering up certain districts to save energy, but still get the job done.

That's a fantastic analogy! So, instead of training the entire network at once, you’re focusing on smaller subnetworks?

Precisely. We sample these subnetworks from different parts of the model – the attention heads, the MLP layers, even entire transformer layers – and gradually increase their size during training.

And the results?  What kind of cost savings did you achieve?

We saw a 26.7% reduction in training costs for GPT-2 and 25% for TinyLlama, without sacrificing performance.  In fact, in some cases, we even saw performance improvements!

Wow, that's incredible!  So, it's not just cheaper; it's potentially better?

That's the exciting part, Matthew!  It seems that focusing the training effort this way helps the model generalize better.

That's a really interesting finding.  What's the theoretical explanation behind this?  Any insights into *why* EST works so well?

Well, our theoretical analysis suggests that EST acts like a structured form of dropout.  It guides the model towards flatter regions in the loss landscape, making optimization easier and faster.

So, a kind of smart dropout, leading to more efficient training?

Exactly!  It's a more targeted approach than random dropout.

This has huge implications, right? More accessible AI for everyone?

Absolutely!  Lower training costs mean more researchers and smaller organizations can participate in the development and application of LLMs.  It could democratize the field.

It really sounds like a game-changer.  Are there any limitations or challenges to EST?  What are some directions for future research?

Definitely. We need to investigate optimal sampling schedules for even greater efficiency.  We also want to explore applying EST to other model architectures beyond Transformers.  There's a lot more to explore.

This has been such a fascinating conversation, Hanqi. Thanks for sharing your insights with us!  To summarize for our listeners, Hanqi and her team have developed Evolving Subnetwork Training, a clever method to dramatically reduce the cost of training large language models without sacrificing performance, and potentially even improving it.  This is a huge step toward making AI more accessible and sustainable. Thank you so much for joining us!

My pleasure, Matthew.  Thanks for having me.


