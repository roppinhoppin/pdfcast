---
audio_file_path: /audio/0e26fdaa9a90f19ad8d796aca3b26351.wav
transcript_path: /transcript/0e26fdaa9a90f19ad8d796aca3b26351.txt
pdffile_path: /pdf/0e26fdaa9a90f19ad8d796aca3b26351.pdf
date: 2024-11-23 23:23:43 +0900
images: ['images/0e26fdaa9a90f19ad8d796aca3b26351/a4f010337bb3aba302f16d1939e42b6489e38cd2b5a0c31a67b937a9ecbfed9d.jpg']
description: AI-generated podcast from the PDF file Böhme - 2024 - Fundamental Challenges in Cybersecurity and a Philosophy of Vulnerability-Guided Hardening_JP / 0e26fdaa9a90f19ad8d796aca3b26351
layout: article
title: Böhme - 2024 - Fundamental Challenges in Cybersecurity and a Philosophy of Vulnerability-Guided Hardening_JP
---

## Transcription
こんにちは！「Code & Conundrums」へようこそ！今日はサイバーセキュリティの奥深い世界に飛び込みます。

初めまして、マシューです。このポッドキャストのホストを務めさせていただきます。

そして、今日はMax Planck Institute for Security and PrivacyのMarcel Böhmeさんをお迎えしています。Marcel、よろしくお願いします！

どうも、マシュー。今日はよろしくお願いします。

では、早速ですが、今日のテーマは「ソフトウェアは本当に安全にできるのか？」ですよね。Marcelさんの論文「Fundamental Challenges in Cybersecurity and a Philosophy of Vulnerability-Guided Hardening」を参考に、深く掘り下げていきたいと思います。

ええ、まさにその通りです。この論文は、最新のハッキング手法やパッチの話ではありません。なぜ、最善を尽くしてもソフトウェアは脆弱性のままなのか、哲学的な視点から考察したものです。

なるほど。例えば、Pwn2Ownコンテストで、たった一人のハッカーが主要なウェブブラウザ全てをハッキングしたという衝撃的な例がありましたよね。一体何が足りないのでしょうか？

その通りです。Google Chrome、Mozilla Firefox、Apple Safari、Microsoft Edge…全てです。これは、長年の研究や対策、そして世界最高レベルと言われるGoogleのレッドチームの存在にも関わらず、起こってしまった出来事です。

信じられないですね…。一体なぜこのようなことが起こるのでしょうか？ソフトウェアは完全に仮想的なもので、完全に記述できるはずなのに…

まさにその点が核心です。プログラムのソースコードは、プログラミング言語の構文と意味論に従って、プログラマーの意図を形式的に表現するものですよね。ソフトウェアの挙動は明確に定義された命令から生まれるので、その全ての特性を形式的に推論できるはずです。

でも、現実はそう簡単ではない。何が問題なのでしょうか？

問題はいくつかあります。まず、「未知の未知」の問題、いわゆる「ブラック・スワン」的な特性です。セキュリティリスクだと考えもしなかったところが、攻撃者に突如として利用される可能性があるのです。SpectreやMeltdownのようなハードウェアの脆弱性を思い出してください。

ああ、あの事件は衝撃でしたね。正式に検証されたシステムでも、脆弱性があったわけですからね。

まさに。そして、セキュリティの定義も難しい。機密性、完全性、可用性…これらはデジタル要塞の比喩で説明できますが、実際には「安全」と「危険」の境界線を引くのは非常に複雑です。

さらに、セキュリティ特性を正確に定義するのも大きな課題です。例えば、タイミング攻撃を防ぐための「定数時間」実行を保証する、というのは一見簡単そうですが、あらゆる秘密依存の最適化（現在と将来の両方）を考慮して正式に定義するのは非常に困難です。

まるでアルゴリズムにおける「公平性」の定義に似ていますね。高レベルの概念を具体的で測定可能な基準で定義する必要がある。

その通りです。そして、推論ツールにも限界があります。私たちはモデルや抽象化を使ってソフトウェアを分析しますが、攻撃者は実際のシステムを操作します。モデルと現実のシステムの間のギャップを攻撃者は突いてくるのです。

コンパイラの最適化もその一例ですね。機能を維持しながらも、サイドチャネルを導入したり、セキュリティ関連のコードを削除したりする可能性があります。

まさに。そして、技術的な課題だけでなく、社会技術的な側面もあります。私たちはコンパイラやOS、オープンソースコードのレビューアーなどを信頼しますが、これらの信頼の基盤は侵害される可能性があります。

Thompsonの「Reflections on Trusting Trust」の講義がまさにそれを示していますね。悪意のあるコンパイラは、脆弱性を永遠に維持する可能性がある。

そうです。さらに、セキュリティはパフォーマンスやコストなどの他の優先事項としばしば衝突するため、利害関係者間の継続的な調整が必要となります。

では、解決策は何でしょうか？

絶対的なセキュリティの証明を目指すのではなく、「脆弱性主導の強化」という考え方です。

それはどんな考え方ですか？

漁網をバグの捕獲に例えると分かりやすいです。どんな網にも、すり抜ける魚はいます。究極の網を作るのではなく、すり抜けた魚に基づいて網を継続的に改良していくのです。

つまり、脆弱性を積極的に探し出し、ツールがそれを見逃した理由を理解し、手法を適応していくということですね。

はい。セキュリティは目的地ではなく、継続的な改善の旅なのです。

今日は本当に興味深いお話ありがとうございました！







